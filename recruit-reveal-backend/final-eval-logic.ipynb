{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install imbalanced-learn xgboost shap scipy scikit-learn pandas numpy autogluon ctgan sentence-transformers mlflow"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 70,
              "statement_ids": [
                66,
                67,
                68,
                69,
                70
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.7373954Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:23:28.737871Z",
              "execution_finish_time": "2025-08-04T00:24:10.4878973Z",
              "parent_msg_id": "1ee80a07-bacc-49c4-a05e-4bb5513aa5cb"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 70, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 351,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (0.13.0)\nRequirement already satisfied: xgboost in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: shap in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (0.44.0)\nRequirement already satisfied: scipy in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (1.11.4)\nRequirement already satisfied: scikit-learn in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (1.6.1)\nRequirement already satisfied: pandas in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (2.3.1)\nRequirement already satisfied: numpy in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: autogluon in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (1.4.0)\nRequirement already satisfied: ctgan in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (0.11.0)\nRequirement already satisfied: sentence-transformers in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (5.0.0)\nRequirement already satisfied: mlflow in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: joblib<2,>=1.1.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from imbalanced-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from imbalanced-learn) (3.3.0)\nRequirement already satisfied: tqdm>=4.27.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (23.2)\nRequirement already satisfied: slicer==0.0.7 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (0.59.0)\nRequirement already satisfied: cloudpickle in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (2.2.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from pandas) (2.9.0)\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: tzdata>=2022.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: autogluon.core[all]==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.features==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.tabular[all]==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.multimodal==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.timeseries[all]==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: networkx<4,>=3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (3.2.1)\nRequirement already satisfied: requests in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (2.32.4)\nRequirement already satisfied: matplotlib<3.11,>=3.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (3.8.2)\nRequirement already satisfied: boto3<2,>=1.10 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (1.40.1)\nRequirement already satisfied: autogluon.common==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (0.2.7)\nRequirement already satisfied: pyarrow>=15.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (20.0.0)\nRequirement already satisfied: ray[default,tune]<2.45,>=2.10.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (2.44.1)\nRequirement already satisfied: Pillow<12,>=10.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (10.2.0)\nRequirement already satisfied: torch<2.8,>=2.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.7.1)\nRequirement already satisfied: lightning<2.8,>=2.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.5.2)\nRequirement already satisfied: transformers[sentencepiece]<4.50,>=4.38.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (4.49.0)\nRequirement already satisfied: accelerate<2.0,>=0.34.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.9.0)\nRequirement already satisfied: fsspec[http]<=2025.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2024.2.0)\nRequirement already satisfied: jsonschema<4.24,>=4.18 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (4.21.1)\nRequirement already satisfied: seqeval<1.3.0,>=1.2.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.2.2)\nRequirement already satisfied: evaluate<0.5.0,>=0.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.4.5)\nRequirement already satisfied: timm<1.0.7,>=0.9.5 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.0.3)\nRequirement already satisfied: torchvision<0.23.0,>=0.16.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.22.1)\nRequirement already satisfied: scikit-image<0.26.0,>=0.19.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.25.2)\nRequirement already satisfied: text-unidecode<1.4,>=1.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.3)\nRequirement already satisfied: torchmetrics<1.8,>=1.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.7.4)\nRequirement already satisfied: omegaconf<2.4.0,>=2.1.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.3.0)\nRequirement already satisfied: pytorch-metric-learning<2.9,>=1.3.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.8.1)\nRequirement already satisfied: nlpaug<1.2.0,>=1.1.10 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.1.11)\nRequirement already satisfied: nltk<3.10,>=3.4.5 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (3.9.1)\nRequirement already satisfied: openmim<0.4.0,>=0.3.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.9)\nRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.7.1)\nRequirement already satisfied: jinja2<3.2,>=3.0.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (3.1.3)\nRequirement already satisfied: tensorboard<3,>=2.9 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.15.2)\nRequirement already satisfied: pytesseract<0.4,>=0.3.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.13)\nRequirement already satisfied: nvidia-ml-py3<8.0,>=7.352.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (7.352.0)\nRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.17.0)\nRequirement already satisfied: catboost<1.3,>=1.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (1.2.8)\nRequirement already satisfied: fastai<2.9,>=2.3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.8.2)\nRequirement already satisfied: loguru in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.7.3)\nRequirement already satisfied: lightgbm<4.7,>=4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (4.2.0)\nRequirement already satisfied: einx in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.3.0)\nRequirement already satisfied: spacy<3.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (3.8.7)\nRequirement already satisfied: huggingface-hub[torch] in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.34.3)\nRequirement already satisfied: pytorch-lightning in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (2.5.2)\nRequirement already satisfied: gluonts<0.17,>=0.15.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.16.2)\nRequirement already satisfied: statsforecast<2.0.2,>=1.7.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (2.0.1)\nRequirement already satisfied: mlforecast<0.15.0,>=0.14.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.14.0)\nRequirement already satisfied: utilsforecast<0.2.12,>=0.2.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.2.11)\nRequirement already satisfied: coreforecast<0.0.17,>=0.0.12 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.0.16)\nRequirement already satisfied: fugue>=0.9.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.9.1)\nRequirement already satisfied: orjson~=3.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (3.11.1)\nRequirement already satisfied: psutil<7.1.0,>=5.7.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.common==1.4.0->autogluon.core[all]==1.4.0->autogluon) (5.9.8)\nRequirement already satisfied: rdt>=1.14.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ctgan) (1.17.1)\nRequirement already satisfied: typing_extensions>=4.5.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from sentence-transformers) (4.14.1)\nRequirement already satisfied: mlflow-skinny==3.1.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (3.1.4)\nRequirement already satisfied: Flask<4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow) (3.0.2)\nRequirement already satisfied: alembic!=1.10.0,<2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (1.16.4)\nRequirement already satisfied: docker<8,>=4.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: graphene<4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (23.0.0)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow) (2.0.28)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (8.1.7)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.61.0)\nRequirement already satisfied: fastapi<1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.42)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (7.0.2)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (4.24.4)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (2.11.7)\nRequirement already satisfied: pyyaml<7,>=5.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (6.0.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.4.4)\nRequirement already satisfied: uvicorn<1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\nRequirement already satisfied: Mako in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: tomli in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (2.0.1)\nRequirement already satisfied: urllib3>=1.26.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.1.0)\nRequirement already satisfied: Werkzeug>=3.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.1)\nRequirement already satisfied: itsdangerous>=2.1.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from Flask<4->mlflow) (2.1.2)\nRequirement already satisfied: blinker>=1.6.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from Flask<4->mlflow) (1.7.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.6)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: filelock in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from huggingface-hub[torch]->autogluon.tabular[all]==1.4.0->autogluon) (3.13.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from huggingface-hub[torch]->autogluon.tabular[all]==1.4.0->autogluon) (1.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (4.49.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (3.1.2)\nRequirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: Faker>=17 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from rdt>=1.14.0->ctgan) (37.5.3)\nRequirement already satisfied: greenlet!=0.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nRequirement already satisfied: sympy>=1.13.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.14.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.11.1.6)\nRequirement already satisfied: triton==3.3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (3.3.1)\nRequirement already satisfied: setuptools>=40.8.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from triton==3.3.1->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (69.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (2023.12.25)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.21.4)\nRequirement already satisfied: safetensors>=0.4.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.5.3)\nRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from numba->shap) (0.42.0)\nRequirement already satisfied: botocore<1.41.0,>=1.40.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core[all]==1.4.0->autogluon) (1.40.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core[all]==1.4.0->autogluon) (1.0.1)\nRequirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core[all]==1.4.0->autogluon) (0.13.1)\nRequirement already satisfied: graphviz in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (0.21)\nRequirement already satisfied: plotly in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (5.18.0)\nRequirement already satisfied: google-auth~=2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.28.2)\nRequirement already satisfied: datasets>=2.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (4.0.0)\nRequirement already satisfied: dill in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.3.8)\nRequirement already satisfied: xxhash in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: multiprocess in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.70.16)\nRequirement already satisfied: pip in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (23.1.2)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.7)\nRequirement already satisfied: fastcore<1.9,>=1.8.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.8.7)\nRequirement already satisfied: fasttransform>=0.0.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.2)\nRequirement already satisfied: fastprogress>=0.2.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.0.3)\nRequirement already satisfied: plum-dispatch in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (2.5.7)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (3.9.3)\nRequirement already satisfied: triad>=0.9.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.9.8)\nRequirement already satisfied: adagio>=0.2.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.2.6)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (4.0.11)\nRequirement already satisfied: toolz~=0.10 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.12.1)\nRequirement already satisfied: future in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (1.0.0)\nRequirement already satisfied: py4j in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (0.10.9.7)\nRequirement already satisfied: zipp>=0.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.4.0->autogluon) (2.1.5)\nRequirement already satisfied: attrs>=22.2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.33.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.18.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.15.0)\nRequirement already satisfied: optuna in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.4.0)\nRequirement already satisfied: window-ops in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.0.15)\nRequirement already satisfied: gdown>=4.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (5.2.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.4.0->autogluon) (4.9.3)\nRequirement already satisfied: colorama in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.4.6)\nRequirement already satisfied: model-index in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.11)\nRequirement already satisfied: opendatalab in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.0.10)\nRequirement already satisfied: rich in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (14.1.0)\nRequirement already satisfied: tabulate in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.9.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow) (0.57b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.4.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.0.8)\nRequirement already satisfied: aiosignal in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.3.1)\nRequirement already satisfied: frozenlist in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.4.1)\nRequirement already satisfied: tensorboardX>=1.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (2.6.4)\nRequirement already satisfied: aiohttp_cors in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.8.1)\nRequirement already satisfied: colorful in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.5.7)\nRequirement already satisfied: py-spy>=0.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: grpcio>=1.42.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.59.3)\nRequirement already satisfied: opencensus in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.11.4)\nRequirement already satisfied: prometheus_client>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.20.0)\nRequirement already satisfied: smart_open in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (7.3.0.post1)\nRequirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (20.23.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (2024.2.2)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2025.5.10)\nRequirement already satisfied: lazy-loader>=0.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (0.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (8.3.4)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.16.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: statsmodels>=0.13.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.14.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from sympy>=1.13.3->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: absl-py>=0.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (2.1.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.5.1)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (0.7.0)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.2.0)\nRequirement already satisfied: h11>=0.8 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\nRequirement already satisfied: frozendict in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from einx->autogluon.tabular[all]==1.4.0->autogluon) (2.4.6)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (4.0.3)\nRequirement already satisfied: beautifulsoup4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (4.12.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: language-data>=1.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (3.7.1)\nRequirement already satisfied: patsy>=0.5.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.5.6)\nRequirement already satisfied: blis<1.3.0,>=1.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.2.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.1.5)\nRequirement already satisfied: fs in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.4.16)\nRequirement already satisfied: shellingham>=1.3.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (2.17.2)\nRequirement already satisfied: distlib<1,>=0.3.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.3.8)\nRequirement already satisfied: platformdirs<4,>=3.5.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (3.11.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.21.1)\nRequirement already satisfied: wrapt in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from smart_open->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.14.1)\nRequirement already satisfied: ordered-set in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (4.1.0)\nRequirement already satisfied: opencensus-context>=0.1.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.1.3)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (2.17.1)\nRequirement already satisfied: pycryptodome in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.23.0)\nRequirement already satisfied: openxlab in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.0.11)\nRequirement already satisfied: colorlog in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries[all]==1.4.0->autogluon) (6.9.0)\nRequirement already satisfied: tenacity>=6.2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (8.2.3)\nRequirement already satisfied: beartype>=0.16.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from plum-dispatch->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.21.0)\nRequirement already satisfied: sniffio>=1.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\nRequirement already satisfied: exceptiongroup in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.2.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.63.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.2.1)\nRequirement already satisfied: mdurl~=0.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.2)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.2.2)\nRequirement already satisfied: soupsieve>1.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (2.5)\nRequirement already satisfied: appdirs~=1.4.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.4.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (1.7.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 351,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: PySpark kernel has been restarted to use updated packages.\n\n"
          ]
        }
      ],
      "execution_count": 351,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import json\n",
        "# import os\n",
        "\n",
        "# def load_athlete_input():\n",
        "#     # Look for real, user-supplied arguments (not -f or .json connection files)\n",
        "#     cli_args = [arg for arg in sys.argv[1:] if not (arg.startswith('-f') or arg.endswith('.json'))]\n",
        "#     if cli_args:\n",
        "#         arg = cli_args[0]\n",
        "#         try:\n",
        "#             athlete_input = json.loads(arg)\n",
        "#             print(\"Loaded athlete input from JSON string.\")\n",
        "#         except json.JSONDecodeError:\n",
        "#             if os.path.isfile(arg):\n",
        "#                 try:\n",
        "#                     with open(arg, \"r\") as f:\n",
        "#                         athlete_input = json.load(f)\n",
        "#                     print(f\"Loaded athlete input from file: {arg}\")\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Failed to parse JSON file: {e}\")\n",
        "#                     athlete_input = None\n",
        "#             else:\n",
        "#                 print(f\"Input is neither a valid JSON string nor a file: {arg}\")\n",
        "#                 athlete_input = None\n",
        "#     else:\n",
        "#         # Default for notebook/test/dev\n",
        "#         athlete_input = {\n",
        "#             'Senior_Yds': 1123, 'Senior_Avg': 17.3, 'Senior_Rec': 65, 'Senior_TD': 12, 'Senior_Rush_Yds': 100,\n",
        "#             'Height_Inches': 71, 'Weight_Lbs': 180, 'Forty_Yard_Dash': 4.40, 'Vertical_Jump': 39, 'Shuttle': 4.05,\n",
        "#             'Broad_Jump': 125, 'State': 'TX', 'position': 'WR', 'grad_year': 2025\n",
        "#         }\n",
        "#         print(\"No valid user input detected – using default test athlete.\")\n",
        "#     if not isinstance(athlete_input, dict):\n",
        "#         raise ValueError(\"No valid athlete input found (check input or Synapse pipeline config).\")\n",
        "#     return athlete_input\n",
        "\n",
        "# athlete_input = load_athlete_input()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 72,
              "statement_ids": [
                72
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.4433647Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:19.0426941Z",
              "execution_finish_time": "2025-08-04T00:24:19.3102915Z",
              "parent_msg_id": "f1c58c20-6313-4a70-9d12-317ecef3b6df"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 72, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 352,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import mstats, percentileofscore \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "np.random.seed(42)\n",
        "\n",
        "# CRITICAL: Add XGBoost safeguard function to prevent duplicate column errors\n",
        "def xgboost_safeguard(X_train, X_test, step_name=\"Model Training\"):\n",
        "    \"\"\"\n",
        "    Comprehensive safeguard function to ensure XGBoost compatibility\n",
        "    Removes duplicate columns and validates data before training\n",
        "    \"\"\"\n",
        "    print(f\"\\n🛡️  XGBoost Safeguard - {step_name}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Step 1: Check for duplicate column names\n",
        "    train_duplicates = X_train.columns.duplicated()\n",
        "    test_duplicates = X_test.columns.duplicated()\n",
        "    \n",
        "    if train_duplicates.any():\n",
        "        duplicate_cols = X_train.columns[train_duplicates].unique()\n",
        "        print(f\"⚠️  Found {len(duplicate_cols)} duplicate columns in training data: {list(duplicate_cols)}\")\n",
        "        X_train = X_train.loc[:, ~X_train.columns.duplicated(keep='first')]\n",
        "        print(f\"✅ Removed duplicates from training data: {X_train.shape}\")\n",
        "    \n",
        "    if test_duplicates.any():\n",
        "        duplicate_cols = X_test.columns[test_duplicates].unique()\n",
        "        print(f\"⚠️  Found {len(duplicate_cols)} duplicate columns in test data: {list(duplicate_cols)}\")\n",
        "        X_test = X_test.loc[:, ~X_test.columns.duplicated(keep='first')]\n",
        "        print(f\"✅ Removed duplicates from test data: {X_test.shape}\")\n",
        "    \n",
        "    # Step 2: Ensure both datasets have the same columns\n",
        "    train_cols = set(X_train.columns)\n",
        "    test_cols = set(X_test.columns)\n",
        "    \n",
        "    if train_cols != test_cols:\n",
        "        print(f\"⚠️  Column mismatch detected\")\n",
        "        print(f\"   Training columns: {len(train_cols)}\")\n",
        "        print(f\"   Test columns: {len(test_cols)}\")\n",
        "        \n",
        "        # Use intersection of columns\n",
        "        common_cols = list(train_cols & test_cols)\n",
        "        print(f\"   Using {len(common_cols)} common columns\")\n",
        "        \n",
        "        X_train = X_train[common_cols]\n",
        "        X_test = X_test[common_cols]\n",
        "    \n",
        "    # Step 3: Validate data types for XGBoost compatibility\n",
        "    invalid_dtypes = []\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype not in ['int64', 'float64', 'int32', 'float32', 'bool', 'int8', 'float16']:\n",
        "            invalid_dtypes.append((col, X_train[col].dtype))\n",
        "    \n",
        "    if invalid_dtypes:\n",
        "        print(f\"⚠️  Found {len(invalid_dtypes)} columns with invalid dtypes:\")\n",
        "        for col, dtype in invalid_dtypes[:5]:  # Show first 5\n",
        "            print(f\"   {col}: {dtype}\")\n",
        "        \n",
        "        # Convert to numeric\n",
        "        for col, _ in invalid_dtypes:\n",
        "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)\n",
        "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n",
        "        print(f\"✅ Converted invalid dtypes to numeric\")\n",
        "    \n",
        "    # Step 4: Check for infinite values\n",
        "    train_inf = np.isinf(X_train).any().any()\n",
        "    test_inf = np.isinf(X_test).any().any()\n",
        "    \n",
        "    if train_inf or test_inf:\n",
        "        print(f\"⚠️  Found infinite values - replacing with NaN then 0\")\n",
        "        X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        print(f\"✅ Replaced infinite values\")\n",
        "    \n",
        "    # Step 5: Final validation - absolutely no duplicates\n",
        "    final_train_dups = X_train.columns.duplicated().any()\n",
        "    final_test_dups = X_test.columns.duplicated().any()\n",
        "    \n",
        "    if final_train_dups or final_test_dups:\n",
        "        print(f\"🚨 CRITICAL: Still have duplicates after safeguards!\")\n",
        "        # Nuclear option: rename all columns to generic names\n",
        "        n_cols = len(X_train.columns)\n",
        "        new_col_names = [f\"feature_{i:03d}\" for i in range(n_cols)]\n",
        "        X_train.columns = new_col_names\n",
        "        X_test.columns = new_col_names\n",
        "        print(f\"🔧 Applied nuclear fix: renamed all columns to generic names\")\n",
        "    \n",
        "    print(f\"✅ XGBoost Safeguard Complete\")\n",
        "    print(f\"   Training data: {X_train.shape}\")\n",
        "    print(f\"   Test data: {X_test.shape}\")\n",
        "    print(f\"   Data types: {X_train.dtypes.value_counts().to_dict()}\")\n",
        "    \n",
        "    return X_train, X_test\n",
        "\n",
        "# Modular functions for easy upgrades\n",
        "def augment_data(X, y):\n",
        "    \"\"\"ADASYN for now, but easy to swap with GAN/ctgan/SMOTE.\"\"\"\n",
        "    try:\n",
        "        adasyn = ADASYN(random_state=42, sampling_strategy='auto')\n",
        "        X_aug, y_aug = adasyn.fit_resample(X, y)\n",
        "        print(f\"ADASYN: {X.shape} -> {X_aug.shape}\")\n",
        "        return X_aug, y_aug\n",
        "    except ValueError as e:\n",
        "        print(f\"ADASYN failed ({e}), using original data\")\n",
        "        return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"XGBClassifier for now, easy to swap with AutoGluon/CatBoost/Ordinal.\"\"\"\n",
        "    # Apply XGBoost safeguard before training\n",
        "    if len(X.shape) == 2:  # Only apply to feature matrices\n",
        "        # Create dummy test set for validation (will be ignored)\n",
        "        X_dummy = X.iloc[:5].copy() if hasattr(X, 'iloc') else X[:5].copy()\n",
        "        X, X_dummy = xgboost_safeguard(X, X_dummy, \"Pre-Training Validation\")\n",
        "    \n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100, \n",
        "        max_depth=3, \n",
        "        learning_rate=0.1, \n",
        "        eval_metric='mlogloss', \n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    return model"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 73,
              "statement_ids": [
                73
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.5851287Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:19.3230131Z",
              "execution_finish_time": "2025-08-04T00:24:21.228619Z",
              "parent_msg_id": "712e176a-9e26-4e95-aa36-e888bc6ad3af"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 73, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 353,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Cell 3: Enhanced Data Acquisition with Intelligent Benchmark Imputation\n",
        "\n",
        "def load_base_csv_enhanced(position):\n",
        "    \"\"\"Enhanced data loading with intelligent missing data handling\"\"\"\n",
        "    # Use abfss for Synapse/ADLS Gen2 (recommended with Linked Service)\n",
        "    paths = {\n",
        "        'qb': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/221 QB FINAL - Sheet1.csv',\n",
        "        'rb': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/RB list 1 - Sheet1.csv',\n",
        "        'wr': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/wr final - Sheet1.csv',\n",
        "        'db': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/db.csv',\n",
        "        'lb': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/lb.csv',\n",
        "        'te': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/te.csv'\n",
        "    }\n",
        "    path = paths.get(position, paths['qb'])\n",
        "    try:\n",
        "        df_spark = spark.read.csv(path, header=True, inferSchema=True)\n",
        "        df = df_spark.toPandas()\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "        print(f\"Loaded {len(df)} rows for {position.upper()}\")\n",
        "        if 'division' in df.columns:\n",
        "            print(f\"Unique divisions for {position.upper()}: {df['division'].unique()}\")\n",
        "        else:\n",
        "            print(f\"No 'division' column found for {position.upper()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Load failed for {position}: {e}\")\n",
        "        df = pd.DataFrame(columns=[\n",
        "            'name','division','state','height_inches','weight_lbs','senior_yds','senior_avg','senior_rec',\n",
        "            'senior_td','junior_yds','junior_avg','junior_rec','junior_td','senior_ypg','senior_tds',\n",
        "            'senior_comp_pct','senior_ypc','senior_rush_yds','grad_year'\n",
        "        ])\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "    \n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Normalize division spelling everywhere!\n",
        "    if 'division' in df.columns:\n",
        "        df['division'] = (\n",
        "            df['division'].astype(str)\n",
        "            .str.strip()\n",
        "            .str.upper()\n",
        "            .str.replace('POWER5', 'POWER 5', regex=False)\n",
        "            .str.replace('FBS', 'POWER 5', regex=False)\n",
        "            .str.replace('D3/NAIA', 'D3', regex=False)\n",
        "            .str.replace('NAIA', 'NAIA', regex=False)\n",
        "        )\n",
        "    \n",
        "    # Always provide 'position' for downstream logic\n",
        "    df['position'] = position.lower()\n",
        "    return df\n",
        "\n",
        "# High School Football Recruiting Guidelines - Combine Benchmarks by Position/Division\n",
        "# Based on industry standards and recruiting data\n",
        "COMBINE_BENCHMARKS = {\n",
        "    'qb': {\n",
        "        'POWER 5': {'forty_yard_dash': (4.6, 4.9), 'vertical_jump': (30, 34), 'shuttle': (4.3, 4.6), 'broad_jump': (108, 118)},\n",
        "        'FCS': {'forty_yard_dash': (4.7, 5.0), 'vertical_jump': (28, 32), 'shuttle': (4.4, 4.7), 'broad_jump': (102, 112)},\n",
        "        'D2': {'forty_yard_dash': (4.8, 5.1), 'vertical_jump': (26, 30), 'shuttle': (4.5, 4.8), 'broad_jump': (96, 106)},\n",
        "        'D3': {'forty_yard_dash': (4.9, 5.3), 'vertical_jump': (24, 28), 'shuttle': (4.6, 4.9), 'broad_jump': (90, 100)},\n",
        "        'NAIA': {'forty_yard_dash': (4.8, 5.2), 'vertical_jump': (25, 29), 'shuttle': (4.5, 4.8), 'broad_jump': (92, 102)}\n",
        "    },\n",
        "    'rb': {\n",
        "        'POWER 5': {'forty_yard_dash': (4.2, 4.5), 'vertical_jump': (34, 38), 'shuttle': (4.0, 4.3), 'broad_jump': (120, 130)},\n",
        "        'FCS': {'forty_yard_dash': (4.3, 4.6), 'vertical_jump': (32, 36), 'shuttle': (4.1, 4.4), 'broad_jump': (110, 120)},\n",
        "        'D2': {'forty_yard_dash': (4.4, 4.7), 'vertical_jump': (30, 34), 'shuttle': (4.2, 4.5), 'broad_jump': (100, 110)},\n",
        "        'D3': {'forty_yard_dash': (4.5, 4.8), 'vertical_jump': (28, 32), 'shuttle': (4.3, 4.6), 'broad_jump': (95, 105)},\n",
        "        'NAIA': {'forty_yard_dash': (4.4, 4.7), 'vertical_jump': (29, 33), 'shuttle': (4.2, 4.5), 'broad_jump': (98, 108)}\n",
        "    },\n",
        "    'wr': {\n",
        "        'POWER 5': {'forty_yard_dash': (4.4, 4.7), 'vertical_jump': (34, 38), 'shuttle': (4.1, 4.4), 'broad_jump': (120, 130)},\n",
        "        'FCS': {'forty_yard_dash': (4.5, 4.8), 'vertical_jump': (33, 37), 'shuttle': (4.2, 4.5), 'broad_jump': (110, 120)},\n",
        "        'D2': {'forty_yard_dash': (4.6, 4.9), 'vertical_jump': (31, 35), 'shuttle': (4.3, 4.6), 'broad_jump': (100, 110)},\n",
        "        'D3': {'forty_yard_dash': (4.7, 5.0), 'vertical_jump': (29, 33), 'shuttle': (4.4, 4.7), 'broad_jump': (95, 105)},\n",
        "        'NAIA': {'forty_yard_dash': (4.6, 4.9), 'vertical_jump': (30, 34), 'shuttle': (4.3, 4.6), 'broad_jump': (98, 108)}\n",
        "    }\n",
        "}\n",
        "\n",
        "def intelligent_combine_imputation(df, position):\n",
        "    \"\"\"Intelligent imputation using benchmark ranges with Bayesian-inspired priors\"\"\"\n",
        "    df = df.copy()\n",
        "    position = position.lower()\n",
        "    \n",
        "    # Normalize division for lookup\n",
        "    df['division_lookup'] = df['division'].str.upper()\n",
        "    \n",
        "    combine_metrics = ['forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump']\n",
        "    position_benchmarks = COMBINE_BENCHMARKS.get(position, COMBINE_BENCHMARKS['qb'])\n",
        "    \n",
        "    imputation_log = []\n",
        "    \n",
        "    for metric in combine_metrics:\n",
        "        if metric not in df.columns:\n",
        "            df[metric] = np.nan\n",
        "            df[f'{metric}_imputed'] = True\n",
        "            imputation_log.append(f\"Created missing column {metric}\")\n",
        "        else:\n",
        "            df[f'{metric}_imputed'] = df[metric].isna()\n",
        "        \n",
        "        missing_mask = df[metric].isna()\n",
        "        if missing_mask.any():\n",
        "            missing_count = missing_mask.sum()\n",
        "            imputation_log.append(f\"Imputing {missing_count} missing {metric} values\")\n",
        "            \n",
        "            # Impute based on division-specific benchmarks\n",
        "            for division in df['division_lookup'].unique():\n",
        "                if pd.isna(division):\n",
        "                    continue\n",
        "                    \n",
        "                div_mask = (df['division_lookup'] == division) & missing_mask\n",
        "                if not div_mask.any():\n",
        "                    continue\n",
        "                \n",
        "                # Get benchmark range for this position/division\n",
        "                if division in position_benchmarks:\n",
        "                    min_val, max_val = position_benchmarks[division][metric]\n",
        "                else:\n",
        "                    # Fallback to D3 benchmarks if division not found\n",
        "                    min_val, max_val = position_benchmarks['D3'][metric]\n",
        "                \n",
        "                # Bayesian-inspired imputation: use normal distribution centered on range midpoint\n",
        "                mean_val = (min_val + max_val) / 2\n",
        "                std_val = (max_val - min_val) / 4  # Assume 95% of values within range\n",
        "                \n",
        "                # Generate values and clip to realistic range\n",
        "                n_samples = div_mask.sum()\n",
        "                imputed_values = np.random.normal(mean_val, std_val, n_samples)\n",
        "                imputed_values = np.clip(imputed_values, min_val * 0.9, max_val * 1.1)\n",
        "                \n",
        "                df.loc[div_mask, metric] = imputed_values\n",
        "                imputation_log.append(f\"  {division}: {n_samples} values from N({mean_val:.2f}, {std_val:.2f})\")\n",
        "    \n",
        "    print(f\"Combine imputation for {position.upper()}:\")\n",
        "    for log_entry in imputation_log:\n",
        "        print(f\"  {log_entry}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def enrich_data_enhanced(df, position, year=2025):\n",
        "    \"\"\"Enhanced data enrichment with more balanced synthetic samples\"\"\"\n",
        "    enrich_data = {\n",
        "        'qb': [\n",
        "            # Power 5 samples\n",
        "            {'name': 'Elite Power 5 QB', 'height_inches': 75, 'weight_lbs': 215, 'senior_ypg': 285, 'senior_tds': 28, 'senior_comp_pct': 68, 'state': 'TX', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 112},\n",
        "            {'name': 'Good Power 5 QB', 'height_inches': 73, 'weight_lbs': 205, 'senior_ypg': 255, 'senior_tds': 24, 'senior_comp_pct': 64, 'state': 'CA', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 31, 'shuttle': 4.5, 'broad_jump': 110},\n",
        "            # FCS samples\n",
        "            {'name': 'Elite FCS QB', 'height_inches': 73, 'weight_lbs': 200, 'senior_ypg': 225, 'senior_tds': 22, 'senior_comp_pct': 62, 'state': 'FL', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 30, 'shuttle': 4.5, 'broad_jump': 108},\n",
        "            {'name': 'Good FCS QB', 'height_inches': 72, 'weight_lbs': 195, 'senior_ypg': 200, 'senior_tds': 18, 'senior_comp_pct': 58, 'state': 'GA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 29, 'shuttle': 4.6, 'broad_jump': 105},\n",
        "            {'name': 'Solid FCS QB', 'height_inches': 71, 'weight_lbs': 190, 'senior_ypg': 180, 'senior_tds': 16, 'senior_comp_pct': 55, 'state': 'NC', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 28, 'shuttle': 4.6, 'broad_jump': 103},\n",
        "            # D2 samples\n",
        "            {'name': 'Elite D2 QB', 'height_inches': 71, 'weight_lbs': 190, 'senior_ypg': 165, 'senior_tds': 16, 'senior_comp_pct': 58, 'state': 'OH', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 28, 'shuttle': 4.6, 'broad_jump': 101},\n",
        "            {'name': 'Good D2 QB', 'height_inches': 70, 'weight_lbs': 185, 'senior_ypg': 145, 'senior_tds': 14, 'senior_comp_pct': 54, 'state': 'MI', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 5.0, 'vertical_jump': 27, 'shuttle': 4.7, 'broad_jump': 98},\n",
        "            # D3 samples\n",
        "            {'name': 'Elite D3 QB', 'height_inches': 70, 'weight_lbs': 180, 'senior_ypg': 125, 'senior_tds': 12, 'senior_comp_pct': 52, 'state': 'IL', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 5.1, 'vertical_jump': 26, 'shuttle': 4.7, 'broad_jump': 95},\n",
        "            {'name': 'Good D3 QB', 'height_inches': 69, 'weight_lbs': 175, 'senior_ypg': 105, 'senior_tds': 10, 'senior_comp_pct': 48, 'state': 'PA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 5.2, 'vertical_jump': 25, 'shuttle': 4.8, 'broad_jump': 92},\n",
        "            # NAIA samples\n",
        "            {'name': 'Elite NAIA QB', 'height_inches': 70, 'weight_lbs': 185, 'senior_ypg': 135, 'senior_tds': 13, 'senior_comp_pct': 55, 'state': 'KS', 'division': 'NAIA', 'grad_year': 2025, 'forty_yard_dash': 5.0, 'vertical_jump': 27, 'shuttle': 4.6, 'broad_jump': 97}\n",
        "        ],\n",
        "        'rb': [\n",
        "            # Power 5 samples\n",
        "            {'name': 'Elite Power 5 RB', 'height_inches': 70, 'weight_lbs': 205, 'senior_ypg': 145, 'senior_tds': 18, 'senior_ypc': 5.8, 'state': 'TX', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.35, 'vertical_jump': 36, 'shuttle': 4.1, 'broad_jump': 125},\n",
        "            {'name': 'Good Power 5 RB', 'height_inches': 69, 'weight_lbs': 195, 'senior_ypg': 125, 'senior_tds': 15, 'senior_ypc': 5.2, 'state': 'FL', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.4, 'vertical_jump': 35, 'shuttle': 4.2, 'broad_jump': 122},\n",
        "            # FCS samples\n",
        "            {'name': 'Elite FCS RB', 'height_inches': 69, 'weight_lbs': 190, 'senior_ypg': 115, 'senior_tds': 14, 'senior_ypc': 4.8, 'state': 'CA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.4, 'vertical_jump': 34, 'shuttle': 4.2, 'broad_jump': 115},\n",
        "            {'name': 'Good FCS RB', 'height_inches': 68, 'weight_lbs': 185, 'senior_ypg': 95, 'senior_tds': 12, 'senior_ypc': 4.4, 'state': 'GA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 33, 'shuttle': 4.3, 'broad_jump': 112},\n",
        "            {'name': 'Solid FCS RB', 'height_inches': 67, 'weight_lbs': 180, 'senior_ypg': 85, 'senior_tds': 10, 'senior_ypc': 4.1, 'state': 'NC', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            # D2 samples\n",
        "            {'name': 'Elite D2 RB', 'height_inches': 68, 'weight_lbs': 180, 'senior_ypg': 85, 'senior_tds': 11, 'senior_ypc': 4.2, 'state': 'OH', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 32, 'shuttle': 4.3, 'broad_jump': 105},\n",
        "            {'name': 'Good D2 RB', 'height_inches': 67, 'weight_lbs': 175, 'senior_ypg': 75, 'senior_tds': 9, 'senior_ypc': 3.9, 'state': 'MI', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 31, 'shuttle': 4.4, 'broad_jump': 102},\n",
        "            # D3 samples\n",
        "            {'name': 'Elite D3 RB', 'height_inches': 67, 'weight_lbs': 170, 'senior_ypg': 65, 'senior_tds': 8, 'senior_ypc': 3.6, 'state': 'IL', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 30, 'shuttle': 4.4, 'broad_jump': 98},\n",
        "            {'name': 'Good D3 RB', 'height_inches': 66, 'weight_lbs': 165, 'senior_ypg': 55, 'senior_tds': 7, 'senior_ypc': 3.3, 'state': 'PA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 29, 'shuttle': 4.5, 'broad_jump': 96},\n",
        "            # NAIA samples\n",
        "            {'name': 'Elite NAIA RB', 'height_inches': 67, 'weight_lbs': 175, 'senior_ypg': 75, 'senior_tds': 9, 'senior_ypc': 3.8, 'state': 'KS', 'division': 'NAIA', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 31, 'shuttle': 4.3, 'broad_jump': 103}\n",
        "        ],\n",
        "        'wr': [\n",
        "            # Power 5 samples\n",
        "            {'name': 'Elite Power 5 WR', 'height_inches': 72, 'weight_lbs': 185, 'senior_yds': 1100, 'senior_avg': 18.5, 'senior_rec': 60, 'senior_td': 14, 'state': 'TX', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.45, 'vertical_jump': 36, 'shuttle': 4.2, 'broad_jump': 125},\n",
        "            {'name': 'Good Power 5 WR', 'height_inches': 71, 'weight_lbs': 180, 'senior_yds': 950, 'senior_avg': 16.8, 'senior_rec': 55, 'senior_td': 12, 'state': 'FL', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 35, 'shuttle': 4.3, 'broad_jump': 122},\n",
        "            # FCS samples\n",
        "            {'name': 'Elite FCS WR', 'height_inches': 71, 'weight_lbs': 175, 'senior_yds': 850, 'senior_avg': 16.0, 'senior_rec': 52, 'senior_td': 10, 'state': 'CA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 35, 'shuttle': 4.3, 'broad_jump': 115},\n",
        "            {'name': 'Good FCS WR', 'height_inches': 70, 'weight_lbs': 170, 'senior_yds': 750, 'senior_avg': 15.2, 'senior_rec': 48, 'senior_td': 8, 'state': 'GA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 34, 'shuttle': 4.4, 'broad_jump': 112},\n",
        "            {'name': 'Solid FCS WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 650, 'senior_avg': 14.5, 'senior_rec': 44, 'senior_td': 7, 'state': 'NC', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 33, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            # D2 samples\n",
        "            {'name': 'Elite D2 WR', 'height_inches': 70, 'weight_lbs': 170, 'senior_yds': 600, 'senior_avg': 14.0, 'senior_rec': 42, 'senior_td': 7, 'state': 'OH', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 33, 'shuttle': 4.4, 'broad_jump': 105},\n",
        "            {'name': 'Good D2 WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 520, 'senior_avg': 13.2, 'senior_rec': 38, 'senior_td': 6, 'state': 'MI', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 32, 'shuttle': 4.5, 'broad_jump': 102},\n",
        "            # D3 samples\n",
        "            {'name': 'Elite D3 WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 450, 'senior_avg': 12.5, 'senior_rec': 35, 'senior_td': 5, 'state': 'IL', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 31, 'shuttle': 4.5, 'broad_jump': 98},\n",
        "            {'name': 'Good D3 WR', 'height_inches': 68, 'weight_lbs': 160, 'senior_yds': 380, 'senior_avg': 11.8, 'senior_rec': 32, 'senior_td': 4, 'state': 'PA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 30, 'shuttle': 4.6, 'broad_jump': 96},\n",
        "            # NAIA samples\n",
        "            {'name': 'Elite NAIA WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 500, 'senior_avg': 13.0, 'senior_rec': 38, 'senior_td': 6, 'state': 'KS', 'division': 'NAIA', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 100}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    position = position.lower()\n",
        "    enrich_df = pd.DataFrame(enrich_data.get(position, []))\n",
        "    if not enrich_df.empty:\n",
        "        enrich_df.columns = enrich_df.columns.str.strip().str.lower()\n",
        "        df = pd.concat([df, enrich_df], ignore_index=True)\n",
        "        print(f\"Added {len(enrich_df)} enhanced synthetic samples for {position.upper()}\")\n",
        "    \n",
        "    # Add hoops_vert feature for multi-sport athletes\n",
        "    df['hoops_vert'] = df.get('vertical_jump', 32)\n",
        "    \n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 74,
              "statement_ids": [
                74
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.679984Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:21.2407968Z",
              "execution_finish_time": "2025-08-04T00:24:21.8623887Z",
              "parent_msg_id": "acb6bd87-41d4-43ea-b4ff-09ada4541fc3"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 74, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 354,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Enrich Data (adds baseline FCS/D2/D3 to all positions)\n",
        "def enrich_data(df, position, year=2025):\n",
        "    enrich_data = {\n",
        "        'qb': [\n",
        "            {'name': 'Sample FCS QB', 'height_inches': 72, 'weight_lbs': 195, 'senior_ypg': 180, 'senior_tds': 20, 'senior_comp_pct': 60, 'state': 'CA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 28, 'shuttle': 4.5, 'broad_jump': 105},\n",
        "            {'name': 'Sample D2 QB', 'height_inches': 71, 'weight_lbs': 185, 'senior_ypg': 140, 'senior_tds': 15, 'senior_comp_pct': 55, 'state': 'FL', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 26, 'shuttle': 4.6, 'broad_jump': 100},\n",
        "            {'name': 'Sample D3 QB', 'height_inches': 70, 'weight_lbs': 175, 'senior_ypg': 100, 'senior_tds': 10, 'senior_comp_pct': 50, 'state': 'GA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 5.0, 'vertical_jump': 24, 'shuttle': 4.7, 'broad_jump': 95}\n",
        "        ],\n",
        "        'rb': [\n",
        "            {'name': 'Sample FCS RB', 'height_inches': 68, 'weight_lbs': 185, 'senior_ypg': 110, 'senior_tds': 15, 'senior_ypc': 4.5, 'state': 'TX', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 30, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            {'name': 'Sample D2 RB', 'height_inches': 67, 'weight_lbs': 175, 'senior_ypg': 90, 'senior_tds': 10, 'senior_ypc': 4.0, 'state': 'FL', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 28, 'shuttle': 4.5, 'broad_jump': 105},\n",
        "            {'name': 'Sample D3 RB', 'height_inches': 66, 'weight_lbs': 165, 'senior_ypg': 70, 'senior_tds': 8, 'senior_ypc': 3.5, 'state': 'CA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 26, 'shuttle': 4.6, 'broad_jump': 100}\n",
        "        ],\n",
        "        'wr': [\n",
        "            {'name': 'Sample FCS WR', 'height_inches': 70, 'weight_lbs': 175, 'senior_yds': 800, 'senior_avg': 15, 'senior_rec': 50, 'senior_td': 8, 'state': 'TX', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            {'name': 'Sample D2 WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 600, 'senior_avg': 13, 'senior_rec': 40, 'senior_td': 6, 'state': 'FL', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 30, 'shuttle': 4.5, 'broad_jump': 105},\n",
        "            {'name': 'Sample D3 WR', 'height_inches': 68, 'weight_lbs': 160, 'senior_yds': 400, 'senior_avg': 11, 'senior_rec': 30, 'senior_td': 4, 'state': 'CA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 28, 'shuttle': 4.6, 'broad_jump': 100}\n",
        "        ]\n",
        "    }\n",
        "    position = position.lower()\n",
        "    enrich_df = pd.DataFrame(enrich_data.get(position, []))\n",
        "    enrich_df.columns = enrich_df.columns.str.strip().str.lower()\n",
        "    df = pd.concat([df, enrich_df], ignore_index=True)\n",
        "    df['hoops_vert'] = df.get('vertical_jump', 32)\n",
        "    return df\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 75,
              "statement_ids": [
                75
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.6892073Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:21.8731237Z",
              "execution_finish_time": "2025-08-04T00:24:22.1397779Z",
              "parent_msg_id": "9baac53f-b0e6-48c5-9855-b71707708042"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 75, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 355,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Cell 4: Enhanced Preprocessing with Intelligent Imputation, Embeddings, and Advanced Features\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def create_state_embeddings(df):\n",
        "    \"\"\"Create state embeddings for talent hotbed representation\"\"\"\n",
        "    # DUPLICATE PREVENTION: Check if state features already exist\n",
        "    if 'state_talent_score' in df.columns:\n",
        "        print(\"    State embeddings already exist - skipping creation to prevent duplicates\")\n",
        "        return df\n",
        "    \n",
        "    # Define state talent tiers based on recruiting density and college production\n",
        "    state_tiers = {\n",
        "        'TX': 'tier_1',  # Elite talent hotbeds\n",
        "        'FL': 'tier_1',\n",
        "        'CA': 'tier_1', \n",
        "        'GA': 'tier_1',\n",
        "        \n",
        "        'OH': 'tier_2',  # Strong talent states\n",
        "        'PA': 'tier_2',\n",
        "        'NC': 'tier_2',\n",
        "        'VA': 'tier_2',\n",
        "        'MI': 'tier_2',\n",
        "        'IL': 'tier_2',\n",
        "        'LA': 'tier_2',\n",
        "        'AL': 'tier_2',\n",
        "        'TN': 'tier_2',\n",
        "        'SC': 'tier_2',\n",
        "        'AZ': 'tier_2',\n",
        "        'NJ': 'tier_2',\n",
        "        'MD': 'tier_2',\n",
        "        \n",
        "        'IN': 'tier_3',  # Moderate talent states\n",
        "        'MO': 'tier_3',\n",
        "        'WI': 'tier_3',\n",
        "        'MN': 'tier_3',\n",
        "        'IA': 'tier_3',\n",
        "        'KY': 'tier_3',\n",
        "        'OK': 'tier_3',\n",
        "        'AR': 'tier_3',\n",
        "        'MS': 'tier_3',\n",
        "        'KS': 'tier_3',\n",
        "        'CO': 'tier_3',\n",
        "        'OR': 'tier_3',\n",
        "        'WA': 'tier_3',\n",
        "        'CT': 'tier_3',\n",
        "        'NV': 'tier_3',\n",
        "        'UT': 'tier_3'\n",
        "    }\n",
        "    \n",
        "    # Create state embeddings using simple numeric encoding (avoid object columns)\n",
        "    df['state_talent_score'] = df['state'].str.upper().map({\n",
        "        'TX': 4, 'FL': 4, 'CA': 4, 'GA': 4,  # Elite\n",
        "        'OH': 3, 'PA': 3, 'NC': 3, 'VA': 3, 'MI': 3, 'IL': 3, 'LA': 3, 'AL': 3, 'TN': 3, 'SC': 3, 'AZ': 3, 'NJ': 3, 'MD': 3,  # Strong\n",
        "        'IN': 2, 'MO': 2, 'WI': 2, 'MN': 2, 'IA': 2, 'KY': 2, 'OK': 2, 'AR': 2, 'MS': 2, 'KS': 2, 'CO': 2, 'OR': 2, 'WA': 2, 'CT': 2, 'NV': 2, 'UT': 2  # Moderate\n",
        "    }).fillna(1).astype(int)  # Default for other states, ensure int type\n",
        "    \n",
        "    # Create binary indicators for state tiers (avoid object columns)\n",
        "    df['state_tier_1'] = (df['state_talent_score'] == 4).astype(int)  # Elite states\n",
        "    df['state_tier_2'] = (df['state_talent_score'] == 3).astype(int)  # Strong states\n",
        "    df['state_tier_3'] = (df['state_talent_score'] == 2).astype(int)  # Moderate states\n",
        "    df['state_tier_4'] = (df['state_talent_score'] == 1).astype(int)  # Other states\n",
        "    \n",
        "    return df\n",
        "\n",
        "def enhanced_feature_engineering(df, position):\n",
        "    \"\"\"Enhanced feature engineering with interaction terms and advanced metrics\"\"\"\n",
        "    df = df.copy()\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # DUPLICATE PREVENTION: Check if enhanced features already exist\n",
        "    if 'state_eff' in df.columns or 'bmi_ypg' in df.columns:\n",
        "        print(f\"    Enhanced features already exist - removing duplicates and reprocessing\")\n",
        "        # Remove existing enhanced features to prevent conflicts\n",
        "        enhanced_cols = ['state_eff', 'bmi_ypg', 'height_traj', 'speed_power_ratio', 'combine_confidence']\n",
        "        for col in enhanced_cols:\n",
        "            if col in df.columns:\n",
        "                df = df.drop(columns=[col])\n",
        "\n",
        "    # Ensure essential columns exist with intelligent defaults\n",
        "    essential_cols = ['height_inches', 'weight_lbs', 'position', 'division', 'state']\n",
        "    for col in essential_cols:\n",
        "        if col not in df.columns:\n",
        "            if col == 'height_inches':\n",
        "                df[col] = 70\n",
        "            elif col == 'weight_lbs':\n",
        "                df[col] = 180\n",
        "            elif col == 'position':\n",
        "                df[col] = position\n",
        "            elif col == 'division':\n",
        "                df[col] = 'D3'\n",
        "            elif col == 'state':\n",
        "                df[col] = 'ZZ'\n",
        "\n",
        "    # Apply intelligent combine imputation\n",
        "    df = intelligent_combine_imputation(df, position)\n",
        "    \n",
        "    # Create state embeddings (now returns only numeric columns)\n",
        "    df = create_state_embeddings(df)\n",
        "    \n",
        "    # Position-aware engineered features\n",
        "    df['games'] = 12\n",
        "    if 'senior_rec' in df.columns:\n",
        "        wr_mask = df['position'].str.lower() == 'wr'\n",
        "        df.loc[wr_mask, 'games'] = df.loc[wr_mask, 'senior_rec'].replace(0, np.nan).fillna(12).clip(8, 15)\n",
        "    if 'senior_yds' in df.columns and 'senior_ypg' in df.columns:\n",
        "        rb_qb_mask = df['position'].str.lower().isin(['rb', 'qb'])\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            games_calc = df.loc[rb_qb_mask, 'senior_yds'] / df.loc[rb_qb_mask, 'senior_ypg']\n",
        "            games_calc = games_calc.replace([np.inf, -np.inf], np.nan).fillna(12).clip(8, 15)\n",
        "            df.loc[rb_qb_mask, 'games'] = games_calc\n",
        "\n",
        "    # Enhanced all_purpose_game calculation for RBs\n",
        "    rb_mask = df['position'].str.lower() == 'rb'\n",
        "    if 'senior_yds' in df.columns:\n",
        "        if 'senior_rec_yds' in df.columns:\n",
        "            df.loc[rb_mask, 'all_purpose_game'] = (\n",
        "                df.loc[rb_mask, 'senior_yds'] + df.loc[rb_mask, 'senior_rec_yds']\n",
        "            ) / df.loc[rb_mask, 'games']\n",
        "        else:\n",
        "            df.loc[rb_mask, 'all_purpose_game'] = df.loc[rb_mask, 'senior_yds'] / df.loc[rb_mask, 'games']\n",
        "    else:\n",
        "        df['all_purpose_game'] = df.get('ypg', 0) + df.get('rec_ypg', 0)\n",
        "\n",
        "    # Derived per-game stats\n",
        "    df['rec_ypg'] = 0.0\n",
        "    df['ypg'] = 0.0\n",
        "    df['tds_game'] = 0.0\n",
        "    df['td_game'] = 0.0\n",
        "    \n",
        "    if 'senior_yds' in df.columns:\n",
        "        wr_mask = df['position'].str.lower() == 'wr'\n",
        "        df.loc[wr_mask, 'rec_ypg'] = df.loc[wr_mask, 'senior_yds'] / df.loc[wr_mask, 'games']\n",
        "        rb_qb_mask = df['position'].str.lower().isin(['rb', 'qb'])\n",
        "        df.loc[rb_qb_mask, 'ypg'] = df.loc[rb_qb_mask, 'senior_yds'] / df.loc[rb_qb_mask, 'games']\n",
        "    if 'senior_td' in df.columns:\n",
        "        wr_mask = df['position'].str.lower() == 'wr'\n",
        "        df.loc[wr_mask, 'tds_game'] = df.loc[wr_mask, 'senior_td'] / df.loc[wr_mask, 'games']\n",
        "        rb_qb_mask = df['position'].str.lower().isin(['rb', 'qb'])\n",
        "        df.loc[rb_qb_mask, 'td_game'] = df.loc[rb_qb_mask, 'senior_td'] / df.loc[rb_qb_mask, 'games']\n",
        "\n",
        "    # Trajectory calculation\n",
        "    if 'senior_ypg' in df.columns and 'junior_ypg' in df.columns:\n",
        "        df['trajectory'] = np.maximum(df['senior_ypg'] - df['junior_ypg'], 0)\n",
        "    else:\n",
        "        df['trajectory'] = 0.0\n",
        "\n",
        "    # Core engineered features (ensure numeric types)\n",
        "    df['bmi'] = ((df['weight_lbs'] / (df['height_inches'] ** 2)) * 703).astype(float)\n",
        "    df['eff_ratio'] = (df.get('senior_tds', 0) / (df.get('senior_ypg', 1) + 1e-6)).astype(float)\n",
        "    df['ath_power'] = (df.get('vertical_jump', 0) * df.get('broad_jump', 0)).astype(float)\n",
        "    df['is_strong_state'] = df['state'].str.upper().isin(['TX', 'FL', 'CA', 'GA']).astype(int)\n",
        "\n",
        "    # ENHANCED INTERACTION FEATURES (ensure numeric types)\n",
        "    \n",
        "    # BMI × YPG (power efficiency)\n",
        "    primary_ypg = df.get('senior_ypg', df.get('ypg', df.get('rec_ypg', 0)))\n",
        "    df['bmi_ypg'] = (df['bmi'] * primary_ypg).astype(float)\n",
        "    \n",
        "    # Height × Trajectory (growth potential with size)\n",
        "    df['height_traj'] = (df['height_inches'] * df['trajectory']).astype(float)\n",
        "    \n",
        "    # State efficiency (talent hotbed × efficiency)\n",
        "    df['state_eff'] = (df['state_talent_score'] * df['eff_ratio']).astype(float)\n",
        "    \n",
        "    # Speed-power ratio (athleticism efficiency)\n",
        "    df['speed_power_ratio'] = (df['ath_power'] / (df['forty_yard_dash'] + 1e-6)).astype(float)\n",
        "    \n",
        "    # Position-specific interaction features\n",
        "    if position.lower() == 'qb':\n",
        "        # Completion percentage × YPG (accuracy under volume)\n",
        "        df['comp_ypg'] = (df.get('senior_comp_pct', 60) * primary_ypg / 100).astype(float)\n",
        "        # Height × Completion % (pocket presence)\n",
        "        df['height_comp'] = (df['height_inches'] * df.get('senior_comp_pct', 60)).astype(float)\n",
        "    elif position.lower() == 'rb':\n",
        "        # YPC × Speed (breakaway ability)\n",
        "        df['ypc_speed'] = (df.get('senior_ypc', 0) * (5.0 - df.get('forty_yard_dash', 4.8))).astype(float)\n",
        "        # Weight × YPC (power running ability)\n",
        "        df['weight_ypc'] = (df['weight_lbs'] * df.get('senior_ypc', 0)).astype(float)\n",
        "    elif position.lower() == 'wr':\n",
        "        # Catch radius (height × vertical)\n",
        "        df['catch_radius'] = (df['height_inches'] * df.get('vertical_jump', 0)).astype(float)\n",
        "        # Speed × YAC (big play ability)\n",
        "        df['speed_yac'] = ((5.0 - df.get('forty_yard_dash', 4.8)) * df.get('senior_avg', 0)).astype(float)\n",
        "\n",
        "    # Combine confidence scores (0-1 based on real vs imputed data)\n",
        "    combine_cols = ['forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump']\n",
        "    imputed_cols = [f'{col}_imputed' for col in combine_cols if f'{col}_imputed' in df.columns]\n",
        "    \n",
        "    if imputed_cols:\n",
        "        df['combine_confidence'] = (1.0 - (df[imputed_cols].sum(axis=1) / len(imputed_cols))).astype(float)\n",
        "    else:\n",
        "        df['combine_confidence'] = 1.0\n",
        "\n",
        "    # Trajectory z-score by position\n",
        "    df['trajectory_z'] = 0.0\n",
        "    for pos in df['position'].unique():\n",
        "        mask = df['position'] == pos\n",
        "        if mask.sum() > 1:\n",
        "            mean_traj = df.loc[mask, 'trajectory'].mean()\n",
        "            std_traj = df.loc[mask, 'trajectory'].std()\n",
        "            if std_traj > 0:\n",
        "                df.loc[mask, 'trajectory_z'] = ((df.loc[mask, 'trajectory'] - mean_traj) / std_traj).astype(float)\n",
        "\n",
        "    # Create position dummies (ensure int type)\n",
        "    position_dummies = pd.get_dummies(df['position'].str.lower(), prefix='pos', dtype=int)\n",
        "    for pos in ['qb', 'rb', 'wr']:\n",
        "        if f'pos_{pos}' not in position_dummies.columns:\n",
        "            position_dummies[f'pos_{pos}'] = 0\n",
        "    df = pd.concat([df, position_dummies], axis=1)\n",
        "\n",
        "    # CRITICAL: Remove duplicate columns after all feature engineering\n",
        "    print(f\"    Before duplicate removal: {df.shape}\")\n",
        "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
        "    print(f\"    After duplicate removal: {df.shape}\")\n",
        "    \n",
        "    # Ensure all numeric columns have proper dtypes for XGBoost\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "        # Convert boolean columns to int\n",
        "        if df[col].dtype == 'bool':\n",
        "            df[col] = df[col].astype(int)\n",
        "\n",
        "    print(f\"Enhanced feature engineering completed for {position.upper()}\")\n",
        "    print(f\"  - Applied intelligent combine imputation\")\n",
        "    print(f\"  - Created state embeddings and talent scores\")\n",
        "    print(f\"  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\")\n",
        "    print(f\"  - Added position-specific features\")\n",
        "    print(f\"  - Calculated combine confidence scores\")\n",
        "    print(f\"  - Applied duplicate column removal\")\n",
        "    print(f\"  - Ensured all columns are XGBoost-compatible (numeric types only)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def preprocess_with_winsorization(df, position):\n",
        "    \"\"\"Compatibility wrapper for enhanced feature engineering\"\"\"\n",
        "    return enhanced_feature_engineering(df, position)\n",
        "\n",
        "def winsorize_and_scale(train_df, test_df, numeric_features):\n",
        "    \"\"\"Legacy wrapper for advanced winsorization and scaling\"\"\"\n",
        "    return advanced_winsorize_and_scale(train_df, test_df, numeric_features)\n",
        "\n",
        "def advanced_winsorize_and_scale(train_df, test_df, numeric_features):\n",
        "    \"\"\"Advanced winsorization and scaling with percentile features\"\"\"\n",
        "    train_processed = train_df.copy()\n",
        "    test_processed = test_df.copy()\n",
        "    \n",
        "    winsorization_log = []\n",
        "    \n",
        "    for feature in numeric_features:\n",
        "        if feature in train_df.columns and train_df[feature].dtype in ['int64', 'float64']:\n",
        "            # Winsorize on training data (1st-99th percentile)\n",
        "            feature_values = train_df[feature].dropna()\n",
        "            \n",
        "            if len(feature_values) > 0:\n",
        "                p1, p99 = np.percentile(feature_values, [1, 99])\n",
        "                \n",
        "                # Apply winsorization to both train and test\n",
        "                train_processed[feature] = np.clip(train_df[feature], p1, p99)\n",
        "                test_processed[feature] = np.clip(test_df[feature], p1, p99)\n",
        "                \n",
        "                # Percentile scaling based on training data\n",
        "                train_values = train_processed[feature].dropna()\n",
        "                if len(train_values) > 0:\n",
        "                    # Create percentile features\n",
        "                    train_processed[f'{feature}_pctile'] = train_processed[feature].apply(\n",
        "                        lambda x: np.percentile(train_values, 100 * (train_values <= x).mean()) if pd.notnull(x) else 50\n",
        "                    ).astype(float)\n",
        "                    test_processed[f'{feature}_pctile'] = test_processed[feature].apply(\n",
        "                        lambda x: np.percentile(train_values, 100 * (train_values <= x).mean()) if pd.notnull(x) else 50\n",
        "                    ).astype(float)\n",
        "                    \n",
        "                    winsorization_log.append(f\"{feature}: [{p1:.2f}, {p99:.2f}]\")\n",
        "    \n",
        "    print(f\"Advanced winsorization applied to {len(winsorization_log)} features\")\n",
        "    for log_entry in winsorization_log[:5]:  # Show first 5\n",
        "        print(f\"  {log_entry}\")\n",
        "    if len(winsorization_log) > 5:\n",
        "        print(f\"  ... and {len(winsorization_log) - 5} more features\")\n",
        "    \n",
        "    # DUPLICATE PREVENTION: Remove any duplicate columns created during winsorization\n",
        "    print(\"Removing duplicates after winsorization...\")\n",
        "    train_processed = train_processed.loc[:, ~train_processed.columns.duplicated(keep='first')]\n",
        "    test_processed = test_processed.loc[:, ~test_processed.columns.duplicated(keep='first')]\n",
        "    \n",
        "    return train_processed, test_processed\n",
        "\n",
        "# Legacy support for older function names\n",
        "def load_base_csv(position):\n",
        "    \"\"\"Legacy wrapper for enhanced data loading\"\"\"\n",
        "    return load_base_csv_enhanced(position)\n",
        "\n",
        "def enrich_data(df, position, year=2025):\n",
        "    \"\"\"Legacy wrapper for enhanced data enrichment\"\"\"\n",
        "    return enrich_data_enhanced(df, position, year)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 76,
              "statement_ids": [
                76
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.7473566Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.1514279Z",
              "execution_finish_time": "2025-08-04T00:24:22.3772302Z",
              "parent_msg_id": "d17df285-08f7-4cbd-8c72-0160da623764"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 76, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 356,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Tiers & Tier Base Assignments (robust to lower-case columns)\n",
        "tiers_qb = {\n",
        "    'Power 5': {'base': 90, 'ypg_min': 250, 'height_min': 74, 'height_max': 78, 'weight_min': 200, 'weight_max': 240,\n",
        "                '40_min': 4.6, '40_max': 4.9, 'vertical_min': 30, 'vertical_max': 34, 'broad_min': 108, 'shuttle_max': 4.5},\n",
        "    'FCS': {'base': 70, 'ypg_min': 200, 'height_min': 72, 'height_max': 76, 'weight_min': 190, 'weight_max': 220,\n",
        "            '40_min': 4.7, '40_max': 5.0, 'vertical_min': 28, 'vertical_max': 32, 'broad_min': 102, 'shuttle_max': 4.6},\n",
        "    'D2': {'base': 50, 'ypg_min': 150, 'height_min': 71, 'height_max': 74, 'weight_min': 180, 'weight_max': 210,\n",
        "           '40_min': 4.8, '40_max': 5.1, 'vertical_min': 26, 'vertical_max': 30, 'broad_min': 96, 'shuttle_max': 4.7},\n",
        "    'D3/NAIA': {'base': 30, 'ypg_min': 0, 'height_min': 70, 'height_max': 999, 'weight_min': 170, 'weight_max': 999,\n",
        "                '40_min': 4.9, '40_max': 999, 'vertical_min': 24, 'vertical_max': 999, 'broad_min': 90, 'shuttle_max': 999}\n",
        "}\n",
        "tiers_rb = {\n",
        "    'Power 5': {'base': 90, 'ypg_min': 150, 'height_min': 69, 'height_max': 74, 'weight_min': 190, 'weight_max': 230,\n",
        "                '40_min': 4.2, '40_max': 4.4, 'vertical_min': 34, 'vertical_max': 36, 'broad_min': 120, 'shuttle_max': 4.2},\n",
        "    'FCS': {'base': 70, 'ypg_min': 120, 'height_min': 68, 'height_max': 73, 'weight_min': 180, 'weight_max': 220,\n",
        "            '40_min': 4.3, '40_max': 4.5, 'vertical_min': 32, 'vertical_max': 34, 'broad_min': 110, 'shuttle_max': 4.3},\n",
        "    'D2': {'base': 50, 'ypg_min': 90, 'height_min': 67, 'height_max': 72, 'weight_min': 170, 'weight_max': 210,\n",
        "           '40_min': 4.4, '40_max': 4.6, 'vertical_min': 31, 'vertical_max': 33, 'broad_min': 100, 'shuttle_max': 4.4},\n",
        "    'D3/NAIA': {'base': 30, 'ypg_min': 0, 'height_min': 66, 'height_max': 999, 'weight_min': 160, 'weight_max': 999,\n",
        "                '40_min': 4.5, '40_max': 4.7, 'vertical_min': 30, 'vertical_max': 32, 'broad_min': 90, 'shuttle_max': 4.5}\n",
        "}\n",
        "tiers_wr = {\n",
        "    'Power 5': {'base': 90, 'rec_ypg_min': 100, 'height_min': 71, 'height_max': 75, 'weight_min': 180, 'weight_max': 210,\n",
        "                '40_min': 4.4, '40_max': 4.6, 'vertical_min': 34, 'vertical_max': 36, 'broad_min': 120, 'shuttle_max': 4.3},\n",
        "    'FCS': {'base': 70, 'rec_ypg_min': 80, 'height_min': 70, 'height_max': 74, 'weight_min': 170, 'weight_max': 200,\n",
        "            '40_min': 4.5, '40_max': 4.7, 'vertical_min': 32, 'vertical_max': 35, 'broad_min': 110, 'shuttle_max': 4.4},\n",
        "    'D2': {'base': 50, 'rec_ypg_min': 60, 'height_min': 69, 'height_max': 73, 'weight_min': 165, 'weight_max': 195,\n",
        "           '40_min': 4.6, '40_max': 4.8, 'vertical_min': 30, 'vertical_max': 33, 'broad_min': 100, 'shuttle_max': 4.5},\n",
        "    'D3/NAIA': {'base': 30, 'rec_ypg_min': 0, 'height_min': 68, 'height_max': 999, 'weight_min': 160, 'weight_max': 999,\n",
        "                '40_min': 4.7, '40_max': 5.0, 'vertical_min': 28, 'vertical_max': 31, 'broad_min': 90, 'shuttle_max': 4.6}\n",
        "}\n",
        "\n",
        "tiers = {'qb': tiers_qb, 'rb': tiers_rb, 'wr': tiers_wr}\n",
        "\n",
        "def safe_get(row, key, default):\n",
        "    \"\"\"Safely get value from row, handling None values.\"\"\"\n",
        "    value = row.get(key, default)\n",
        "    return default if value is None else value\n",
        "\n",
        "def assign_tier_base(row, position):\n",
        "    tiers_pos = tiers.get(position, tiers['qb'])\n",
        "    for name, rules in sorted(tiers_pos.items(), key=lambda x: x[1]['base'], reverse=True):\n",
        "        checks = []\n",
        "        if position == 'wr':\n",
        "            checks.append(safe_get(row, 'rec_ypg', 0) >= rules.get('rec_ypg_min', 0))\n",
        "        elif position == 'qb':\n",
        "            checks.append(safe_get(row, 'senior_ypg', 0) >= rules['ypg_min'])\n",
        "        elif position == 'rb':\n",
        "            checks.append(safe_get(row, 'ypg', 0) >= rules['ypg_min'])\n",
        "        checks += [\n",
        "            rules['height_min'] <= safe_get(row, 'height_inches', 0) <= rules['height_max'],\n",
        "            rules['weight_min'] <= safe_get(row, 'weight_lbs', 0) <= rules['weight_max'],\n",
        "            rules['40_min'] <= safe_get(row, 'forty_yard_dash', 5.0) <= rules['40_max'],\n",
        "            (rules['vertical_min'] - 1) <= safe_get(row, 'vertical_jump', 0) <= (rules['vertical_max'] + 1),\n",
        "            safe_get(row, 'shuttle', 5.0) <= rules['shuttle_max'],\n",
        "            safe_get(row, 'broad_jump', 0) >= rules['broad_min']\n",
        "        ]\n",
        "        if sum(checks) >= len(checks) * 0.6:\n",
        "            return rules['base'], name\n",
        "    return tiers_pos['D3/NAIA']['base'], 'D3/NAIA'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 77,
              "statement_ids": [
                77
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.7861784Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.391028Z",
              "execution_finish_time": "2025-08-04T00:24:22.6323973Z",
              "parent_msg_id": "8595f862-2717-4b02-98b4-974e71bf1d2d"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 77, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 357,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Meta-Score: Performance, Versatility, Athleticism, Bonus, Rule Score\n",
        "\n",
        "def safe_percentileofscore(series, value):\n",
        "    \"\"\"Safely compute percentile score, handling missing columns or empty series.\"\"\"\n",
        "    if series is None or len(series.dropna()) == 0:\n",
        "        return 0\n",
        "    return percentileofscore(series.dropna(), value if value is not None else 0)\n",
        "\n",
        "def safe_get(row, key, default):\n",
        "    \"\"\"Safely get value from row, handling None values.\"\"\"\n",
        "    value = row.get(key, default)\n",
        "    return default if value is None else value\n",
        "\n",
        "def compute_performance(df, row, position):\n",
        "    if position == 'qb':\n",
        "        ypg_pct = safe_percentileofscore(df.get('senior_ypg'), safe_get(row, 'senior_ypg', 0))\n",
        "        td_pct = safe_percentileofscore(df.get('senior_tds'), safe_get(row, 'senior_tds', 0))\n",
        "        comp_pct = safe_percentileofscore(df.get('senior_comp_pct'), safe_get(row, 'senior_comp_pct', 0))\n",
        "        traj_pct = safe_percentileofscore(df.get('trajectory'), safe_get(row, 'trajectory', 0))\n",
        "        return (0.4 * ypg_pct + 0.3 * td_pct + 0.2 * comp_pct + 0.1 * traj_pct + 0.1 * safe_get(row, 'trajectory_z', 0)) * 0.35\n",
        "    elif position == 'rb':\n",
        "        ypg_pct = safe_percentileofscore(df.get('ypg'), safe_get(row, 'ypg', 0))\n",
        "        td_pct = safe_percentileofscore(df.get('td_game'), safe_get(row, 'td_game', 0))\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_ypc'), safe_get(row, 'senior_ypc', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        return (0.4 * ypg_pct + 0.3 * td_pct + 0.2 * ypc_pct + 0.1 * rec_pct + 0.1 * safe_get(row, 'eff_ratio', 0)) * 0.35\n",
        "    elif position == 'wr':\n",
        "        ypg_pct = safe_percentileofscore(df.get('rec_ypg'), safe_get(row, 'rec_ypg', 0))\n",
        "        td_pct = safe_percentileofscore(df.get('tds_game'), safe_get(row, 'tds_game', 0))\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_avg'), safe_get(row, 'senior_avg', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        return (0.4 * ypg_pct + 0.3 * td_pct + 0.2 * ypc_pct + 0.1 * rec_pct + 0.1 * safe_get(row, 'eff_ratio', 0)) * 0.35\n",
        "    return 0\n",
        "\n",
        "def compute_versatility(df, row, position):\n",
        "    if position == 'qb':\n",
        "        comp_pct = safe_percentileofscore(df.get('senior_comp_pct'), safe_get(row, 'senior_comp_pct', 0))\n",
        "        speed_pct = 100 - safe_percentileofscore(df.get('forty_yard_dash'), safe_get(row, 'forty_yard_dash', 5.0))\n",
        "        return (0.5 * comp_pct + 0.5 * speed_pct) * 0.35\n",
        "    elif position == 'rb':\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_ypc'), safe_get(row, 'senior_ypc', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        ap_pct = safe_percentileofscore(df.get('all_purpose_game'), safe_get(row, 'all_purpose_game', 0))\n",
        "        return (0.4 * ypc_pct + 0.3 * rec_pct + 0.3 * ap_pct) * 0.4\n",
        "    elif position == 'wr':\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_avg'), safe_get(row, 'senior_avg', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        rush_pct = safe_percentileofscore(df.get('senior_rush_yds'), safe_get(row, 'senior_rush_yds', 0))\n",
        "        return (0.5 * ypc_pct + 0.3 * rec_pct + 0.2 * rush_pct) * 0.4\n",
        "    return 0\n",
        "\n",
        "def compute_athleticism(df, row, position):\n",
        "    f_pct = 100 - safe_percentileofscore(df.get('forty_yard_dash'), safe_get(row, 'forty_yard_dash', 5.0))\n",
        "    v_pct = safe_percentileofscore(df.get('vertical_jump'), safe_get(row, 'vertical_jump', 0))\n",
        "    s_pct = 100 - safe_percentileofscore(df.get('shuttle'), safe_get(row, 'shuttle', 5.0))\n",
        "    b_pct = safe_percentileofscore(df.get('broad_jump'), safe_get(row, 'broad_jump', 0))\n",
        "    return (0.3 * f_pct + 0.3 * v_pct + 0.2 * s_pct + 0.2 * b_pct) * 0.25\n",
        "\n",
        "def compute_bonus(row, position):\n",
        "    b = 0\n",
        "    th_40 = 4.7 if position == 'qb' else 4.5\n",
        "    th_sh = 4.4 if position == 'qb' else 4.3\n",
        "    if safe_get(row, 'forty_yard_dash', np.nan) < th_40: b += 10\n",
        "    if safe_get(row, 'shuttle', np.nan) < th_sh: b += 5\n",
        "    if safe_get(row, 'trajectory_z', 0) > 1: b += 5\n",
        "    if safe_get(row, 'is_strong_state', 0): b += 3\n",
        "    if safe_get(row, 'hoops_vert', 0) > 35: b += 4\n",
        "    pctile_cols = [c for c in row.index if '_pos_pctile' in c]\n",
        "    if sum(safe_get(row, c, 0) > 0.9 for c in pctile_cols) >= 3: b += 7\n",
        "    return b\n",
        "\n",
        "    \n",
        "def compute_rule_score(df, position):\n",
        "    # Drop all-NaN rows and those missing 'position'\n",
        "    df = df.dropna(how='all')\n",
        "    df = df[df['position'].notnull()]\n",
        "    results = []\n",
        "    tiers_used = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if not isinstance(row, pd.Series):\n",
        "            continue\n",
        "        pos = str(row.get('position', position)).lower()\n",
        "        base, tier_name = assign_tier_base(row, pos)\n",
        "        bonus = compute_bonus(row, pos)\n",
        "        perf = compute_performance(df, row, pos)\n",
        "        vers = compute_versatility(df, row, pos)\n",
        "        ath = compute_athleticism(df, row, pos)\n",
        "        multiplier = safe_get(row, 'multiplier', 1.0)\n",
        "        score = (base * 0.6 + (perf + vers + ath) * 0.4) * (1 + bonus / 100) * multiplier\n",
        "        score = np.clip(score, 0, 100)\n",
        "        results.append(score)\n",
        "        tiers_used.append(tier_name)\n",
        "    df = df.copy()\n",
        "    df['rule_score'] = results\n",
        "    df['rule_score_tier'] = tiers_used\n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 78,
              "statement_ids": [
                78
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.8453549Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.6435214Z",
              "execution_finish_time": "2025-08-04T00:24:22.8749747Z",
              "parent_msg_id": "9041d1e6-1e95-4ff1-b416-f7df2962c31b"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 78, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 358,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Enhanced Pipeline Usage Example with New Functions\n",
        "\n",
        "# 1. Load Data with Enhanced Functions\n",
        "df_qb = load_base_csv_enhanced('qb')\n",
        "df_rb = load_base_csv_enhanced('rb')\n",
        "df_wr = load_base_csv_enhanced('wr')\n",
        "\n",
        "# 2. Enrich data for balanced division representation\n",
        "df_qb = enrich_data_enhanced(df_qb, 'qb')\n",
        "df_rb = enrich_data_enhanced(df_rb, 'rb')\n",
        "df_wr = enrich_data_enhanced(df_wr, 'wr')\n",
        "\n",
        "# 3. Concatenate all positions for multi-position modeling\n",
        "combined_df = pd.concat([df_qb, df_rb, df_wr], ignore_index=True)\n",
        "\n",
        "# 4. Enhanced division normalization\n",
        "combined_df['division_normalized'] = combined_df['division'].str.strip().str.upper()\n",
        "division_map = {'POWER 5': 3, 'FCS': 2, 'D2': 1, 'D3': 0, 'NAIA': 0}\n",
        "combined_df['division_num'] = combined_df['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "# Fix any unmapped divisions\n",
        "unmapped_mask = combined_df['division_num'] == -1\n",
        "if unmapped_mask.any():\n",
        "    print(f\"Fixing {unmapped_mask.sum()} unmapped division values...\")\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('POWER|P5|FBS', na=False), 'division_num'] = 3\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('FCS', na=False), 'division_num'] = 2\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D2|DIV 2|DIVISION 2', na=False), 'division_num'] = 1\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D3|DIV 3|DIVISION 3|NAIA', na=False), 'division_num'] = 0\n",
        "\n",
        "print(\"Enhanced class distribution:\")\n",
        "print(combined_df['division_num'].value_counts().sort_index())\n",
        "\n",
        "# 5. Apply enhanced feature engineering\n",
        "combined_df = enhanced_feature_engineering(combined_df, 'multi')\n",
        "\n",
        "# 6. Compute rule score\n",
        "combined_df = compute_rule_score(combined_df, 'multi')\n",
        "\n",
        "# 7. Output: ready for accuracy evaluation and model training!\n",
        "print(f\"\\nDataset ready with {len(combined_df)} total samples and enhanced features\")\n",
        "combined_df[['name', 'position', 'division_normalized', 'division_num', 'rule_score', 'combine_confidence']].head()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 79,
              "statement_ids": [
                79
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.8884922Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.8850282Z",
              "execution_finish_time": "2025-08-04T00:24:37.614558Z",
              "parent_msg_id": "5f7c71da-47a7-4c0b-a5ff-1a80752c2202"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 79, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 220 rows for QB\nUnique divisions for QB: ['Power 5' 'FCS' 'D3' 'D2' 'NAIA']\nLoaded 194 rows for RB\nUnique divisions for RB: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nLoaded 158 rows for WR\nUnique divisions for WR: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for QB\nAdded 10 enhanced synthetic samples for RB\nAdded 10 enhanced synthetic samples for WR\nEnhanced class distribution:\ndivision_num\n0    150\n1    105\n2    107\n3    240\nName: count, dtype: int64\nCombine imputation for MULTI:\n  Imputing 525 missing forty_yard_dash values\n    POWER 5: 201 values from N(4.75, 0.08)\n    FCS: 85 values from N(4.85, 0.07)\n    D3: 61 values from N(5.10, 0.10)\n    D2: 99 values from N(4.95, 0.07)\n    NAIA: 79 values from N(5.00, 0.10)\n  Imputing 525 missing vertical_jump values\n    POWER 5: 201 values from N(32.00, 1.00)\n    FCS: 85 values from N(30.00, 1.00)\n    D3: 61 values from N(26.00, 1.00)\n    D2: 99 values from N(28.00, 1.00)\n    NAIA: 79 values from N(27.00, 1.00)\n  Imputing 538 missing shuttle values\n    POWER 5: 201 values from N(4.45, 0.07)\n    FCS: 98 values from N(4.55, 0.07)\n    D3: 61 values from N(4.75, 0.08)\n    D2: 99 values from N(4.65, 0.07)\n    NAIA: 79 values from N(4.65, 0.07)\n  Imputing 558 missing broad_jump values\n    POWER 5: 221 values from N(113.00, 2.50)\n    FCS: 98 values from N(107.00, 2.50)\n    D3: 61 values from N(95.00, 2.50)\n    D2: 99 values from N(101.00, 2.50)\n    NAIA: 79 values from N(97.00, 2.50)\n    Before duplicate removal: (602, 96)\n    After duplicate removal: (602, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\n\nDataset ready with 572 total samples and enhanced features\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "  name position division_normalized  division_num  rule_score  \\\n0  NaN       qb             POWER 5             3   70.016632   \n1  NaN       qb             POWER 5             3   74.422153   \n2  NaN       qb             POWER 5             3   60.169521   \n3  NaN       qb             POWER 5             3   55.080066   \n4  NaN       qb             POWER 5             3   75.674207   \n\n   combine_confidence  \n0                 0.0  \n1                 0.0  \n2                 0.0  \n3                 0.0  \n4                 0.0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>position</th>\n      <th>division_normalized</th>\n      <th>division_num</th>\n      <th>rule_score</th>\n      <th>combine_confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>70.016632</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>74.422153</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>60.169521</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>55.080066</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>75.674207</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 359,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "def train_and_evaluate(train_df, features):\n",
        "    X = train_df[features].fillna(0)\n",
        "    y = train_df['Division_Num']\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        eval_metric='mlogloss',\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    return model\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 80,
              "statement_ids": [
                80
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.9796202Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:37.6253623Z",
              "execution_finish_time": "2025-08-04T00:24:37.846605Z",
              "parent_msg_id": "7860c6ec-7298-4ae7-9375-1f975ed6f4cc"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 80, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 360,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPREHENSIVE ACCURACY BOOST - FIXED DUPLICATE HANDLING\n",
        "# Target: 80%+ exact accuracy with proper duplicate prevention\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE ACCURACY BOOST WITH DUPLICATE PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Load and combine data using enhanced functions\n",
        "print(\"\\nSTEP 1: ENHANCED DATA LOADING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "positions = ['qb', 'rb', 'wr']\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for pos in positions:\n",
        "    df = load_base_csv_enhanced(pos)\n",
        "    df = enrich_data_enhanced(df, pos)\n",
        "    df['position'] = pos.lower()\n",
        "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "    # Remove duplicates after each concatenation\n",
        "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]\n",
        "\n",
        "# Division normalization\n",
        "combined_df['division_normalized'] = combined_df['division'].str.strip().str.upper()\n",
        "division_map = {'POWER 5': 3, 'FCS': 2, 'D2': 1, 'D3': 0, 'NAIA': 0}\n",
        "combined_df['division_num'] = combined_df['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "print(f\"Total samples: {len(combined_df)}\")\n",
        "print(f\"Class distribution: {dict(combined_df['division_num'].value_counts().sort_index())}\")\n",
        "\n",
        "# STEP 2: Train/Test Split\n",
        "print(\"\\nSTEP 2: TRAIN/TEST SPLIT\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "try:\n",
        "    train_df, test_df = train_test_split(\n",
        "        combined_df, \n",
        "        test_size=0.15, \n",
        "        stratify=combined_df['division_num'], \n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"✓ Stratified split: Train={len(train_df)}, Test={len(test_df)}\")\n",
        "    use_full_dataset = False\n",
        "except:\n",
        "    print(\"⚠ Stratified split failed - using full dataset\")\n",
        "    train_df = test_df = combined_df.copy()\n",
        "    use_full_dataset = True\n",
        "\n",
        "# STEP 3: Enhanced Feature Engineering with AGGRESSIVE duplicate prevention\n",
        "print(\"\\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH DUPLICATE PREVENTION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "def comprehensive_duplicate_removal(df, step_name=\"\"):\n",
        "    \"\"\"Aggressively remove all duplicate columns at every step\"\"\"\n",
        "    print(f\"  {step_name} - Before: {df.shape}\")\n",
        "    \n",
        "    # Method 1: Remove exact duplicate column names\n",
        "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
        "    \n",
        "    # Method 2: Check for any remaining duplicates and handle them\n",
        "    duplicate_cols = df.columns[df.columns.duplicated()].unique()\n",
        "    if len(duplicate_cols) > 0:\n",
        "        print(f\"    Found {len(duplicate_cols)} remaining duplicates: {list(duplicate_cols)}\")\n",
        "        for dup_col in duplicate_cols:\n",
        "            # Keep only the first occurrence\n",
        "            dup_indices = df.columns.get_loc(dup_col)\n",
        "            if hasattr(dup_indices, '__iter__'):\n",
        "                # Multiple occurrences - drop all but first\n",
        "                cols_to_drop = [df.columns[i] for i in dup_indices[1:]]\n",
        "                df = df.drop(columns=cols_to_drop)\n",
        "    \n",
        "    print(f\"  {step_name} - After: {df.shape}\")\n",
        "    \n",
        "    # Final verification\n",
        "    if df.columns.duplicated().any():\n",
        "        print(f\"    ERROR: Still have duplicates!\")\n",
        "        remaining_dups = df.columns[df.columns.duplicated()].unique()\n",
        "        print(f\"    Remaining: {list(remaining_dups)}\")\n",
        "        # Nuclear option: rename duplicates\n",
        "        df.columns = [f\"{col}_{i}\" if df.columns.tolist().count(col) > 1 and df.columns.tolist()[:i+1].count(col) > 1 \n",
        "                     else col for i, col in enumerate(df.columns)]\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply enhanced feature engineering\n",
        "train_df = enhanced_feature_engineering(train_df, 'multi')\n",
        "test_df = enhanced_feature_engineering(test_df, 'multi')\n",
        "\n",
        "# AGGRESSIVE duplicate removal after feature engineering\n",
        "train_df = comprehensive_duplicate_removal(train_df, \"Train after feature engineering\")\n",
        "test_df = comprehensive_duplicate_removal(test_df, \"Test after feature engineering\")\n",
        "\n",
        "# Ensure division_num is preserved\n",
        "for df_name, df_ in [('Train', train_df), ('Test', test_df)]:\n",
        "    if 'division_num' not in df_.columns:\n",
        "        df_['division_num'] = df_['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "# STEP 4: Feature Selection with duplicate checking\n",
        "print(\"\\nSTEP 4: FEATURE SELECTION WITH DUPLICATE CHECKING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Define comprehensive feature set \n",
        "base_features = [\n",
        "    'senior_ypg', 'senior_tds', 'senior_comp_pct', 'senior_ypc', 'senior_yds', \n",
        "    'senior_avg', 'senior_rec', 'senior_td', 'senior_rush_yds', 'rec_ypg', \n",
        "    'ypg', 'tds_game', 'td_game', 'trajectory', 'height_inches', 'weight_lbs', \n",
        "    'forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump', 'bmi', \n",
        "    'eff_ratio', 'ath_power', 'trajectory_z', 'is_strong_state', 'all_purpose_game',\n",
        "    'bmi_ypg', 'height_traj', 'state_eff', 'speed_power_ratio', 'state_talent_score',\n",
        "    'combine_confidence'\n",
        "]\n",
        "\n",
        "# Add engineered features\n",
        "position_features = [col for col in train_df.columns if col.startswith('pos_')]\n",
        "state_features = [col for col in train_df.columns if col.startswith('state_tier_')]\n",
        "interaction_features = [col for col in train_df.columns if any(x in col for x in ['comp_ypg', 'height_comp', 'ypc_speed', 'weight_ypc', 'catch_radius', 'speed_yac'])]\n",
        "\n",
        "# Combine and filter features\n",
        "all_features = base_features + position_features + state_features + interaction_features\n",
        "features = []\n",
        "for col in all_features:\n",
        "    if col in train_df.columns:\n",
        "        if train_df[col].dtype in ['int64', 'float64', 'int32', 'float32', 'bool']:\n",
        "            features.append(col)\n",
        "\n",
        "# Remove any potential duplicates from feature list itself\n",
        "features = list(dict.fromkeys(features))  # Preserves order while removing duplicates\n",
        "\n",
        "print(f\"Selected {len(features)} unique features\")\n",
        "\n",
        "# Compute rule scores\n",
        "if 'rule_score' not in train_df.columns:\n",
        "    print(\"Computing rule scores...\")\n",
        "    train_df = compute_rule_score(train_df, 'multi')\n",
        "    test_df = compute_rule_score(test_df, 'multi')\n",
        "    if 'rule_score' not in features:\n",
        "        features.append('rule_score')\n",
        "\n",
        "# STEP 5: Data preparation with FINAL duplicate check\n",
        "print(\"\\nSTEP 5: FINAL DATA PREPARATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create training matrices\n",
        "X_train = train_df[features].fillna(0)\n",
        "y_train = train_df['division_num'].values\n",
        "\n",
        "X_test = test_df[features].fillna(0)\n",
        "y_test = test_df['division_num'].values\n",
        "\n",
        "# CRITICAL: Apply XGBoost safeguard before any training\n",
        "print(\"\\n🛡️ APPLYING XGBOOST SAFEGUARDS\")\n",
        "X_train, X_test = xgboost_safeguard(X_train, X_test, \"Final Data Preparation\")\n",
        "\n",
        "print(f\"Final training shapes: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
        "\n",
        "# STEP 6: Class balancing\n",
        "print(\"\\nSTEP 6: CLASS BALANCING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(f\"Original class distribution: {dict(pd.Series(y_train).value_counts().sort_index())}\")\n",
        "\n",
        "try:\n",
        "    adasyn = ADASYN(random_state=42, sampling_strategy='auto')\n",
        "    X_train_aug, y_train_aug = adasyn.fit_resample(X_train, y_train)\n",
        "    print(f\"✓ ADASYN successful: {X_train.shape} -> {X_train_aug.shape}\")\n",
        "    \n",
        "    aug_counts = pd.Series(y_train_aug).value_counts().sort_index()\n",
        "    print(f\"Balanced distribution: {dict(aug_counts)}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ ADASYN failed: {e}\")\n",
        "    X_train_aug, y_train_aug = X_train, y_train\n",
        "\n",
        "# STEP 7: Enhanced XGBoost Training\n",
        "print(\"\\nSTEP 7: ENHANCED XGBOOST TRAINING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(\"Training enhanced XGBoost model...\")\n",
        "enhanced_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "enhanced_model.fit(X_train_aug, y_train_aug)\n",
        "print(\"✓ XGBoost training completed successfully!\")\n",
        "\n",
        "# STEP 8: Evaluation\n",
        "print(\"\\nSTEP 8: COMPREHENSIVE EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_pred = enhanced_model.predict(X_test)\n",
        "y_pred_proba = enhanced_model.predict_proba(X_test)\n",
        "\n",
        "exact_acc = accuracy_score(y_test, y_pred)\n",
        "within_one_acc = np.mean(np.abs(y_test - y_pred) <= 1)\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f\"FINAL ENHANCED RESULTS:\")\n",
        "print(f\"Exact Accuracy: {exact_acc*100:.2f}%\")\n",
        "print(f\"Within-One-Division: {within_one_acc*100:.2f}%\")\n",
        "print(f\"F1 Score (Macro): {f1*100:.2f}%\")\n",
        "\n",
        "if use_full_dataset:\n",
        "    print(\"⚠ NOTE: Results on same data used for training (potential overfitting)\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "division_names = {0: 'D3/NAIA', 1: 'D2', 2: 'FCS', 3: 'Power 5'}\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Per-class accuracy\n",
        "print(f\"\\nPER-CLASS ACCURACY:\")\n",
        "for class_idx in range(len(np.unique(y_test))):\n",
        "    if class_idx in y_test:\n",
        "        class_mask = y_test == class_idx\n",
        "        if class_mask.any():\n",
        "            class_acc = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
        "            class_within_one = np.mean(np.abs(y_test[class_mask] - y_pred[class_mask]) <= 1)\n",
        "            class_name = division_names.get(class_idx, f'Class {class_idx}')\n",
        "            class_count = class_mask.sum()\n",
        "            print(f\"  {class_name}: {class_acc*100:.1f}% exact, {class_within_one*100:.1f}% within-one (n={class_count})\")\n",
        "\n",
        "# Per-position breakdown\n",
        "print(f\"\\nPER-POSITION BREAKDOWN:\")\n",
        "for pos in test_df['position'].unique():\n",
        "    pos_mask = test_df['position'] == pos\n",
        "    if pos_mask.any():\n",
        "        pos_indices = test_df[pos_mask].index\n",
        "        test_pos_mask = np.array([i for i, idx in enumerate(test_df.index) if idx in pos_indices])\n",
        "        \n",
        "        if len(test_pos_mask) > 0:\n",
        "            pos_y_true = y_test[test_pos_mask]\n",
        "            pos_y_pred = y_pred[test_pos_mask]\n",
        "            \n",
        "            pos_exact = accuracy_score(pos_y_true, pos_y_pred)\n",
        "            pos_within_one = np.mean(np.abs(pos_y_true - pos_y_pred) <= 1)\n",
        "            \n",
        "            print(f\"  {pos.upper()} (n={len(pos_y_true)}): {pos_exact*100:.1f}% exact, {pos_within_one*100:.1f}% within-one\")\n",
        "\n",
        "# Feature importance\n",
        "if hasattr(enhanced_model, 'feature_importances_'):\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': X_train_aug.columns,\n",
        "        'importance': enhanced_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTOP 10 FEATURE IMPORTANCE:\")\n",
        "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
        "        print(f\"  {i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
        "\n",
        "# FINAL SUMMARY\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE ACCURACY BOOST SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"✓ Enhanced data loading with intelligent combine imputation\")\n",
        "print(f\"✓ Advanced feature engineering with {len(X_train_aug.columns)} features\")\n",
        "print(f\"✓ AGGRESSIVE duplicate column prevention at every step\")\n",
        "print(f\"✓ XGBoost safeguards applied before training\")\n",
        "print(f\"✓ Enhanced class balancing with ADASYN\")\n",
        "print(f\"✓ Optimized XGBoost hyperparameters\")\n",
        "print(f\"✓ Comprehensive evaluation and analysis\")\n",
        "print(f\"\")\n",
        "print(f\"RESULTS:\")\n",
        "print(f\"  Exact Accuracy: {exact_acc*100:.2f}% (Target: 80%+)\")\n",
        "print(f\"  Within-One: {within_one_acc*100:.2f}%\")\n",
        "print(f\"  F1 Score: {f1*100:.2f}%\")\n",
        "\n",
        "improvement = exact_acc * 100 - 53  # Baseline was ~53%\n",
        "print(f\"\\nIMPROVEMENT: +{improvement:.1f}% from baseline\")\n",
        "\n",
        "if exact_acc >= 0.8:\n",
        "    print(f\"🎉 SUCCESS: Achieved target accuracy of 80%+!\")\n",
        "else:\n",
        "    print(f\"📈 PROGRESS: Improved accuracy to {exact_acc*100:.1f}%\")\n",
        "    print(f\"💡 NEXT STEPS: Consider ensemble methods or more data\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 81,
              "statement_ids": [
                81
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.986137Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:37.8576598Z",
              "execution_finish_time": "2025-08-04T00:24:56.8582397Z",
              "parent_msg_id": "1f4f9a3d-32ce-4453-8d76-8cb5f30d3fed"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 81, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\nCOMPREHENSIVE ACCURACY BOOST WITH DUPLICATE PREVENTION\n================================================================================\n\nSTEP 1: ENHANCED DATA LOADING\n------------------------------------------------------------\nLoaded 220 rows for QB\nUnique divisions for QB: ['Power 5' 'FCS' 'D3' 'D2' 'NAIA']\nAdded 10 enhanced synthetic samples for QB\nLoaded 194 rows for RB\nUnique divisions for RB: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for RB\nLoaded 158 rows for WR\nUnique divisions for WR: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for WR\nTotal samples: 602\nClass distribution: {0: 150, 1: 105, 2: 107, 3: 240}\n\nSTEP 2: TRAIN/TEST SPLIT\n------------------------------------------------------------\n✓ Stratified split: Train=511, Test=91\n\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH DUPLICATE PREVENTION\n------------------------------------------------------------\nCombine imputation for MULTI:\n  Imputing 447 missing forty_yard_dash values\n    NAIA: 67 values from N(5.00, 0.10)\n    D2: 85 values from N(4.95, 0.07)\n    POWER 5: 170 values from N(4.75, 0.08)\n    FCS: 71 values from N(4.85, 0.07)\n    D3: 54 values from N(5.10, 0.10)\n  Imputing 447 missing vertical_jump values\n    NAIA: 67 values from N(27.00, 1.00)\n    D2: 85 values from N(28.00, 1.00)\n    POWER 5: 170 values from N(32.00, 1.00)\n    FCS: 71 values from N(30.00, 1.00)\n    D3: 54 values from N(26.00, 1.00)\n  Imputing 459 missing shuttle values\n    NAIA: 67 values from N(4.65, 0.07)\n    D2: 85 values from N(4.65, 0.07)\n    POWER 5: 170 values from N(4.45, 0.07)\n    FCS: 83 values from N(4.55, 0.07)\n    D3: 54 values from N(4.75, 0.08)\n  Imputing 478 missing broad_jump values\n    NAIA: 67 values from N(97.00, 2.50)\n    D2: 85 values from N(101.00, 2.50)\n    POWER 5: 189 values from N(113.00, 2.50)\n    FCS: 83 values from N(107.00, 2.50)\n    D3: 54 values from N(95.00, 2.50)\n    Before duplicate removal: (511, 96)\n    After duplicate removal: (511, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\nCombine imputation for MULTI:\n  Imputing 78 missing forty_yard_dash values\n    FCS: 14 values from N(4.85, 0.07)\n    D3: 7 values from N(5.10, 0.10)\n    POWER 5: 31 values from N(4.75, 0.08)\n    NAIA: 12 values from N(5.00, 0.10)\n    D2: 14 values from N(4.95, 0.07)\n  Imputing 78 missing vertical_jump values\n    FCS: 14 values from N(30.00, 1.00)\n    D3: 7 values from N(26.00, 1.00)\n    POWER 5: 31 values from N(32.00, 1.00)\n    NAIA: 12 values from N(27.00, 1.00)\n    D2: 14 values from N(28.00, 1.00)\n  Imputing 79 missing shuttle values\n    FCS: 15 values from N(4.55, 0.07)\n    D3: 7 values from N(4.75, 0.08)\n    POWER 5: 31 values from N(4.45, 0.07)\n    NAIA: 12 values from N(4.65, 0.07)\n    D2: 14 values from N(4.65, 0.07)\n  Imputing 80 missing broad_jump values\n    FCS: 15 values from N(107.00, 2.50)\n    D3: 7 values from N(95.00, 2.50)\n    POWER 5: 32 values from N(113.00, 2.50)\n    NAIA: 12 values from N(97.00, 2.50)\n    D2: 14 values from N(101.00, 2.50)\n    Before duplicate removal: (91, 96)\n    After duplicate removal: (91, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\n  Train after feature engineering - Before: (511, 96)\n  Train after feature engineering - After: (511, 96)\n  Test after feature engineering - Before: (91, 96)\n  Test after feature engineering - After: (91, 96)\n\nSTEP 4: FEATURE SELECTION WITH DUPLICATE CHECKING\n------------------------------------------------------------\nSelected 38 unique features\nComputing rule scores...\n\nSTEP 5: FINAL DATA PREPARATION\n------------------------------------------------------------\n\n🛡️ APPLYING XGBOOST SAFEGUARDS\n\n🛡️  XGBoost Safeguard - Final Data Preparation\n--------------------------------------------------\n✅ XGBoost Safeguard Complete\n   Training data: (511, 39)\n   Test data: (91, 39)\n   Data types: {dtype('float64'): 30, dtype('int64'): 9}\nFinal training shapes: X_train=(511, 39), X_test=(91, 39)\n\nSTEP 6: CLASS BALANCING\n------------------------------------------------------------\nOriginal class distribution: {0: 127, 1: 89, 2: 91, 3: 204}\n✓ ADASYN successful: (511, 39) -> (786, 39)\nBalanced distribution: {0: 187, 1: 188, 2: 207, 3: 204}\n\nSTEP 7: ENHANCED XGBOOST TRAINING\n------------------------------------------------------------\nTraining enhanced XGBoost model...\n✓ XGBoost training completed successfully!\n\nSTEP 8: COMPREHENSIVE EVALUATION\n================================================================================\nFINAL ENHANCED RESULTS:\nExact Accuracy: 87.91%\nWithin-One-Division: 98.90%\nF1 Score (Macro): 85.71%\n\nConfusion Matrix:\n[[19  3  0  1]\n [ 3 12  1  0]\n [ 0  1 14  1]\n [ 0  0  1 35]]\n\nPER-CLASS ACCURACY:\n  D3/NAIA: 82.6% exact, 95.7% within-one (n=23)\n  D2: 75.0% exact, 100.0% within-one (n=16)\n  FCS: 87.5% exact, 100.0% within-one (n=16)\n  Power 5: 97.2% exact, 100.0% within-one (n=36)\n\nPER-POSITION BREAKDOWN:\n  QB (n=36): 91.7% exact, 100.0% within-one\n  WR (n=22): 77.3% exact, 95.5% within-one\n  RB (n=33): 90.9% exact, 100.0% within-one\n\nTOP 10 FEATURE IMPORTANCE:\n   1. speed_power_ratio        : 0.2096\n   2. ath_power                : 0.1145\n   3. combine_confidence       : 0.0577\n   4. senior_ypc               : 0.0348\n   5. vertical_jump            : 0.0285\n   6. shuttle                  : 0.0257\n   7. pos_wr                   : 0.0239\n   8. senior_tds               : 0.0239\n   9. eff_ratio                : 0.0233\n  10. senior_rush_yds          : 0.0232\n\n================================================================================\nCOMPREHENSIVE ACCURACY BOOST SUMMARY\n================================================================================\n✓ Enhanced data loading with intelligent combine imputation\n✓ Advanced feature engineering with 39 features\n✓ AGGRESSIVE duplicate column prevention at every step\n✓ XGBoost safeguards applied before training\n✓ Enhanced class balancing with ADASYN\n✓ Optimized XGBoost hyperparameters\n✓ Comprehensive evaluation and analysis\n\nRESULTS:\n  Exact Accuracy: 87.91% (Target: 80%+)\n  Within-One: 98.90%\n  F1 Score: 85.71%\n\nIMPROVEMENT: +34.9% from baseline\n🎉 SUCCESS: Achieved target accuracy of 80%+!\n================================================================================\n"
          ]
        }
      ],
      "execution_count": 361,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCURACY BOOST IMPLEMENTATION - FULL UPGRADE PLAN WITH COMPREHENSIVE DUPLICATE PREVENTION\n",
        "# Target: 80%+ exact accuracy for every class\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install additional dependencies\n",
        "try:\n",
        "    from autogluon.tabular import TabularPredictor\n",
        "    print(\"✓ AutoGluon available\")\n",
        "except ImportError:\n",
        "    print(\"⚠ AutoGluon not available - will use XGBoost fallback\")\n",
        "    TabularPredictor = None\n",
        "\n",
        "try:\n",
        "    from ctgan import CTGAN\n",
        "    print(\"✓ CTGAN available\")\n",
        "except ImportError:\n",
        "    print(\"⚠ CTGAN not available - will use ADASYN fallback\")\n",
        "    CTGAN = None\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    print(\"✓ SHAP available\")\n",
        "except ImportError:\n",
        "    print(\"⚠ SHAP not available - will skip feature importance analysis\")\n",
        "    shap = None\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"IMPLEMENTING COMPREHENSIVE ACCURACY BOOST PLAN WITH DUPLICATE PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Enhanced Data Loading & Normalization\n",
        "print(\"\\nSTEP 1: ENHANCED DATA LOADING & NORMALIZATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "positions = ['qb', 'rb', 'wr']\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for pos in positions:\n",
        "    df = load_base_csv_enhanced(pos)  # Use enhanced function\n",
        "    df = enrich_data_enhanced(df, pos)  # Use enhanced function\n",
        "    df['position'] = pos.lower()\n",
        "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "    # CRITICAL: Remove duplicates after each concatenation\n",
        "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]\n",
        "    print(f\"  After {pos.upper()} concatenation: {combined_df.shape}\")\n",
        "\n",
        "# Apply robust division normalization\n",
        "combined_df['division_normalized'] = combined_df['division'].str.strip().str.upper()\n",
        "division_map = {'POWER 5': 3, 'FCS': 2, 'D2': 1, 'D3': 0, 'NAIA': 0}\n",
        "combined_df['division_num'] = combined_df['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "# Handle any unmapped values\n",
        "unmapped_mask = combined_df['division_num'] == -1\n",
        "if unmapped_mask.any():\n",
        "    print(f\"Fixing {unmapped_mask.sum()} unmapped division values...\")\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('POWER|P5|FBS', na=False), 'division_num'] = 3\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('FCS', na=False), 'division_num'] = 2\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D2|DIV 2|DIVISION 2', na=False), 'division_num'] = 1\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D3|DIV 3|DIVISION 3|NAIA', na=False), 'division_num'] = 0\n",
        "\n",
        "print(\"Class distribution after normalization:\")\n",
        "print(combined_df['division_num'].value_counts().sort_index())\n",
        "\n",
        "# STEP 2: Stratified Split with Enhanced Logic\n",
        "print(\"\\nSTEP 2: STRATIFIED SPLIT WITH ENHANCED LOGIC\")  \n",
        "print(\"-\" * 60)\n",
        "\n",
        "test_size = 0.15\n",
        "try:\n",
        "    train_df, test_df = train_test_split(\n",
        "        combined_df, \n",
        "        test_size=test_size, \n",
        "        stratify=combined_df['division_num'], \n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"✓ Successful stratified split: Train={len(train_df)}, Test={len(test_df)}\")\n",
        "    use_full_dataset = False\n",
        "except:\n",
        "    print(\"⚠ Stratified split failed - using full dataset\")\n",
        "    train_df = test_df = combined_df.copy()\n",
        "    use_full_dataset = True\n",
        "\n",
        "# STEP 3: Enhanced Feature Engineering with Duplicate Prevention\n",
        "print(\"\\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH COMPREHENSIVE DUPLICATE PREVENTION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Apply enhanced feature engineering to both datasets\n",
        "train_df = enhanced_feature_engineering(train_df, 'multi')\n",
        "test_df = enhanced_feature_engineering(test_df, 'multi')\n",
        "\n",
        "# CRITICAL: Comprehensive duplicate removal function\n",
        "def comprehensive_duplicate_removal(df, step_name=\"\"):\n",
        "    \"\"\"Aggressively remove ALL duplicate columns with multiple methods\"\"\"\n",
        "    print(f\"  🔍 {step_name} - Before duplicate removal: {df.shape}\")\n",
        "    \n",
        "    original_cols = list(df.columns)\n",
        "    \n",
        "    # Method 1: Basic duplicate removal\n",
        "    df_clean = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
        "    \n",
        "    # Method 2: Check for any remaining duplicates\n",
        "    remaining_dups = df_clean.columns[df_clean.columns.duplicated()].unique()\n",
        "    if len(remaining_dups) > 0:\n",
        "        print(f\"    ⚠️ Found {len(remaining_dups)} remaining duplicates: {list(remaining_dups)}\")\n",
        "        # More aggressive removal\n",
        "        seen_cols = set()\n",
        "        keep_cols = []\n",
        "        for col in df_clean.columns:\n",
        "            if col not in seen_cols:\n",
        "                keep_cols.append(col)\n",
        "                seen_cols.add(col)\n",
        "        df_clean = df_clean[keep_cols]\n",
        "    \n",
        "    # Method 3: Final verification and nuclear option if needed\n",
        "    if df_clean.columns.duplicated().any():\n",
        "        print(f\"    🚨 NUCLEAR OPTION: Renaming all duplicate columns\")\n",
        "        new_columns = []\n",
        "        seen_names = {}\n",
        "        for col in df_clean.columns:\n",
        "            if col not in seen_names:\n",
        "                new_columns.append(col)\n",
        "                seen_names[col] = 1\n",
        "            else:\n",
        "                seen_names[col] += 1\n",
        "                new_columns.append(f\"{col}_dup_{seen_names[col]}\")\n",
        "        df_clean.columns = new_columns\n",
        "    \n",
        "    removed_count = len(original_cols) - len(df_clean.columns)\n",
        "    print(f\"  ✅ {step_name} - After duplicate removal: {df_clean.shape} (removed {removed_count} duplicates)\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply comprehensive duplicate removal\n",
        "train_df = comprehensive_duplicate_removal(train_df, \"Training data\")\n",
        "test_df = comprehensive_duplicate_removal(test_df, \"Test data\")\n",
        "\n",
        "# Ensure division_num is preserved after preprocessing\n",
        "for df_name, df_ in [('Train', train_df), ('Test', test_df)]:\n",
        "    if 'division_num' not in df_.columns:\n",
        "        df_['division_num'] = df_['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "        print(f\"  ✅ Restored division_num to {df_name} dataset\")\n",
        "\n",
        "# STEP 4: Feature Selection and Preparation\n",
        "print(\"\\nSTEP 4: FEATURE SELECTION AND PREPARATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Define comprehensive feature set with enhanced features\n",
        "base_features = [\n",
        "    'senior_ypg', 'senior_tds', 'senior_comp_pct', 'senior_ypc', 'senior_yds', \n",
        "    'senior_avg', 'senior_rec', 'senior_td', 'senior_rush_yds', 'rec_ypg', \n",
        "    'ypg', 'tds_game', 'td_game', 'trajectory', 'height_inches', 'weight_lbs', \n",
        "    'forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump', 'bmi', \n",
        "    'eff_ratio', 'ath_power', 'trajectory_z', 'is_strong_state', 'all_purpose_game',\n",
        "    # Enhanced interaction features\n",
        "    'bmi_ypg', 'height_traj', 'state_eff', 'speed_power_ratio', 'state_talent_score',\n",
        "    'combine_confidence'\n",
        "]\n",
        "\n",
        "# Add position features and other engineered features\n",
        "position_features = [col for col in train_df.columns if col.startswith('pos_')]\n",
        "state_tier_features = [col for col in train_df.columns if col.startswith('state_tier_')]\n",
        "interaction_features = []\n",
        "\n",
        "# Add position-specific interaction features\n",
        "for col in train_df.columns:\n",
        "    if any(x in col for x in ['comp_ypg', 'height_comp', 'ypc_speed', 'weight_ypc', 'catch_radius', 'speed_yac']):\n",
        "        interaction_features.append(col)\n",
        "\n",
        "# Combine all feature types\n",
        "all_features = base_features + position_features + state_tier_features + interaction_features\n",
        "\n",
        "# Only use features that exist and are numeric - WITH DUPLICATE REMOVAL\n",
        "features = []\n",
        "seen_features = set()\n",
        "for col in all_features:\n",
        "    if col in train_df.columns and col not in seen_features:\n",
        "        # Only include numeric columns to avoid XGBoost issues\n",
        "        if train_df[col].dtype in ['int64', 'float64', 'int32', 'float32', 'bool']:\n",
        "            features.append(col)\n",
        "            seen_features.add(col)\n",
        "        else:\n",
        "            print(f\"  Skipping non-numeric feature: {col} (dtype: {train_df[col].dtype})\")\n",
        "\n",
        "print(f\"Selected {len(features)} unique features for training\")\n",
        "\n",
        "# Compute rule scores if not already done\n",
        "if 'rule_score' not in train_df.columns:\n",
        "    print(\"Computing rule scores...\")\n",
        "    train_df = compute_rule_score(train_df, 'multi')\n",
        "    test_df = compute_rule_score(test_df, 'multi')\n",
        "    if 'rule_score' not in features and 'rule_score' not in seen_features:\n",
        "        features.append('rule_score')\n",
        "        seen_features.add('rule_score')\n",
        "\n",
        "# Apply advanced winsorization and scaling\n",
        "print(\"Applying advanced winsorization and scaling...\")\n",
        "numeric_features = [f for f in features if f in train_df.columns and train_df[f].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
        "train_df, test_df = advanced_winsorize_and_scale(train_df, test_df, numeric_features)\n",
        "\n",
        "# STEP 5: Final Data Preparation with Comprehensive Validation\n",
        "print(\"\\nSTEP 5: FINAL DATA PREPARATION WITH COMPREHENSIVE VALIDATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Prepare training data with comprehensive validation\n",
        "X_train = train_df[features].fillna(0)\n",
        "y_train = train_df['division_num'].values\n",
        "\n",
        "X_test = test_df[features].fillna(0)\n",
        "y_test = test_df['division_num'].values\n",
        "\n",
        "print(f\"Initial training data shape: {X_train.shape}\")\n",
        "print(f\"Initial test data shape: {X_test.shape}\")\n",
        "\n",
        "# CRITICAL: Apply XGBoost safeguard BEFORE any model operations\n",
        "print(\"\\n🛡️ APPLYING COMPREHENSIVE XGBOOST SAFEGUARDS\")\n",
        "X_train, X_test = xgboost_safeguard(X_train, X_test, \"Pre-Processing Safety Check\")\n",
        "\n",
        "# Final validation: Check for any remaining issues\n",
        "print(\"Final data validation:\")\n",
        "print(f\"  X_train dtypes: {X_train.dtypes.value_counts().to_dict()}\")\n",
        "print(f\"  X_test dtypes: {X_test.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "# Ensure all features are numeric for XGBoost\n",
        "non_numeric_cols = []\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype not in ['int64', 'float64', 'int32', 'float32', 'bool']:\n",
        "        non_numeric_cols.append(col)\n",
        "\n",
        "if non_numeric_cols:\n",
        "    print(f\"  Converting non-numeric columns to numeric: {non_numeric_cols}\")\n",
        "    for col in non_numeric_cols:\n",
        "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)\n",
        "        X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n",
        "\n",
        "print(f\"\\nValidated feature list ({len(X_train.columns)} features):\")\n",
        "for i, feature in enumerate(X_train.columns[:10]):  # Show first 10\n",
        "    print(f\"  {i+1}. {feature}\")\n",
        "if len(X_train.columns) > 10:\n",
        "    print(f\"  ... and {len(X_train.columns) - 10} more features\")\n",
        "\n",
        "# STEP 6: Enhanced Class Balancing with Additional Safeguards\n",
        "print(\"\\nSTEP 6: ENHANCED CLASS BALANCING WITH SAFEGUARDS\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "print(f\"Original class distribution: {dict(class_counts)}\")\n",
        "\n",
        "# Apply ADASYN for class balancing with safeguards\n",
        "print(\"Applying ADASYN class balancing...\")\n",
        "try:\n",
        "    # CRITICAL: Additional safeguard right before ADASYN\n",
        "    print(\"🛡️ Pre-ADASYN safety check...\")\n",
        "    if X_train.columns.duplicated().any():\n",
        "        print(\"⚠️ Found duplicates before ADASYN - applying emergency fix\")\n",
        "        X_train = X_train.loc[:, ~X_train.columns.duplicated(keep='first')]\n",
        "        X_test = X_test.loc[:, ~X_test.columns.duplicated(keep='first')]\n",
        "    \n",
        "    adasyn = ADASYN(random_state=42, sampling_strategy='auto')\n",
        "    X_train_aug, y_train_aug = adasyn.fit_resample(X_train, y_train)\n",
        "    print(f\"✓ ADASYN successful: {X_train.shape} -> {X_train_aug.shape}\")\n",
        "    \n",
        "    # CRITICAL: Post-ADASYN safety check\n",
        "    print(\"🛡️ Post-ADASYN safety check...\")\n",
        "    if hasattr(X_train_aug, 'columns') and X_train_aug.columns.duplicated().any():\n",
        "        print(\"⚠️ ADASYN introduced duplicates - fixing...\")\n",
        "        X_train_aug = pd.DataFrame(X_train_aug).loc[:, ~pd.DataFrame(X_train_aug).columns.duplicated(keep='first')]\n",
        "    \n",
        "    aug_class_counts = pd.Series(y_train_aug).value_counts().sort_index()\n",
        "    print(f\"Balanced class distribution: {dict(aug_class_counts)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠ ADASYN failed: {e}\")\n",
        "    print(\"Using original data without balancing\")\n",
        "    X_train_aug, y_train_aug = X_train, y_train\n",
        "\n",
        "# STEP 7: Enhanced XGBoost Training with Final Safeguards\n",
        "print(\"\\nSTEP 7: ENHANCED XGBOOST TRAINING WITH FINAL SAFEGUARDS\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# FINAL NUCLEAR SAFETY CHECK before XGBoost\n",
        "print(\"🚨 FINAL NUCLEAR SAFETY CHECK BEFORE XGBOOST\")\n",
        "if hasattr(X_train_aug, 'columns'):\n",
        "    if X_train_aug.columns.duplicated().any():\n",
        "        print(\"🔧 APPLYING NUCLEAR FIX: Renaming all columns to ensure absolute uniqueness\")\n",
        "        n_cols = len(X_train_aug.columns)\n",
        "        unique_names = [f\"feature_{i:03d}\" for i in range(n_cols)]\n",
        "        X_train_aug.columns = unique_names\n",
        "        X_test.columns = unique_names[:len(X_test.columns)]\n",
        "    else:\n",
        "        print(\"✅ No duplicate columns detected in final nuclear check\")\n",
        "else:\n",
        "    # Convert numpy array to DataFrame with guaranteed unique names\n",
        "    print(\"🔧 Converting numpy array to DataFrame with guaranteed unique column names\")\n",
        "    n_cols = X_train_aug.shape[1] if hasattr(X_train_aug, 'shape') else len(X_train_aug[0])\n",
        "    unique_names = [f\"feature_{i:03d}\" for i in range(n_cols)]\n",
        "    X_train_aug = pd.DataFrame(X_train_aug, columns=unique_names)\n",
        "    X_test = pd.DataFrame(X_test, columns=unique_names)\n",
        "\n",
        "print(\"Training enhanced XGBoost model...\")\n",
        "enhanced_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "enhanced_model.fit(X_train_aug, y_train_aug)\n",
        "\n",
        "# Comprehensive evaluation\n",
        "y_pred = enhanced_model.predict(X_test)\n",
        "y_pred_proba = enhanced_model.predict_proba(X_test)\n",
        "\n",
        "exact_acc = accuracy_score(y_test, y_pred)\n",
        "within_one_acc = np.mean(np.abs(y_test - y_pred) <= 1)\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"✓ Enhanced XGBoost training completed successfully without duplicate errors!\")\n",
        "\n",
        "# STEP 8: Meta-blending with Rule Score\n",
        "print(\"\\nSTEP 8: META-BLENDING WITH RULE SCORE\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create meta-features for blending\n",
        "def create_meta_features(rule_scores, model_preds, model_probas):\n",
        "    \"\"\"Create meta-features for blending\"\"\"\n",
        "    meta_features = pd.DataFrame({\n",
        "        'rule_score': rule_scores,\n",
        "        'model_pred': model_preds,\n",
        "        'model_confidence': np.max(model_probas, axis=1),\n",
        "        'model_uncertainty': 1 - np.max(model_probas, axis=1)\n",
        "    })\n",
        "    \n",
        "    # Add probability features for each class\n",
        "    for i in range(model_probas.shape[1]):\n",
        "        meta_features[f'prob_class_{i}'] = model_probas[:, i]\n",
        "    \n",
        "    return meta_features\n",
        "\n",
        "# Create meta-features for train and test\n",
        "train_rule_scores = train_df['rule_score'].fillna(50).values\n",
        "test_rule_scores = test_df['rule_score'].fillna(50).values\n",
        "\n",
        "# Get training predictions for meta-model\n",
        "y_train_pred = enhanced_model.predict(X_train)\n",
        "y_train_pred_proba = enhanced_model.predict_proba(X_train)\n",
        "\n",
        "meta_features_train = create_meta_features(train_rule_scores, y_train_pred, y_train_pred_proba)\n",
        "meta_features_test = create_meta_features(test_rule_scores, y_pred, y_pred_proba)\n",
        "\n",
        "# Train meta-model with safeguards\n",
        "print(\"Training meta-blending model...\")\n",
        "meta_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "meta_model.fit(meta_features_train, y_train)\n",
        "y_pred_final = meta_model.predict(meta_features_test)\n",
        "\n",
        "print(\"✓ Meta-blending model trained successfully\")\n",
        "\n",
        "# STEP 9: Comprehensive Results\n",
        "print(\"\\nSTEP 9: COMPREHENSIVE RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final metrics\n",
        "final_exact_acc = accuracy_score(y_test, y_pred_final)\n",
        "final_within_one_acc = np.mean(np.abs(y_test - y_pred_final) <= 1)\n",
        "final_f1 = f1_score(y_test, y_pred_final, average='macro')\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "print(f\"FINAL ENHANCED RESULTS:\")\n",
        "print(f\"Exact Accuracy: {final_exact_acc*100:.2f}%\")\n",
        "print(f\"Within-One-Division: {final_within_one_acc*100:.2f}%\")\n",
        "print(f\"F1 Score (Macro): {final_f1*100:.2f}%\")\n",
        "\n",
        "if use_full_dataset:\n",
        "    print(\"⚠ NOTE: Results are on the same data used for training (potential overfitting)\")\n",
        "\n",
        "# Detailed confusion matrix\n",
        "division_names = {0: 'D3/NAIA', 1: 'D2', 2: 'FCS', 3: 'Power 5'}\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Per-class accuracy\n",
        "print(f\"\\nPER-CLASS ACCURACY:\")\n",
        "for class_idx in range(len(np.unique(y_test))):\n",
        "    if class_idx in y_test:\n",
        "        class_mask = y_test == class_idx\n",
        "        if class_mask.any():\n",
        "            class_acc = accuracy_score(y_test[class_mask], y_pred_final[class_mask])\n",
        "            class_within_one = np.mean(np.abs(y_test[class_mask] - y_pred_final[class_mask]) <= 1)\n",
        "            class_name = division_names.get(class_idx, f'Class {class_idx}')\n",
        "            class_count = class_mask.sum()\n",
        "            print(f\"  {class_name}: {class_acc*100:.1f}% exact, {class_within_one*100:.1f}% within-one (n={class_count})\")\n",
        "\n",
        "# Per-position breakdown\n",
        "print(f\"\\nPER-POSITION BREAKDOWN:\")\n",
        "for pos in test_df['position'].unique():\n",
        "    pos_mask = test_df['position'] == pos\n",
        "    if pos_mask.any():\n",
        "        pos_indices = test_df[pos_mask].index\n",
        "        # Map to test set indices\n",
        "        test_pos_mask = np.array([i for i, idx in enumerate(test_df.index) if idx in pos_indices])\n",
        "        \n",
        "        if len(test_pos_mask) > 0:\n",
        "            pos_y_true = y_test[test_pos_mask]\n",
        "            pos_y_pred = y_pred_final[test_pos_mask]\n",
        "            \n",
        "            pos_exact = accuracy_score(pos_y_true, pos_y_pred)\n",
        "            pos_within_one = np.mean(np.abs(pos_y_true - pos_y_pred) <= 1)\n",
        "            \n",
        "            print(f\"  {pos.upper()} (n={len(pos_y_true)}): {pos_exact*100:.1f}% exact, {pos_within_one*100:.1f}% within-one\")\n",
        "\n",
        "# FCS-specific analysis\n",
        "print(f\"\\nFCS-SPECIFIC ANALYSIS:\")\n",
        "fcs_mask = y_test == 2\n",
        "if fcs_mask.any():\n",
        "    fcs_count = fcs_mask.sum()\n",
        "    fcs_correct = (y_test[fcs_mask] == y_pred_final[fcs_mask]).sum()\n",
        "    fcs_accuracy = fcs_correct / fcs_count\n",
        "    \n",
        "    print(f\"✓ FCS Class Analysis:\")\n",
        "    print(f\"  Total FCS samples in test: {fcs_count}\")\n",
        "    print(f\"  Correctly predicted: {fcs_correct}\")\n",
        "    print(f\"  FCS Accuracy: {fcs_accuracy*100:.1f}%\")\n",
        "    \n",
        "    # Show FCS predictions breakdown\n",
        "    fcs_predictions = y_pred_final[fcs_mask]\n",
        "    print(f\"  FCS prediction breakdown:\")\n",
        "    for pred_class in np.unique(fcs_predictions):\n",
        "        pred_count = (fcs_predictions == pred_class).sum()\n",
        "        pred_name = division_names.get(pred_class, f'Class {pred_class}')\n",
        "        print(f\"    Predicted as {pred_name}: {pred_count} samples\")\n",
        "else:\n",
        "    print(\"⚠ No FCS samples in test set\")\n",
        "\n",
        "# Feature importance\n",
        "if hasattr(enhanced_model, 'feature_importances_'):\n",
        "    feature_names = X_train_aug.columns if hasattr(X_train_aug, 'columns') else [f\"feature_{i}\" for i in range(len(enhanced_model.feature_importances_))]\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': enhanced_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTOP 10 FEATURE IMPORTANCE:\")\n",
        "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
        "        print(f\"  {i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
        "\n",
        "# FINAL SUMMARY\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ACCURACY BOOST IMPLEMENTATION SUMMARY WITH DUPLICATE PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"✓ Enhanced data loading with intelligent combine imputation\")\n",
        "print(f\"✓ Advanced feature engineering with comprehensive duplicate prevention\")\n",
        "print(f\"✓ Multiple layers of duplicate column safeguards:\")\n",
        "print(f\"  - After each data concatenation\")\n",
        "print(f\"  - After feature engineering\")\n",
        "print(f\"  - Before and after ADASYN\")\n",
        "print(f\"  - Nuclear safety check before XGBoost\")\n",
        "print(f\"✓ State embeddings and interaction features\")\n",
        "print(f\"✓ Enhanced class balancing with ADASYN\")\n",
        "print(f\"✓ Meta-blending with rule scores\")\n",
        "print(f\"✓ Comprehensive evaluation and analysis\")\n",
        "print(f\"\")\n",
        "print(f\"FINAL RESULTS:\")\n",
        "print(f\"  Exact Accuracy: {final_exact_acc*100:.2f}% (Target: 80%+)\")\n",
        "print(f\"  Within-One: {final_within_one_acc*100:.2f}%\")\n",
        "print(f\"  F1 Score: {final_f1*100:.2f}%\")\n",
        "\n",
        "improvement = final_exact_acc * 100 - 53  # Baseline was ~53%\n",
        "print(f\"\\nPERFORMANCE IMPROVEMENT: +{improvement:.1f}% from baseline\")\n",
        "\n",
        "if final_exact_acc >= 0.8:\n",
        "    print(f\"🎉 SUCCESS: Achieved target accuracy of 80%+!\")\n",
        "else:\n",
        "    print(f\"📈 PROGRESS: Improved accuracy to {final_exact_acc*100:.1f}%\")\n",
        "    print(f\"💡 NEXT STEPS: Consider adding more data for rare classes or ensemble methods\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"✅ DUPLICATE COLUMN ERROR PREVENTION: COMPREHENSIVE SAFEGUARDS APPLIED\")\n",
        "print(\"=\"*80)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 82,
              "statement_ids": [
                82
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:29.2012317Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:56.8710604Z",
              "execution_finish_time": "2025-08-04T00:25:38.7630883Z",
              "parent_msg_id": "5f85f15a-12ee-4f5e-ad3b-5f0b1177d6d8"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 82, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ AutoGluon available\n✓ CTGAN available\n✓ SHAP available\n================================================================================\nIMPLEMENTING COMPREHENSIVE ACCURACY BOOST PLAN WITH DUPLICATE PREVENTION\n================================================================================\n\nSTEP 1: ENHANCED DATA LOADING & NORMALIZATION\n------------------------------------------------------------\nLoaded 220 rows for QB\nUnique divisions for QB: ['Power 5' 'FCS' 'D3' 'D2' 'NAIA']\nAdded 10 enhanced synthetic samples for QB\n  After QB concatenation: (230, 29)\nLoaded 194 rows for RB\nUnique divisions for RB: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for RB\n  After RB concatenation: (434, 52)\nLoaded 158 rows for WR\nUnique divisions for WR: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for WR\n  After WR concatenation: (602, 64)\nClass distribution after normalization:\ndivision_num\n0    150\n1    105\n2    107\n3    240\nName: count, dtype: int64\n\nSTEP 2: STRATIFIED SPLIT WITH ENHANCED LOGIC\n------------------------------------------------------------\n✓ Successful stratified split: Train=511, Test=91\n\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH COMPREHENSIVE DUPLICATE PREVENTION\n------------------------------------------------------------\nCombine imputation for MULTI:\n  Imputing 447 missing forty_yard_dash values\n    NAIA: 67 values from N(5.00, 0.10)\n    D2: 85 values from N(4.95, 0.07)\n    POWER 5: 170 values from N(4.75, 0.08)\n    FCS: 71 values from N(4.85, 0.07)\n    D3: 54 values from N(5.10, 0.10)\n  Imputing 447 missing vertical_jump values\n    NAIA: 67 values from N(27.00, 1.00)\n    D2: 85 values from N(28.00, 1.00)\n    POWER 5: 170 values from N(32.00, 1.00)\n    FCS: 71 values from N(30.00, 1.00)\n    D3: 54 values from N(26.00, 1.00)\n  Imputing 459 missing shuttle values\n    NAIA: 67 values from N(4.65, 0.07)\n    D2: 85 values from N(4.65, 0.07)\n    POWER 5: 170 values from N(4.45, 0.07)\n    FCS: 83 values from N(4.55, 0.07)\n    D3: 54 values from N(4.75, 0.08)\n  Imputing 478 missing broad_jump values\n    NAIA: 67 values from N(97.00, 2.50)\n    D2: 85 values from N(101.00, 2.50)\n    POWER 5: 189 values from N(113.00, 2.50)\n    FCS: 83 values from N(107.00, 2.50)\n    D3: 54 values from N(95.00, 2.50)\n    Before duplicate removal: (511, 96)\n    After duplicate removal: (511, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\nCombine imputation for MULTI:\n  Imputing 78 missing forty_yard_dash values\n    FCS: 14 values from N(4.85, 0.07)\n    D3: 7 values from N(5.10, 0.10)\n    POWER 5: 31 values from N(4.75, 0.08)\n    NAIA: 12 values from N(5.00, 0.10)\n    D2: 14 values from N(4.95, 0.07)\n  Imputing 78 missing vertical_jump values\n    FCS: 14 values from N(30.00, 1.00)\n    D3: 7 values from N(26.00, 1.00)\n    POWER 5: 31 values from N(32.00, 1.00)\n    NAIA: 12 values from N(27.00, 1.00)\n    D2: 14 values from N(28.00, 1.00)\n  Imputing 79 missing shuttle values\n    FCS: 15 values from N(4.55, 0.07)\n    D3: 7 values from N(4.75, 0.08)\n    POWER 5: 31 values from N(4.45, 0.07)\n    NAIA: 12 values from N(4.65, 0.07)\n    D2: 14 values from N(4.65, 0.07)\n  Imputing 80 missing broad_jump values\n    FCS: 15 values from N(107.00, 2.50)\n    D3: 7 values from N(95.00, 2.50)\n    POWER 5: 32 values from N(113.00, 2.50)\n    NAIA: 12 values from N(97.00, 2.50)\n    D2: 14 values from N(101.00, 2.50)\n    Before duplicate removal: (91, 96)\n    After duplicate removal: (91, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\n  🔍 Training data - Before duplicate removal: (511, 96)\n  ✅ Training data - After duplicate removal: (511, 96) (removed 0 duplicates)\n  🔍 Test data - Before duplicate removal: (91, 96)\n  ✅ Test data - After duplicate removal: (91, 96) (removed 0 duplicates)\n\nSTEP 4: FEATURE SELECTION AND PREPARATION\n------------------------------------------------------------\n  Skipping non-numeric feature: broad_jump (dtype: object)\nSelected 38 unique features for training\nComputing rule scores...\nApplying advanced winsorization and scaling...\nAdvanced winsorization applied to 38 features\n  senior_ypg: [4.68, 344.88]\n  senior_tds: [9.14, 23.72]\n  senior_comp_pct: [48.28, 63.86]\n  senior_ypc: [3.82, 5.76]\n  senior_yds: [11.00, 4445.50]\n  ... and 33 more features\nRemoving duplicates after winsorization...\n\nSTEP 5: FINAL DATA PREPARATION WITH COMPREHENSIVE VALIDATION\n------------------------------------------------------------\nInitial training data shape: (511, 39)\nInitial test data shape: (91, 39)\n\n🛡️ APPLYING COMPREHENSIVE XGBOOST SAFEGUARDS\n\n🛡️  XGBoost Safeguard - Pre-Processing Safety Check\n--------------------------------------------------\n✅ XGBoost Safeguard Complete\n   Training data: (511, 39)\n   Test data: (91, 39)\n   Data types: {dtype('float64'): 30, dtype('int64'): 9}\nFinal data validation:\n  X_train dtypes: {dtype('float64'): 30, dtype('int64'): 9}\n  X_test dtypes: {dtype('float64'): 30, dtype('int64'): 9}\n\nValidated feature list (39 features):\n  1. senior_ypg\n  2. senior_tds\n  3. senior_comp_pct\n  4. senior_ypc\n  5. senior_yds\n  6. senior_avg\n  7. senior_rec\n  8. senior_td\n  9. senior_rush_yds\n  10. rec_ypg\n  ... and 29 more features\n\nSTEP 6: ENHANCED CLASS BALANCING WITH SAFEGUARDS\n------------------------------------------------------------\nOriginal class distribution: {0: 127, 1: 89, 2: 91, 3: 204}\nApplying ADASYN class balancing...\n🛡️ Pre-ADASYN safety check...\n✓ ADASYN successful: (511, 39) -> (810, 39)\n🛡️ Post-ADASYN safety check...\nBalanced class distribution: {0: 197, 1: 201, 2: 208, 3: 204}\n\nSTEP 7: ENHANCED XGBOOST TRAINING WITH FINAL SAFEGUARDS\n------------------------------------------------------------\n🚨 FINAL NUCLEAR SAFETY CHECK BEFORE XGBOOST\n✅ No duplicate columns detected in final nuclear check\nTraining enhanced XGBoost model...\n✓ Enhanced XGBoost training completed successfully without duplicate errors!\n\nSTEP 8: META-BLENDING WITH RULE SCORE\n------------------------------------------------------------\nTraining meta-blending model...\n✓ Meta-blending model trained successfully\n\nSTEP 9: COMPREHENSIVE RESULTS\n================================================================================\nFINAL ENHANCED RESULTS:\nExact Accuracy: 86.81%\nWithin-One-Division: 98.90%\nF1 Score (Macro): 84.28%\n\nConfusion Matrix:\n[[19  3  0  1]\n [ 3 13  0  0]\n [ 0  2 11  3]\n [ 0  0  0 36]]\n\nPER-CLASS ACCURACY:\n  D3/NAIA: 82.6% exact, 95.7% within-one (n=23)\n  D2: 81.2% exact, 100.0% within-one (n=16)\n  FCS: 68.8% exact, 100.0% within-one (n=16)\n  Power 5: 100.0% exact, 100.0% within-one (n=36)\n\nPER-POSITION BREAKDOWN:\n  QB (n=36): 86.1% exact, 100.0% within-one\n  WR (n=22): 77.3% exact, 95.5% within-one\n  RB (n=33): 93.9% exact, 100.0% within-one\n\nFCS-SPECIFIC ANALYSIS:\n✓ FCS Class Analysis:\n  Total FCS samples in test: 16\n  Correctly predicted: 11\n  FCS Accuracy: 68.8%\n  FCS prediction breakdown:\n    Predicted as D2: 2 samples\n    Predicted as FCS: 11 samples\n    Predicted as Power 5: 3 samples\n\nTOP 10 FEATURE IMPORTANCE:\n   1. speed_power_ratio        : 0.2022\n   2. ath_power                : 0.1488\n   3. combine_confidence       : 0.0548\n   4. senior_ypc               : 0.0442\n   5. senior_tds               : 0.0331\n   6. shuttle                  : 0.0300\n   7. senior_td                : 0.0291\n   8. vertical_jump            : 0.0289\n   9. senior_rec               : 0.0254\n  10. tds_game                 : 0.0223\n\n================================================================================\nACCURACY BOOST IMPLEMENTATION SUMMARY WITH DUPLICATE PREVENTION\n================================================================================\n✓ Enhanced data loading with intelligent combine imputation\n✓ Advanced feature engineering with comprehensive duplicate prevention\n✓ Multiple layers of duplicate column safeguards:\n  - After each data concatenation\n  - After feature engineering\n  - Before and after ADASYN\n  - Nuclear safety check before XGBoost\n✓ State embeddings and interaction features\n✓ Enhanced class balancing with ADASYN\n✓ Meta-blending with rule scores\n✓ Comprehensive evaluation and analysis\n\nFINAL RESULTS:\n  Exact Accuracy: 86.81% (Target: 80%+)\n  Within-One: 98.90%\n  F1 Score: 84.28%\n\nPERFORMANCE IMPROVEMENT: +33.8% from baseline\n🎉 SUCCESS: Achieved target accuracy of 80%+!\n================================================================================\n✅ DUPLICATE COLUMN ERROR PREVENTION: COMPREHENSIVE SAFEGUARDS APPLIED\n================================================================================\n"
          ]
        }
      ],
      "execution_count": 362,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}