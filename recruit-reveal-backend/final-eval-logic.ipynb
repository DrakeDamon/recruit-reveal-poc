{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install imbalanced-learn xgboost shap scipy scikit-learn pandas numpy autogluon ctgan sentence-transformers mlflow"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 70,
              "statement_ids": [
                66,
                67,
                68,
                69,
                70
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.7373954Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:23:28.737871Z",
              "execution_finish_time": "2025-08-04T00:24:10.4878973Z",
              "parent_msg_id": "1ee80a07-bacc-49c4-a05e-4bb5513aa5cb"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 70, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 351,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (0.13.0)\nRequirement already satisfied: xgboost in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: shap in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (0.44.0)\nRequirement already satisfied: scipy in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (1.11.4)\nRequirement already satisfied: scikit-learn in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (1.6.1)\nRequirement already satisfied: pandas in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (2.3.1)\nRequirement already satisfied: numpy in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: autogluon in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (1.4.0)\nRequirement already satisfied: ctgan in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (0.11.0)\nRequirement already satisfied: sentence-transformers in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (5.0.0)\nRequirement already satisfied: mlflow in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: joblib<2,>=1.1.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from imbalanced-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from imbalanced-learn) (3.3.0)\nRequirement already satisfied: tqdm>=4.27.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (23.2)\nRequirement already satisfied: slicer==0.0.7 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (0.59.0)\nRequirement already satisfied: cloudpickle in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from shap) (2.2.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from pandas) (2.9.0)\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: tzdata>=2022.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: autogluon.core[all]==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.features==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.tabular[all]==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.multimodal==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: autogluon.timeseries[all]==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon) (1.4.0)\nRequirement already satisfied: networkx<4,>=3.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (3.2.1)\nRequirement already satisfied: requests in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (2.32.4)\nRequirement already satisfied: matplotlib<3.11,>=3.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (3.8.2)\nRequirement already satisfied: boto3<2,>=1.10 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (1.40.1)\nRequirement already satisfied: autogluon.common==1.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (0.2.7)\nRequirement already satisfied: pyarrow>=15.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (20.0.0)\nRequirement already satisfied: ray[default,tune]<2.45,>=2.10.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (2.44.1)\nRequirement already satisfied: Pillow<12,>=10.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (10.2.0)\nRequirement already satisfied: torch<2.8,>=2.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.7.1)\nRequirement already satisfied: lightning<2.8,>=2.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.5.2)\nRequirement already satisfied: transformers[sentencepiece]<4.50,>=4.38.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (4.49.0)\nRequirement already satisfied: accelerate<2.0,>=0.34.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.9.0)\nRequirement already satisfied: fsspec[http]<=2025.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2024.2.0)\nRequirement already satisfied: jsonschema<4.24,>=4.18 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (4.21.1)\nRequirement already satisfied: seqeval<1.3.0,>=1.2.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.2.2)\nRequirement already satisfied: evaluate<0.5.0,>=0.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.4.5)\nRequirement already satisfied: timm<1.0.7,>=0.9.5 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.0.3)\nRequirement already satisfied: torchvision<0.23.0,>=0.16.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.22.1)\nRequirement already satisfied: scikit-image<0.26.0,>=0.19.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.25.2)\nRequirement already satisfied: text-unidecode<1.4,>=1.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.3)\nRequirement already satisfied: torchmetrics<1.8,>=1.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.7.4)\nRequirement already satisfied: omegaconf<2.4.0,>=2.1.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.3.0)\nRequirement already satisfied: pytorch-metric-learning<2.9,>=1.3.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.8.1)\nRequirement already satisfied: nlpaug<1.2.0,>=1.1.10 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.1.11)\nRequirement already satisfied: nltk<3.10,>=3.4.5 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (3.9.1)\nRequirement already satisfied: openmim<0.4.0,>=0.3.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.9)\nRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.7.1)\nRequirement already satisfied: jinja2<3.2,>=3.0.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (3.1.3)\nRequirement already satisfied: tensorboard<3,>=2.9 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.15.2)\nRequirement already satisfied: pytesseract<0.4,>=0.3.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.13)\nRequirement already satisfied: nvidia-ml-py3<8.0,>=7.352.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (7.352.0)\nRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.17.0)\nRequirement already satisfied: catboost<1.3,>=1.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (1.2.8)\nRequirement already satisfied: fastai<2.9,>=2.3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.8.2)\nRequirement already satisfied: loguru in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.7.3)\nRequirement already satisfied: lightgbm<4.7,>=4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (4.2.0)\nRequirement already satisfied: einx in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.3.0)\nRequirement already satisfied: spacy<3.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (3.8.7)\nRequirement already satisfied: huggingface-hub[torch] in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.34.3)\nRequirement already satisfied: pytorch-lightning in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (2.5.2)\nRequirement already satisfied: gluonts<0.17,>=0.15.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.16.2)\nRequirement already satisfied: statsforecast<2.0.2,>=1.7.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (2.0.1)\nRequirement already satisfied: mlforecast<0.15.0,>=0.14.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.14.0)\nRequirement already satisfied: utilsforecast<0.2.12,>=0.2.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.2.11)\nRequirement already satisfied: coreforecast<0.0.17,>=0.0.12 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.0.16)\nRequirement already satisfied: fugue>=0.9.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (0.9.1)\nRequirement already satisfied: orjson~=3.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (3.11.1)\nRequirement already satisfied: psutil<7.1.0,>=5.7.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from autogluon.common==1.4.0->autogluon.core[all]==1.4.0->autogluon) (5.9.8)\nRequirement already satisfied: rdt>=1.14.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ctgan) (1.17.1)\nRequirement already satisfied: typing_extensions>=4.5.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from sentence-transformers) (4.14.1)\nRequirement already satisfied: mlflow-skinny==3.1.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (3.1.4)\nRequirement already satisfied: Flask<4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow) (3.0.2)\nRequirement already satisfied: alembic!=1.10.0,<2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (1.16.4)\nRequirement already satisfied: docker<8,>=4.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: graphene<4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow) (23.0.0)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow) (2.0.28)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (8.1.7)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.61.0)\nRequirement already satisfied: fastapi<1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.42)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (7.0.2)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (4.24.4)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (2.11.7)\nRequirement already satisfied: pyyaml<7,>=5.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (6.0.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.4.4)\nRequirement already satisfied: uvicorn<1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\nRequirement already satisfied: Mako in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: tomli in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (2.0.1)\nRequirement already satisfied: urllib3>=1.26.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.1.0)\nRequirement already satisfied: Werkzeug>=3.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.1)\nRequirement already satisfied: itsdangerous>=2.1.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from Flask<4->mlflow) (2.1.2)\nRequirement already satisfied: blinker>=1.6.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from Flask<4->mlflow) (1.7.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.6)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: filelock in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from huggingface-hub[torch]->autogluon.tabular[all]==1.4.0->autogluon) (3.13.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from huggingface-hub[torch]->autogluon.tabular[all]==1.4.0->autogluon) (1.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (4.49.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core[all]==1.4.0->autogluon) (3.1.2)\nRequirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: Faker>=17 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from rdt>=1.14.0->ctgan) (37.5.3)\nRequirement already satisfied: greenlet!=0.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nRequirement already satisfied: sympy>=1.13.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.14.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.11.1.6)\nRequirement already satisfied: triton==3.3.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (3.3.1)\nRequirement already satisfied: setuptools>=40.8.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from triton==3.3.1->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (69.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (2023.12.25)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.21.4)\nRequirement already satisfied: safetensors>=0.4.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.5.3)\nRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from numba->shap) (0.42.0)\nRequirement already satisfied: botocore<1.41.0,>=1.40.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core[all]==1.4.0->autogluon) (1.40.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core[all]==1.4.0->autogluon) (1.0.1)\nRequirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core[all]==1.4.0->autogluon) (0.13.1)\nRequirement already satisfied: graphviz in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (0.21)\nRequirement already satisfied: plotly in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (5.18.0)\nRequirement already satisfied: google-auth~=2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.28.2)\nRequirement already satisfied: datasets>=2.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (4.0.0)\nRequirement already satisfied: dill in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.3.8)\nRequirement already satisfied: xxhash in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: multiprocess in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.70.16)\nRequirement already satisfied: pip in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (23.1.2)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.7)\nRequirement already satisfied: fastcore<1.9,>=1.8.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.8.7)\nRequirement already satisfied: fasttransform>=0.0.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.2)\nRequirement already satisfied: fastprogress>=0.2.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.0.3)\nRequirement already satisfied: plum-dispatch in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (2.5.7)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (3.9.3)\nRequirement already satisfied: triad>=0.9.7 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.9.8)\nRequirement already satisfied: adagio>=0.2.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.2.6)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (4.0.11)\nRequirement already satisfied: toolz~=0.10 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.12.1)\nRequirement already satisfied: future in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (1.0.0)\nRequirement already satisfied: py4j in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (0.10.9.7)\nRequirement already satisfied: zipp>=0.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.4.0->autogluon) (2.1.5)\nRequirement already satisfied: attrs>=22.2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.33.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.18.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.15.0)\nRequirement already satisfied: optuna in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.4.0)\nRequirement already satisfied: window-ops in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.0.15)\nRequirement already satisfied: gdown>=4.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (5.2.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.4.0->autogluon) (4.9.3)\nRequirement already satisfied: colorama in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.4.6)\nRequirement already satisfied: model-index in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.11)\nRequirement already satisfied: opendatalab in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.0.10)\nRequirement already satisfied: rich in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (14.1.0)\nRequirement already satisfied: tabulate in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.9.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow) (0.57b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.4.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.0.8)\nRequirement already satisfied: aiosignal in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.3.1)\nRequirement already satisfied: frozenlist in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.4.1)\nRequirement already satisfied: tensorboardX>=1.9 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (2.6.4)\nRequirement already satisfied: aiohttp_cors in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.8.1)\nRequirement already satisfied: colorful in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.5.7)\nRequirement already satisfied: py-spy>=0.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: grpcio>=1.42.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.59.3)\nRequirement already satisfied: opencensus in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.11.4)\nRequirement already satisfied: prometheus_client>=0.7.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.20.0)\nRequirement already satisfied: smart_open in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (7.3.0.post1)\nRequirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (20.23.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (2024.2.2)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2025.5.10)\nRequirement already satisfied: lazy-loader>=0.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (0.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (8.3.4)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.16.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.5.0)\nRequirement already satisfied: statsmodels>=0.13.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.14.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from sympy>=1.13.3->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: absl-py>=0.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (2.1.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.5.1)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (0.7.0)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.2.0)\nRequirement already satisfied: h11>=0.8 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\nRequirement already satisfied: frozendict in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from einx->autogluon.tabular[all]==1.4.0->autogluon) (2.4.6)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (4.0.3)\nRequirement already satisfied: beautifulsoup4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (4.12.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (1.4.0)\nRequirement already satisfied: language-data>=1.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (3.7.1)\nRequirement already satisfied: patsy>=0.5.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.5.6)\nRequirement already satisfied: blis<1.3.0,>=1.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.2.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.1.5)\nRequirement already satisfied: fs in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.4.16)\nRequirement already satisfied: shellingham>=1.3.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (2.17.2)\nRequirement already satisfied: distlib<1,>=0.3.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.3.8)\nRequirement already satisfied: platformdirs<4,>=3.5.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (3.11.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.21.1)\nRequirement already satisfied: wrapt in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from smart_open->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.14.1)\nRequirement already satisfied: ordered-set in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (4.1.0)\nRequirement already satisfied: opencensus-context>=0.1.3 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (0.1.3)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (2.17.1)\nRequirement already satisfied: pycryptodome in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.23.0)\nRequirement already satisfied: openxlab in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.0.11)\nRequirement already satisfied: colorlog in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries[all]==1.4.0->autogluon) (6.9.0)\nRequirement already satisfied: tenacity>=6.2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (8.2.3)\nRequirement already satisfied: beartype>=0.16.2 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from plum-dispatch->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.21.0)\nRequirement already satisfied: sniffio>=1.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\nRequirement already satisfied: exceptiongroup in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.2.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0->autogluon.core[all]==1.4.0->autogluon) (1.63.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.2.1)\nRequirement already satisfied: mdurl~=0.1 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.2)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.2.2)\nRequirement already satisfied: soupsieve>1.2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (2.5)\nRequirement already satisfied: appdirs~=1.4.3 in /nfs4/pyenv-d09ef3f6-4f29-40a2-8b55-d7c7427d95a2/lib/python3.10/site-packages (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.4.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests->autogluon.core[all]==1.4.0->autogluon) (1.7.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 351,
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: PySpark kernel has been restarted to use updated packages.\n\n"
          ]
        }
      ],
      "execution_count": 351,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import json\n",
        "# import os\n",
        "\n",
        "# def load_athlete_input():\n",
        "#     # Look for real, user-supplied arguments (not -f or .json connection files)\n",
        "#     cli_args = [arg for arg in sys.argv[1:] if not (arg.startswith('-f') or arg.endswith('.json'))]\n",
        "#     if cli_args:\n",
        "#         arg = cli_args[0]\n",
        "#         try:\n",
        "#             athlete_input = json.loads(arg)\n",
        "#             print(\"Loaded athlete input from JSON string.\")\n",
        "#         except json.JSONDecodeError:\n",
        "#             if os.path.isfile(arg):\n",
        "#                 try:\n",
        "#                     with open(arg, \"r\") as f:\n",
        "#                         athlete_input = json.load(f)\n",
        "#                     print(f\"Loaded athlete input from file: {arg}\")\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Failed to parse JSON file: {e}\")\n",
        "#                     athlete_input = None\n",
        "#             else:\n",
        "#                 print(f\"Input is neither a valid JSON string nor a file: {arg}\")\n",
        "#                 athlete_input = None\n",
        "#     else:\n",
        "#         # Default for notebook/test/dev\n",
        "#         athlete_input = {\n",
        "#             'Senior_Yds': 1123, 'Senior_Avg': 17.3, 'Senior_Rec': 65, 'Senior_TD': 12, 'Senior_Rush_Yds': 100,\n",
        "#             'Height_Inches': 71, 'Weight_Lbs': 180, 'Forty_Yard_Dash': 4.40, 'Vertical_Jump': 39, 'Shuttle': 4.05,\n",
        "#             'Broad_Jump': 125, 'State': 'TX', 'position': 'WR', 'grad_year': 2025\n",
        "#         }\n",
        "#         print(\"No valid user input detected  using default test athlete.\")\n",
        "#     if not isinstance(athlete_input, dict):\n",
        "#         raise ValueError(\"No valid athlete input found (check input or Synapse pipeline config).\")\n",
        "#     return athlete_input\n",
        "\n",
        "# athlete_input = load_athlete_input()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 72,
              "statement_ids": [
                72
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.4433647Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:19.0426941Z",
              "execution_finish_time": "2025-08-04T00:24:19.3102915Z",
              "parent_msg_id": "f1c58c20-6313-4a70-9d12-317ecef3b6df"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 72, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 352,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import mstats, percentileofscore \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "np.random.seed(42)\n",
        "\n",
        "# CRITICAL: Add XGBoost safeguard function to prevent duplicate column errors\n",
        "def xgboost_safeguard(X_train, X_test, step_name=\"Model Training\"):\n",
        "    \"\"\"\n",
        "    Comprehensive safeguard function to ensure XGBoost compatibility\n",
        "    Removes duplicate columns and validates data before training\n",
        "    \"\"\"\n",
        "    print(f\"\\n  XGBoost Safeguard - {step_name}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Step 1: Check for duplicate column names\n",
        "    train_duplicates = X_train.columns.duplicated()\n",
        "    test_duplicates = X_test.columns.duplicated()\n",
        "    \n",
        "    if train_duplicates.any():\n",
        "        duplicate_cols = X_train.columns[train_duplicates].unique()\n",
        "        print(f\"  Found {len(duplicate_cols)} duplicate columns in training data: {list(duplicate_cols)}\")\n",
        "        X_train = X_train.loc[:, ~X_train.columns.duplicated(keep='first')]\n",
        "        print(f\" Removed duplicates from training data: {X_train.shape}\")\n",
        "    \n",
        "    if test_duplicates.any():\n",
        "        duplicate_cols = X_test.columns[test_duplicates].unique()\n",
        "        print(f\"  Found {len(duplicate_cols)} duplicate columns in test data: {list(duplicate_cols)}\")\n",
        "        X_test = X_test.loc[:, ~X_test.columns.duplicated(keep='first')]\n",
        "        print(f\" Removed duplicates from test data: {X_test.shape}\")\n",
        "    \n",
        "    # Step 2: Ensure both datasets have the same columns\n",
        "    train_cols = set(X_train.columns)\n",
        "    test_cols = set(X_test.columns)\n",
        "    \n",
        "    if train_cols != test_cols:\n",
        "        print(f\"  Column mismatch detected\")\n",
        "        print(f\"   Training columns: {len(train_cols)}\")\n",
        "        print(f\"   Test columns: {len(test_cols)}\")\n",
        "        \n",
        "        # Use intersection of columns\n",
        "        common_cols = list(train_cols & test_cols)\n",
        "        print(f\"   Using {len(common_cols)} common columns\")\n",
        "        \n",
        "        X_train = X_train[common_cols]\n",
        "        X_test = X_test[common_cols]\n",
        "    \n",
        "    # Step 3: Validate data types for XGBoost compatibility\n",
        "    invalid_dtypes = []\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype not in ['int64', 'float64', 'int32', 'float32', 'bool', 'int8', 'float16']:\n",
        "            invalid_dtypes.append((col, X_train[col].dtype))\n",
        "    \n",
        "    if invalid_dtypes:\n",
        "        print(f\"  Found {len(invalid_dtypes)} columns with invalid dtypes:\")\n",
        "        for col, dtype in invalid_dtypes[:5]:  # Show first 5\n",
        "            print(f\"   {col}: {dtype}\")\n",
        "        \n",
        "        # Convert to numeric\n",
        "        for col, _ in invalid_dtypes:\n",
        "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)\n",
        "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n",
        "        print(f\" Converted invalid dtypes to numeric\")\n",
        "    \n",
        "    # Step 4: Check for infinite values\n",
        "    train_inf = np.isinf(X_train).any().any()\n",
        "    test_inf = np.isinf(X_test).any().any()\n",
        "    \n",
        "    if train_inf or test_inf:\n",
        "        print(f\"  Found infinite values - replacing with NaN then 0\")\n",
        "        X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        print(f\" Replaced infinite values\")\n",
        "    \n",
        "    # Step 5: Final validation - absolutely no duplicates\n",
        "    final_train_dups = X_train.columns.duplicated().any()\n",
        "    final_test_dups = X_test.columns.duplicated().any()\n",
        "    \n",
        "    if final_train_dups or final_test_dups:\n",
        "        print(f\" CRITICAL: Still have duplicates after safeguards!\")\n",
        "        # Nuclear option: rename all columns to generic names\n",
        "        n_cols = len(X_train.columns)\n",
        "        new_col_names = [f\"feature_{i:03d}\" for i in range(n_cols)]\n",
        "        X_train.columns = new_col_names\n",
        "        X_test.columns = new_col_names\n",
        "        print(f\" Applied nuclear fix: renamed all columns to generic names\")\n",
        "    \n",
        "    print(f\" XGBoost Safeguard Complete\")\n",
        "    print(f\"   Training data: {X_train.shape}\")\n",
        "    print(f\"   Test data: {X_test.shape}\")\n",
        "    print(f\"   Data types: {X_train.dtypes.value_counts().to_dict()}\")\n",
        "    \n",
        "    return X_train, X_test\n",
        "\n",
        "# Modular functions for easy upgrades\n",
        "def augment_data(X, y):\n",
        "    \"\"\"ADASYN for now, but easy to swap with GAN/ctgan/SMOTE.\"\"\"\n",
        "    try:\n",
        "        adasyn = ADASYN(random_state=42, sampling_strategy='auto')\n",
        "        X_aug, y_aug = adasyn.fit_resample(X, y)\n",
        "        print(f\"ADASYN: {X.shape} -> {X_aug.shape}\")\n",
        "        return X_aug, y_aug\n",
        "    except ValueError as e:\n",
        "        print(f\"ADASYN failed ({e}), using original data\")\n",
        "        return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"XGBClassifier for now, easy to swap with AutoGluon/CatBoost/Ordinal.\"\"\"\n",
        "    # Apply XGBoost safeguard before training\n",
        "    if len(X.shape) == 2:  # Only apply to feature matrices\n",
        "        # Create dummy test set for validation (will be ignored)\n",
        "        X_dummy = X.iloc[:5].copy() if hasattr(X, 'iloc') else X[:5].copy()\n",
        "        X, X_dummy = xgboost_safeguard(X, X_dummy, \"Pre-Training Validation\")\n",
        "    \n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100, \n",
        "        max_depth=3, \n",
        "        learning_rate=0.1, \n",
        "        eval_metric='mlogloss', \n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    return model"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 73,
              "statement_ids": [
                73
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.5851287Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:19.3230131Z",
              "execution_finish_time": "2025-08-04T00:24:21.228619Z",
              "parent_msg_id": "712e176a-9e26-4e95-aa36-e888bc6ad3af"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 73, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 353,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Cell 3: Enhanced Data Acquisition with Intelligent Benchmark Imputation\n",
        "\n",
        "def load_base_csv_enhanced(position):\n",
        "    \"\"\"Enhanced data loading with intelligent missing data handling\"\"\"\n",
        "    # Use abfss for Synapse/ADLS Gen2 (recommended with Linked Service)\n",
        "    paths = {\n",
        "        'qb': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/221 QB FINAL - Sheet1.csv',\n",
        "        'rb': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/RB list 1 - Sheet1.csv',\n",
        "        'wr': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/wr final - Sheet1.csv',\n",
        "        'db': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/db.csv',\n",
        "        'lb': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/lb.csv',\n",
        "        'te': 'abfss://data@recruitrevealstorage2026.dfs.core.windows.net/te.csv'\n",
        "    }\n",
        "    path = paths.get(position, paths['qb'])\n",
        "    try:\n",
        "        df_spark = spark.read.csv(path, header=True, inferSchema=True)\n",
        "        df = df_spark.toPandas()\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "        print(f\"Loaded {len(df)} rows for {position.upper()}\")\n",
        "        if 'division' in df.columns:\n",
        "            print(f\"Unique divisions for {position.upper()}: {df['division'].unique()}\")\n",
        "        else:\n",
        "            print(f\"No 'division' column found for {position.upper()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Load failed for {position}: {e}\")\n",
        "        df = pd.DataFrame(columns=[\n",
        "            'name','division','state','height_inches','weight_lbs','senior_yds','senior_avg','senior_rec',\n",
        "            'senior_td','junior_yds','junior_avg','junior_rec','junior_td','senior_ypg','senior_tds',\n",
        "            'senior_comp_pct','senior_ypc','senior_rush_yds','grad_year'\n",
        "        ])\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "    \n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Normalize division spelling everywhere!\n",
        "    if 'division' in df.columns:\n",
        "        df['division'] = (\n",
        "            df['division'].astype(str)\n",
        "            .str.strip()\n",
        "            .str.upper()\n",
        "            .str.replace('POWER5', 'POWER 5', regex=False)\n",
        "            .str.replace('FBS', 'POWER 5', regex=False)\n",
        "            .str.replace('D3/NAIA', 'D3', regex=False)\n",
        "            .str.replace('NAIA', 'NAIA', regex=False)\n",
        "        )\n",
        "    \n",
        "    # Always provide 'position' for downstream logic\n",
        "    df['position'] = position.lower()\n",
        "    return df\n",
        "\n",
        "# High School Football Recruiting Guidelines - Combine Benchmarks by Position/Division\n",
        "# Based on industry standards and recruiting data\n",
        "COMBINE_BENCHMARKS = {\n",
        "    'qb': {\n",
        "        'POWER 5': {'forty_yard_dash': (4.6, 4.9), 'vertical_jump': (30, 34), 'shuttle': (4.3, 4.6), 'broad_jump': (108, 118)},\n",
        "        'FCS': {'forty_yard_dash': (4.7, 5.0), 'vertical_jump': (28, 32), 'shuttle': (4.4, 4.7), 'broad_jump': (102, 112)},\n",
        "        'D2': {'forty_yard_dash': (4.8, 5.1), 'vertical_jump': (26, 30), 'shuttle': (4.5, 4.8), 'broad_jump': (96, 106)},\n",
        "        'D3': {'forty_yard_dash': (4.9, 5.3), 'vertical_jump': (24, 28), 'shuttle': (4.6, 4.9), 'broad_jump': (90, 100)},\n",
        "        'NAIA': {'forty_yard_dash': (4.8, 5.2), 'vertical_jump': (25, 29), 'shuttle': (4.5, 4.8), 'broad_jump': (92, 102)}\n",
        "    },\n",
        "    'rb': {\n",
        "        'POWER 5': {'forty_yard_dash': (4.2, 4.5), 'vertical_jump': (34, 38), 'shuttle': (4.0, 4.3), 'broad_jump': (120, 130)},\n",
        "        'FCS': {'forty_yard_dash': (4.3, 4.6), 'vertical_jump': (32, 36), 'shuttle': (4.1, 4.4), 'broad_jump': (110, 120)},\n",
        "        'D2': {'forty_yard_dash': (4.4, 4.7), 'vertical_jump': (30, 34), 'shuttle': (4.2, 4.5), 'broad_jump': (100, 110)},\n",
        "        'D3': {'forty_yard_dash': (4.5, 4.8), 'vertical_jump': (28, 32), 'shuttle': (4.3, 4.6), 'broad_jump': (95, 105)},\n",
        "        'NAIA': {'forty_yard_dash': (4.4, 4.7), 'vertical_jump': (29, 33), 'shuttle': (4.2, 4.5), 'broad_jump': (98, 108)}\n",
        "    },\n",
        "    'wr': {\n",
        "        'POWER 5': {'forty_yard_dash': (4.4, 4.7), 'vertical_jump': (34, 38), 'shuttle': (4.1, 4.4), 'broad_jump': (120, 130)},\n",
        "        'FCS': {'forty_yard_dash': (4.5, 4.8), 'vertical_jump': (33, 37), 'shuttle': (4.2, 4.5), 'broad_jump': (110, 120)},\n",
        "        'D2': {'forty_yard_dash': (4.6, 4.9), 'vertical_jump': (31, 35), 'shuttle': (4.3, 4.6), 'broad_jump': (100, 110)},\n",
        "        'D3': {'forty_yard_dash': (4.7, 5.0), 'vertical_jump': (29, 33), 'shuttle': (4.4, 4.7), 'broad_jump': (95, 105)},\n",
        "        'NAIA': {'forty_yard_dash': (4.6, 4.9), 'vertical_jump': (30, 34), 'shuttle': (4.3, 4.6), 'broad_jump': (98, 108)}\n",
        "    }\n",
        "}\n",
        "\n",
        "def intelligent_combine_imputation(df, position):\n",
        "    \"\"\"Intelligent imputation using benchmark ranges with Bayesian-inspired priors\"\"\"\n",
        "    df = df.copy()\n",
        "    position = position.lower()\n",
        "    \n",
        "    # Normalize division for lookup\n",
        "    df['division_lookup'] = df['division'].str.upper()\n",
        "    \n",
        "    combine_metrics = ['forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump']\n",
        "    position_benchmarks = COMBINE_BENCHMARKS.get(position, COMBINE_BENCHMARKS['qb'])\n",
        "    \n",
        "    imputation_log = []\n",
        "    \n",
        "    for metric in combine_metrics:\n",
        "        if metric not in df.columns:\n",
        "            df[metric] = np.nan\n",
        "            df[f'{metric}_imputed'] = True\n",
        "            imputation_log.append(f\"Created missing column {metric}\")\n",
        "        else:\n",
        "            df[f'{metric}_imputed'] = df[metric].isna()\n",
        "        \n",
        "        missing_mask = df[metric].isna()\n",
        "        if missing_mask.any():\n",
        "            missing_count = missing_mask.sum()\n",
        "            imputation_log.append(f\"Imputing {missing_count} missing {metric} values\")\n",
        "            \n",
        "            # Impute based on division-specific benchmarks\n",
        "            for division in df['division_lookup'].unique():\n",
        "                if pd.isna(division):\n",
        "                    continue\n",
        "                    \n",
        "                div_mask = (df['division_lookup'] == division) & missing_mask\n",
        "                if not div_mask.any():\n",
        "                    continue\n",
        "                \n",
        "                # Get benchmark range for this position/division\n",
        "                if division in position_benchmarks:\n",
        "                    min_val, max_val = position_benchmarks[division][metric]\n",
        "                else:\n",
        "                    # Fallback to D3 benchmarks if division not found\n",
        "                    min_val, max_val = position_benchmarks['D3'][metric]\n",
        "                \n",
        "                # Bayesian-inspired imputation: use normal distribution centered on range midpoint\n",
        "                mean_val = (min_val + max_val) / 2\n",
        "                std_val = (max_val - min_val) / 4  # Assume 95% of values within range\n",
        "                \n",
        "                # Generate values and clip to realistic range\n",
        "                n_samples = div_mask.sum()\n",
        "                imputed_values = np.random.normal(mean_val, std_val, n_samples)\n",
        "                imputed_values = np.clip(imputed_values, min_val * 0.9, max_val * 1.1)\n",
        "                \n",
        "                df.loc[div_mask, metric] = imputed_values\n",
        "                imputation_log.append(f\"  {division}: {n_samples} values from N({mean_val:.2f}, {std_val:.2f})\")\n",
        "    \n",
        "    print(f\"Combine imputation for {position.upper()}:\")\n",
        "    for log_entry in imputation_log:\n",
        "        print(f\"  {log_entry}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def enrich_data_enhanced(df, position, year=2025):\n",
        "    \"\"\"Enhanced data enrichment with more balanced synthetic samples\"\"\"\n",
        "    enrich_data = {\n",
        "        'qb': [\n",
        "            # Power 5 samples\n",
        "            {'name': 'Elite Power 5 QB', 'height_inches': 75, 'weight_lbs': 215, 'senior_ypg': 285, 'senior_tds': 28, 'senior_comp_pct': 68, 'state': 'TX', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 112},\n",
        "            {'name': 'Good Power 5 QB', 'height_inches': 73, 'weight_lbs': 205, 'senior_ypg': 255, 'senior_tds': 24, 'senior_comp_pct': 64, 'state': 'CA', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 31, 'shuttle': 4.5, 'broad_jump': 110},\n",
        "            # FCS samples\n",
        "            {'name': 'Elite FCS QB', 'height_inches': 73, 'weight_lbs': 200, 'senior_ypg': 225, 'senior_tds': 22, 'senior_comp_pct': 62, 'state': 'FL', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 30, 'shuttle': 4.5, 'broad_jump': 108},\n",
        "            {'name': 'Good FCS QB', 'height_inches': 72, 'weight_lbs': 195, 'senior_ypg': 200, 'senior_tds': 18, 'senior_comp_pct': 58, 'state': 'GA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 29, 'shuttle': 4.6, 'broad_jump': 105},\n",
        "            {'name': 'Solid FCS QB', 'height_inches': 71, 'weight_lbs': 190, 'senior_ypg': 180, 'senior_tds': 16, 'senior_comp_pct': 55, 'state': 'NC', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 28, 'shuttle': 4.6, 'broad_jump': 103},\n",
        "            # D2 samples\n",
        "            {'name': 'Elite D2 QB', 'height_inches': 71, 'weight_lbs': 190, 'senior_ypg': 165, 'senior_tds': 16, 'senior_comp_pct': 58, 'state': 'OH', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 28, 'shuttle': 4.6, 'broad_jump': 101},\n",
        "            {'name': 'Good D2 QB', 'height_inches': 70, 'weight_lbs': 185, 'senior_ypg': 145, 'senior_tds': 14, 'senior_comp_pct': 54, 'state': 'MI', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 5.0, 'vertical_jump': 27, 'shuttle': 4.7, 'broad_jump': 98},\n",
        "            # D3 samples\n",
        "            {'name': 'Elite D3 QB', 'height_inches': 70, 'weight_lbs': 180, 'senior_ypg': 125, 'senior_tds': 12, 'senior_comp_pct': 52, 'state': 'IL', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 5.1, 'vertical_jump': 26, 'shuttle': 4.7, 'broad_jump': 95},\n",
        "            {'name': 'Good D3 QB', 'height_inches': 69, 'weight_lbs': 175, 'senior_ypg': 105, 'senior_tds': 10, 'senior_comp_pct': 48, 'state': 'PA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 5.2, 'vertical_jump': 25, 'shuttle': 4.8, 'broad_jump': 92},\n",
        "            # NAIA samples\n",
        "            {'name': 'Elite NAIA QB', 'height_inches': 70, 'weight_lbs': 185, 'senior_ypg': 135, 'senior_tds': 13, 'senior_comp_pct': 55, 'state': 'KS', 'division': 'NAIA', 'grad_year': 2025, 'forty_yard_dash': 5.0, 'vertical_jump': 27, 'shuttle': 4.6, 'broad_jump': 97}\n",
        "        ],\n",
        "        'rb': [\n",
        "            # Power 5 samples\n",
        "            {'name': 'Elite Power 5 RB', 'height_inches': 70, 'weight_lbs': 205, 'senior_ypg': 145, 'senior_tds': 18, 'senior_ypc': 5.8, 'state': 'TX', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.35, 'vertical_jump': 36, 'shuttle': 4.1, 'broad_jump': 125},\n",
        "            {'name': 'Good Power 5 RB', 'height_inches': 69, 'weight_lbs': 195, 'senior_ypg': 125, 'senior_tds': 15, 'senior_ypc': 5.2, 'state': 'FL', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.4, 'vertical_jump': 35, 'shuttle': 4.2, 'broad_jump': 122},\n",
        "            # FCS samples\n",
        "            {'name': 'Elite FCS RB', 'height_inches': 69, 'weight_lbs': 190, 'senior_ypg': 115, 'senior_tds': 14, 'senior_ypc': 4.8, 'state': 'CA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.4, 'vertical_jump': 34, 'shuttle': 4.2, 'broad_jump': 115},\n",
        "            {'name': 'Good FCS RB', 'height_inches': 68, 'weight_lbs': 185, 'senior_ypg': 95, 'senior_tds': 12, 'senior_ypc': 4.4, 'state': 'GA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 33, 'shuttle': 4.3, 'broad_jump': 112},\n",
        "            {'name': 'Solid FCS RB', 'height_inches': 67, 'weight_lbs': 180, 'senior_ypg': 85, 'senior_tds': 10, 'senior_ypc': 4.1, 'state': 'NC', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            # D2 samples\n",
        "            {'name': 'Elite D2 RB', 'height_inches': 68, 'weight_lbs': 180, 'senior_ypg': 85, 'senior_tds': 11, 'senior_ypc': 4.2, 'state': 'OH', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 32, 'shuttle': 4.3, 'broad_jump': 105},\n",
        "            {'name': 'Good D2 RB', 'height_inches': 67, 'weight_lbs': 175, 'senior_ypg': 75, 'senior_tds': 9, 'senior_ypc': 3.9, 'state': 'MI', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 31, 'shuttle': 4.4, 'broad_jump': 102},\n",
        "            # D3 samples\n",
        "            {'name': 'Elite D3 RB', 'height_inches': 67, 'weight_lbs': 170, 'senior_ypg': 65, 'senior_tds': 8, 'senior_ypc': 3.6, 'state': 'IL', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 30, 'shuttle': 4.4, 'broad_jump': 98},\n",
        "            {'name': 'Good D3 RB', 'height_inches': 66, 'weight_lbs': 165, 'senior_ypg': 55, 'senior_tds': 7, 'senior_ypc': 3.3, 'state': 'PA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 29, 'shuttle': 4.5, 'broad_jump': 96},\n",
        "            # NAIA samples\n",
        "            {'name': 'Elite NAIA RB', 'height_inches': 67, 'weight_lbs': 175, 'senior_ypg': 75, 'senior_tds': 9, 'senior_ypc': 3.8, 'state': 'KS', 'division': 'NAIA', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 31, 'shuttle': 4.3, 'broad_jump': 103}\n",
        "        ],\n",
        "        'wr': [\n",
        "            # Power 5 samples\n",
        "            {'name': 'Elite Power 5 WR', 'height_inches': 72, 'weight_lbs': 185, 'senior_yds': 1100, 'senior_avg': 18.5, 'senior_rec': 60, 'senior_td': 14, 'state': 'TX', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.45, 'vertical_jump': 36, 'shuttle': 4.2, 'broad_jump': 125},\n",
        "            {'name': 'Good Power 5 WR', 'height_inches': 71, 'weight_lbs': 180, 'senior_yds': 950, 'senior_avg': 16.8, 'senior_rec': 55, 'senior_td': 12, 'state': 'FL', 'division': 'POWER 5', 'grad_year': 2025, 'forty_yard_dash': 4.5, 'vertical_jump': 35, 'shuttle': 4.3, 'broad_jump': 122},\n",
        "            # FCS samples\n",
        "            {'name': 'Elite FCS WR', 'height_inches': 71, 'weight_lbs': 175, 'senior_yds': 850, 'senior_avg': 16.0, 'senior_rec': 52, 'senior_td': 10, 'state': 'CA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 35, 'shuttle': 4.3, 'broad_jump': 115},\n",
        "            {'name': 'Good FCS WR', 'height_inches': 70, 'weight_lbs': 170, 'senior_yds': 750, 'senior_avg': 15.2, 'senior_rec': 48, 'senior_td': 8, 'state': 'GA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 34, 'shuttle': 4.4, 'broad_jump': 112},\n",
        "            {'name': 'Solid FCS WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 650, 'senior_avg': 14.5, 'senior_rec': 44, 'senior_td': 7, 'state': 'NC', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 33, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            # D2 samples\n",
        "            {'name': 'Elite D2 WR', 'height_inches': 70, 'weight_lbs': 170, 'senior_yds': 600, 'senior_avg': 14.0, 'senior_rec': 42, 'senior_td': 7, 'state': 'OH', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 33, 'shuttle': 4.4, 'broad_jump': 105},\n",
        "            {'name': 'Good D2 WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 520, 'senior_avg': 13.2, 'senior_rec': 38, 'senior_td': 6, 'state': 'MI', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 32, 'shuttle': 4.5, 'broad_jump': 102},\n",
        "            # D3 samples\n",
        "            {'name': 'Elite D3 WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 450, 'senior_avg': 12.5, 'senior_rec': 35, 'senior_td': 5, 'state': 'IL', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 31, 'shuttle': 4.5, 'broad_jump': 98},\n",
        "            {'name': 'Good D3 WR', 'height_inches': 68, 'weight_lbs': 160, 'senior_yds': 380, 'senior_avg': 11.8, 'senior_rec': 32, 'senior_td': 4, 'state': 'PA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 30, 'shuttle': 4.6, 'broad_jump': 96},\n",
        "            # NAIA samples\n",
        "            {'name': 'Elite NAIA WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 500, 'senior_avg': 13.0, 'senior_rec': 38, 'senior_td': 6, 'state': 'KS', 'division': 'NAIA', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 100}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    position = position.lower()\n",
        "    enrich_df = pd.DataFrame(enrich_data.get(position, []))\n",
        "    if not enrich_df.empty:\n",
        "        enrich_df.columns = enrich_df.columns.str.strip().str.lower()\n",
        "        df = pd.concat([df, enrich_df], ignore_index=True)\n",
        "        print(f\"Added {len(enrich_df)} enhanced synthetic samples for {position.upper()}\")\n",
        "    \n",
        "    # Add hoops_vert feature for multi-sport athletes\n",
        "    df['hoops_vert'] = df.get('vertical_jump', 32)\n",
        "    \n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 74,
              "statement_ids": [
                74
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.679984Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:21.2407968Z",
              "execution_finish_time": "2025-08-04T00:24:21.8623887Z",
              "parent_msg_id": "acb6bd87-41d4-43ea-b4ff-09ada4541fc3"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 74, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 354,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Enrich Data (adds baseline FCS/D2/D3 to all positions)\n",
        "def enrich_data(df, position, year=2025):\n",
        "    enrich_data = {\n",
        "        'qb': [\n",
        "            {'name': 'Sample FCS QB', 'height_inches': 72, 'weight_lbs': 195, 'senior_ypg': 180, 'senior_tds': 20, 'senior_comp_pct': 60, 'state': 'CA', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 28, 'shuttle': 4.5, 'broad_jump': 105},\n",
        "            {'name': 'Sample D2 QB', 'height_inches': 71, 'weight_lbs': 185, 'senior_ypg': 140, 'senior_tds': 15, 'senior_comp_pct': 55, 'state': 'FL', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 26, 'shuttle': 4.6, 'broad_jump': 100},\n",
        "            {'name': 'Sample D3 QB', 'height_inches': 70, 'weight_lbs': 175, 'senior_ypg': 100, 'senior_tds': 10, 'senior_comp_pct': 50, 'state': 'GA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 5.0, 'vertical_jump': 24, 'shuttle': 4.7, 'broad_jump': 95}\n",
        "        ],\n",
        "        'rb': [\n",
        "            {'name': 'Sample FCS RB', 'height_inches': 68, 'weight_lbs': 185, 'senior_ypg': 110, 'senior_tds': 15, 'senior_ypc': 4.5, 'state': 'TX', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 30, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            {'name': 'Sample D2 RB', 'height_inches': 67, 'weight_lbs': 175, 'senior_ypg': 90, 'senior_tds': 10, 'senior_ypc': 4.0, 'state': 'FL', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 28, 'shuttle': 4.5, 'broad_jump': 105},\n",
        "            {'name': 'Sample D3 RB', 'height_inches': 66, 'weight_lbs': 165, 'senior_ypg': 70, 'senior_tds': 8, 'senior_ypc': 3.5, 'state': 'CA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.9, 'vertical_jump': 26, 'shuttle': 4.6, 'broad_jump': 100}\n",
        "        ],\n",
        "        'wr': [\n",
        "            {'name': 'Sample FCS WR', 'height_inches': 70, 'weight_lbs': 175, 'senior_yds': 800, 'senior_avg': 15, 'senior_rec': 50, 'senior_td': 8, 'state': 'TX', 'division': 'FCS', 'grad_year': 2025, 'forty_yard_dash': 4.6, 'vertical_jump': 32, 'shuttle': 4.4, 'broad_jump': 110},\n",
        "            {'name': 'Sample D2 WR', 'height_inches': 69, 'weight_lbs': 165, 'senior_yds': 600, 'senior_avg': 13, 'senior_rec': 40, 'senior_td': 6, 'state': 'FL', 'division': 'D2', 'grad_year': 2025, 'forty_yard_dash': 4.7, 'vertical_jump': 30, 'shuttle': 4.5, 'broad_jump': 105},\n",
        "            {'name': 'Sample D3 WR', 'height_inches': 68, 'weight_lbs': 160, 'senior_yds': 400, 'senior_avg': 11, 'senior_rec': 30, 'senior_td': 4, 'state': 'CA', 'division': 'D3', 'grad_year': 2025, 'forty_yard_dash': 4.8, 'vertical_jump': 28, 'shuttle': 4.6, 'broad_jump': 100}\n",
        "        ]\n",
        "    }\n",
        "    position = position.lower()\n",
        "    enrich_df = pd.DataFrame(enrich_data.get(position, []))\n",
        "    enrich_df.columns = enrich_df.columns.str.strip().str.lower()\n",
        "    df = pd.concat([df, enrich_df], ignore_index=True)\n",
        "    df['hoops_vert'] = df.get('vertical_jump', 32)\n",
        "    return df\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 75,
              "statement_ids": [
                75
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.6892073Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:21.8731237Z",
              "execution_finish_time": "2025-08-04T00:24:22.1397779Z",
              "parent_msg_id": "9baac53f-b0e6-48c5-9855-b71707708042"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 75, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 355,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Cell 4: Enhanced Preprocessing with Intelligent Imputation, Embeddings, and Advanced Features\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def create_state_embeddings(df):\n",
        "    \"\"\"Create state embeddings for talent hotbed representation\"\"\"\n",
        "    # DUPLICATE PREVENTION: Check if state features already exist\n",
        "    if 'state_talent_score' in df.columns:\n",
        "        print(\"    State embeddings already exist - skipping creation to prevent duplicates\")\n",
        "        return df\n",
        "    \n",
        "    # Define state talent tiers based on recruiting density and college production\n",
        "    state_tiers = {\n",
        "        'TX': 'tier_1',  # Elite talent hotbeds\n",
        "        'FL': 'tier_1',\n",
        "        'CA': 'tier_1', \n",
        "        'GA': 'tier_1',\n",
        "        \n",
        "        'OH': 'tier_2',  # Strong talent states\n",
        "        'PA': 'tier_2',\n",
        "        'NC': 'tier_2',\n",
        "        'VA': 'tier_2',\n",
        "        'MI': 'tier_2',\n",
        "        'IL': 'tier_2',\n",
        "        'LA': 'tier_2',\n",
        "        'AL': 'tier_2',\n",
        "        'TN': 'tier_2',\n",
        "        'SC': 'tier_2',\n",
        "        'AZ': 'tier_2',\n",
        "        'NJ': 'tier_2',\n",
        "        'MD': 'tier_2',\n",
        "        \n",
        "        'IN': 'tier_3',  # Moderate talent states\n",
        "        'MO': 'tier_3',\n",
        "        'WI': 'tier_3',\n",
        "        'MN': 'tier_3',\n",
        "        'IA': 'tier_3',\n",
        "        'KY': 'tier_3',\n",
        "        'OK': 'tier_3',\n",
        "        'AR': 'tier_3',\n",
        "        'MS': 'tier_3',\n",
        "        'KS': 'tier_3',\n",
        "        'CO': 'tier_3',\n",
        "        'OR': 'tier_3',\n",
        "        'WA': 'tier_3',\n",
        "        'CT': 'tier_3',\n",
        "        'NV': 'tier_3',\n",
        "        'UT': 'tier_3'\n",
        "    }\n",
        "    \n",
        "    # Create state embeddings using simple numeric encoding (avoid object columns)\n",
        "    df['state_talent_score'] = df['state'].str.upper().map({\n",
        "        'TX': 4, 'FL': 4, 'CA': 4, 'GA': 4,  # Elite\n",
        "        'OH': 3, 'PA': 3, 'NC': 3, 'VA': 3, 'MI': 3, 'IL': 3, 'LA': 3, 'AL': 3, 'TN': 3, 'SC': 3, 'AZ': 3, 'NJ': 3, 'MD': 3,  # Strong\n",
        "        'IN': 2, 'MO': 2, 'WI': 2, 'MN': 2, 'IA': 2, 'KY': 2, 'OK': 2, 'AR': 2, 'MS': 2, 'KS': 2, 'CO': 2, 'OR': 2, 'WA': 2, 'CT': 2, 'NV': 2, 'UT': 2  # Moderate\n",
        "    }).fillna(1).astype(int)  # Default for other states, ensure int type\n",
        "    \n",
        "    # Create binary indicators for state tiers (avoid object columns)\n",
        "    df['state_tier_1'] = (df['state_talent_score'] == 4).astype(int)  # Elite states\n",
        "    df['state_tier_2'] = (df['state_talent_score'] == 3).astype(int)  # Strong states\n",
        "    df['state_tier_3'] = (df['state_talent_score'] == 2).astype(int)  # Moderate states\n",
        "    df['state_tier_4'] = (df['state_talent_score'] == 1).astype(int)  # Other states\n",
        "    \n",
        "    return df\n",
        "\n",
        "def enhanced_feature_engineering(df, position):\n",
        "    \"\"\"Enhanced feature engineering with interaction terms and advanced metrics\"\"\"\n",
        "    df = df.copy()\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # DUPLICATE PREVENTION: Check if enhanced features already exist\n",
        "    if 'state_eff' in df.columns or 'bmi_ypg' in df.columns:\n",
        "        print(f\"    Enhanced features already exist - removing duplicates and reprocessing\")\n",
        "        # Remove existing enhanced features to prevent conflicts\n",
        "        enhanced_cols = ['state_eff', 'bmi_ypg', 'height_traj', 'speed_power_ratio', 'combine_confidence']\n",
        "        for col in enhanced_cols:\n",
        "            if col in df.columns:\n",
        "                df = df.drop(columns=[col])\n",
        "\n",
        "    # Ensure essential columns exist with intelligent defaults\n",
        "    essential_cols = ['height_inches', 'weight_lbs', 'position', 'division', 'state']\n",
        "    for col in essential_cols:\n",
        "        if col not in df.columns:\n",
        "            if col == 'height_inches':\n",
        "                df[col] = 70\n",
        "            elif col == 'weight_lbs':\n",
        "                df[col] = 180\n",
        "            elif col == 'position':\n",
        "                df[col] = position\n",
        "            elif col == 'division':\n",
        "                df[col] = 'D3'\n",
        "            elif col == 'state':\n",
        "                df[col] = 'ZZ'\n",
        "\n",
        "    # Apply intelligent combine imputation\n",
        "    df = intelligent_combine_imputation(df, position)\n",
        "    \n",
        "    # Create state embeddings (now returns only numeric columns)\n",
        "    df = create_state_embeddings(df)\n",
        "    \n",
        "    # Position-aware engineered features\n",
        "    df['games'] = 12\n",
        "    if 'senior_rec' in df.columns:\n",
        "        wr_mask = df['position'].str.lower() == 'wr'\n",
        "        df.loc[wr_mask, 'games'] = df.loc[wr_mask, 'senior_rec'].replace(0, np.nan).fillna(12).clip(8, 15)\n",
        "    if 'senior_yds' in df.columns and 'senior_ypg' in df.columns:\n",
        "        rb_qb_mask = df['position'].str.lower().isin(['rb', 'qb'])\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            games_calc = df.loc[rb_qb_mask, 'senior_yds'] / df.loc[rb_qb_mask, 'senior_ypg']\n",
        "            games_calc = games_calc.replace([np.inf, -np.inf], np.nan).fillna(12).clip(8, 15)\n",
        "            df.loc[rb_qb_mask, 'games'] = games_calc\n",
        "\n",
        "    # Enhanced all_purpose_game calculation for RBs\n",
        "    rb_mask = df['position'].str.lower() == 'rb'\n",
        "    if 'senior_yds' in df.columns:\n",
        "        if 'senior_rec_yds' in df.columns:\n",
        "            df.loc[rb_mask, 'all_purpose_game'] = (\n",
        "                df.loc[rb_mask, 'senior_yds'] + df.loc[rb_mask, 'senior_rec_yds']\n",
        "            ) / df.loc[rb_mask, 'games']\n",
        "        else:\n",
        "            df.loc[rb_mask, 'all_purpose_game'] = df.loc[rb_mask, 'senior_yds'] / df.loc[rb_mask, 'games']\n",
        "    else:\n",
        "        df['all_purpose_game'] = df.get('ypg', 0) + df.get('rec_ypg', 0)\n",
        "\n",
        "    # Derived per-game stats\n",
        "    df['rec_ypg'] = 0.0\n",
        "    df['ypg'] = 0.0\n",
        "    df['tds_game'] = 0.0\n",
        "    df['td_game'] = 0.0\n",
        "    \n",
        "    if 'senior_yds' in df.columns:\n",
        "        wr_mask = df['position'].str.lower() == 'wr'\n",
        "        df.loc[wr_mask, 'rec_ypg'] = df.loc[wr_mask, 'senior_yds'] / df.loc[wr_mask, 'games']\n",
        "        rb_qb_mask = df['position'].str.lower().isin(['rb', 'qb'])\n",
        "        df.loc[rb_qb_mask, 'ypg'] = df.loc[rb_qb_mask, 'senior_yds'] / df.loc[rb_qb_mask, 'games']\n",
        "    if 'senior_td' in df.columns:\n",
        "        wr_mask = df['position'].str.lower() == 'wr'\n",
        "        df.loc[wr_mask, 'tds_game'] = df.loc[wr_mask, 'senior_td'] / df.loc[wr_mask, 'games']\n",
        "        rb_qb_mask = df['position'].str.lower().isin(['rb', 'qb'])\n",
        "        df.loc[rb_qb_mask, 'td_game'] = df.loc[rb_qb_mask, 'senior_td'] / df.loc[rb_qb_mask, 'games']\n",
        "\n",
        "    # Trajectory calculation\n",
        "    if 'senior_ypg' in df.columns and 'junior_ypg' in df.columns:\n",
        "        df['trajectory'] = np.maximum(df['senior_ypg'] - df['junior_ypg'], 0)\n",
        "    else:\n",
        "        df['trajectory'] = 0.0\n",
        "\n",
        "    # Core engineered features (ensure numeric types)\n",
        "    df['bmi'] = ((df['weight_lbs'] / (df['height_inches'] ** 2)) * 703).astype(float)\n",
        "    df['eff_ratio'] = (df.get('senior_tds', 0) / (df.get('senior_ypg', 1) + 1e-6)).astype(float)\n",
        "    df['ath_power'] = (df.get('vertical_jump', 0) * df.get('broad_jump', 0)).astype(float)\n",
        "    df['is_strong_state'] = df['state'].str.upper().isin(['TX', 'FL', 'CA', 'GA']).astype(int)\n",
        "\n",
        "    # ENHANCED INTERACTION FEATURES (ensure numeric types)\n",
        "    \n",
        "    # BMI  YPG (power efficiency)\n",
        "    primary_ypg = df.get('senior_ypg', df.get('ypg', df.get('rec_ypg', 0)))\n",
        "    df['bmi_ypg'] = (df['bmi'] * primary_ypg).astype(float)\n",
        "    \n",
        "    # Height  Trajectory (growth potential with size)\n",
        "    df['height_traj'] = (df['height_inches'] * df['trajectory']).astype(float)\n",
        "    \n",
        "    # State efficiency (talent hotbed  efficiency)\n",
        "    df['state_eff'] = (df['state_talent_score'] * df['eff_ratio']).astype(float)\n",
        "    \n",
        "    # Speed-power ratio (athleticism efficiency)\n",
        "    df['speed_power_ratio'] = (df['ath_power'] / (df['forty_yard_dash'] + 1e-6)).astype(float)\n",
        "    \n",
        "    # Position-specific interaction features\n",
        "    if position.lower() == 'qb':\n",
        "        # Completion percentage  YPG (accuracy under volume)\n",
        "        df['comp_ypg'] = (df.get('senior_comp_pct', 60) * primary_ypg / 100).astype(float)\n",
        "        # Height  Completion % (pocket presence)\n",
        "        df['height_comp'] = (df['height_inches'] * df.get('senior_comp_pct', 60)).astype(float)\n",
        "    elif position.lower() == 'rb':\n",
        "        # YPC  Speed (breakaway ability)\n",
        "        df['ypc_speed'] = (df.get('senior_ypc', 0) * (5.0 - df.get('forty_yard_dash', 4.8))).astype(float)\n",
        "        # Weight  YPC (power running ability)\n",
        "        df['weight_ypc'] = (df['weight_lbs'] * df.get('senior_ypc', 0)).astype(float)\n",
        "    elif position.lower() == 'wr':\n",
        "        # Catch radius (height  vertical)\n",
        "        df['catch_radius'] = (df['height_inches'] * df.get('vertical_jump', 0)).astype(float)\n",
        "        # Speed  YAC (big play ability)\n",
        "        df['speed_yac'] = ((5.0 - df.get('forty_yard_dash', 4.8)) * df.get('senior_avg', 0)).astype(float)\n",
        "\n",
        "    # Combine confidence scores (0-1 based on real vs imputed data)\n",
        "    combine_cols = ['forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump']\n",
        "    imputed_cols = [f'{col}_imputed' for col in combine_cols if f'{col}_imputed' in df.columns]\n",
        "    \n",
        "    if imputed_cols:\n",
        "        df['combine_confidence'] = (1.0 - (df[imputed_cols].sum(axis=1) / len(imputed_cols))).astype(float)\n",
        "    else:\n",
        "        df['combine_confidence'] = 1.0\n",
        "\n",
        "    # Trajectory z-score by position\n",
        "    df['trajectory_z'] = 0.0\n",
        "    for pos in df['position'].unique():\n",
        "        mask = df['position'] == pos\n",
        "        if mask.sum() > 1:\n",
        "            mean_traj = df.loc[mask, 'trajectory'].mean()\n",
        "            std_traj = df.loc[mask, 'trajectory'].std()\n",
        "            if std_traj > 0:\n",
        "                df.loc[mask, 'trajectory_z'] = ((df.loc[mask, 'trajectory'] - mean_traj) / std_traj).astype(float)\n",
        "\n",
        "    # Create position dummies (ensure int type)\n",
        "    position_dummies = pd.get_dummies(df['position'].str.lower(), prefix='pos', dtype=int)\n",
        "    for pos in ['qb', 'rb', 'wr']:\n",
        "        if f'pos_{pos}' not in position_dummies.columns:\n",
        "            position_dummies[f'pos_{pos}'] = 0\n",
        "    df = pd.concat([df, position_dummies], axis=1)\n",
        "\n",
        "    # CRITICAL: Remove duplicate columns after all feature engineering\n",
        "    print(f\"    Before duplicate removal: {df.shape}\")\n",
        "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
        "    print(f\"    After duplicate removal: {df.shape}\")\n",
        "    \n",
        "    # Ensure all numeric columns have proper dtypes for XGBoost\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "        # Convert boolean columns to int\n",
        "        if df[col].dtype == 'bool':\n",
        "            df[col] = df[col].astype(int)\n",
        "\n",
        "    print(f\"Enhanced feature engineering completed for {position.upper()}\")\n",
        "    print(f\"  - Applied intelligent combine imputation\")\n",
        "    print(f\"  - Created state embeddings and talent scores\")\n",
        "    print(f\"  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\")\n",
        "    print(f\"  - Added position-specific features\")\n",
        "    print(f\"  - Calculated combine confidence scores\")\n",
        "    print(f\"  - Applied duplicate column removal\")\n",
        "    print(f\"  - Ensured all columns are XGBoost-compatible (numeric types only)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def preprocess_with_winsorization(df, position):\n",
        "    \"\"\"Compatibility wrapper for enhanced feature engineering\"\"\"\n",
        "    return enhanced_feature_engineering(df, position)\n",
        "\n",
        "def winsorize_and_scale(train_df, test_df, numeric_features):\n",
        "    \"\"\"Legacy wrapper for advanced winsorization and scaling\"\"\"\n",
        "    return advanced_winsorize_and_scale(train_df, test_df, numeric_features)\n",
        "\n",
        "def advanced_winsorize_and_scale(train_df, test_df, numeric_features):\n",
        "    \"\"\"Advanced winsorization and scaling with percentile features\"\"\"\n",
        "    train_processed = train_df.copy()\n",
        "    test_processed = test_df.copy()\n",
        "    \n",
        "    winsorization_log = []\n",
        "    \n",
        "    for feature in numeric_features:\n",
        "        if feature in train_df.columns and train_df[feature].dtype in ['int64', 'float64']:\n",
        "            # Winsorize on training data (1st-99th percentile)\n",
        "            feature_values = train_df[feature].dropna()\n",
        "            \n",
        "            if len(feature_values) > 0:\n",
        "                p1, p99 = np.percentile(feature_values, [1, 99])\n",
        "                \n",
        "                # Apply winsorization to both train and test\n",
        "                train_processed[feature] = np.clip(train_df[feature], p1, p99)\n",
        "                test_processed[feature] = np.clip(test_df[feature], p1, p99)\n",
        "                \n",
        "                # Percentile scaling based on training data\n",
        "                train_values = train_processed[feature].dropna()\n",
        "                if len(train_values) > 0:\n",
        "                    # Create percentile features\n",
        "                    train_processed[f'{feature}_pctile'] = train_processed[feature].apply(\n",
        "                        lambda x: np.percentile(train_values, 100 * (train_values <= x).mean()) if pd.notnull(x) else 50\n",
        "                    ).astype(float)\n",
        "                    test_processed[f'{feature}_pctile'] = test_processed[feature].apply(\n",
        "                        lambda x: np.percentile(train_values, 100 * (train_values <= x).mean()) if pd.notnull(x) else 50\n",
        "                    ).astype(float)\n",
        "                    \n",
        "                    winsorization_log.append(f\"{feature}: [{p1:.2f}, {p99:.2f}]\")\n",
        "    \n",
        "    print(f\"Advanced winsorization applied to {len(winsorization_log)} features\")\n",
        "    for log_entry in winsorization_log[:5]:  # Show first 5\n",
        "        print(f\"  {log_entry}\")\n",
        "    if len(winsorization_log) > 5:\n",
        "        print(f\"  ... and {len(winsorization_log) - 5} more features\")\n",
        "    \n",
        "    # DUPLICATE PREVENTION: Remove any duplicate columns created during winsorization\n",
        "    print(\"Removing duplicates after winsorization...\")\n",
        "    train_processed = train_processed.loc[:, ~train_processed.columns.duplicated(keep='first')]\n",
        "    test_processed = test_processed.loc[:, ~test_processed.columns.duplicated(keep='first')]\n",
        "    \n",
        "    return train_processed, test_processed\n",
        "\n",
        "# Legacy support for older function names\n",
        "def load_base_csv(position):\n",
        "    \"\"\"Legacy wrapper for enhanced data loading\"\"\"\n",
        "    return load_base_csv_enhanced(position)\n",
        "\n",
        "def enrich_data(df, position, year=2025):\n",
        "    \"\"\"Legacy wrapper for enhanced data enrichment\"\"\"\n",
        "    return enrich_data_enhanced(df, position, year)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 76,
              "statement_ids": [
                76
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.7473566Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.1514279Z",
              "execution_finish_time": "2025-08-04T00:24:22.3772302Z",
              "parent_msg_id": "d17df285-08f7-4cbd-8c72-0160da623764"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 76, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 356,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Tiers & Tier Base Assignments (robust to lower-case columns)\n",
        "tiers_qb = {\n",
        "    'Power 5': {'base': 90, 'ypg_min': 250, 'height_min': 74, 'height_max': 78, 'weight_min': 200, 'weight_max': 240,\n",
        "                '40_min': 4.6, '40_max': 4.9, 'vertical_min': 30, 'vertical_max': 34, 'broad_min': 108, 'shuttle_max': 4.5},\n",
        "    'FCS': {'base': 70, 'ypg_min': 200, 'height_min': 72, 'height_max': 76, 'weight_min': 190, 'weight_max': 220,\n",
        "            '40_min': 4.7, '40_max': 5.0, 'vertical_min': 28, 'vertical_max': 32, 'broad_min': 102, 'shuttle_max': 4.6},\n",
        "    'D2': {'base': 50, 'ypg_min': 150, 'height_min': 71, 'height_max': 74, 'weight_min': 180, 'weight_max': 210,\n",
        "           '40_min': 4.8, '40_max': 5.1, 'vertical_min': 26, 'vertical_max': 30, 'broad_min': 96, 'shuttle_max': 4.7},\n",
        "    'D3/NAIA': {'base': 30, 'ypg_min': 0, 'height_min': 70, 'height_max': 999, 'weight_min': 170, 'weight_max': 999,\n",
        "                '40_min': 4.9, '40_max': 999, 'vertical_min': 24, 'vertical_max': 999, 'broad_min': 90, 'shuttle_max': 999}\n",
        "}\n",
        "tiers_rb = {\n",
        "    'Power 5': {'base': 90, 'ypg_min': 150, 'height_min': 69, 'height_max': 74, 'weight_min': 190, 'weight_max': 230,\n",
        "                '40_min': 4.2, '40_max': 4.4, 'vertical_min': 34, 'vertical_max': 36, 'broad_min': 120, 'shuttle_max': 4.2},\n",
        "    'FCS': {'base': 70, 'ypg_min': 120, 'height_min': 68, 'height_max': 73, 'weight_min': 180, 'weight_max': 220,\n",
        "            '40_min': 4.3, '40_max': 4.5, 'vertical_min': 32, 'vertical_max': 34, 'broad_min': 110, 'shuttle_max': 4.3},\n",
        "    'D2': {'base': 50, 'ypg_min': 90, 'height_min': 67, 'height_max': 72, 'weight_min': 170, 'weight_max': 210,\n",
        "           '40_min': 4.4, '40_max': 4.6, 'vertical_min': 31, 'vertical_max': 33, 'broad_min': 100, 'shuttle_max': 4.4},\n",
        "    'D3/NAIA': {'base': 30, 'ypg_min': 0, 'height_min': 66, 'height_max': 999, 'weight_min': 160, 'weight_max': 999,\n",
        "                '40_min': 4.5, '40_max': 4.7, 'vertical_min': 30, 'vertical_max': 32, 'broad_min': 90, 'shuttle_max': 4.5}\n",
        "}\n",
        "tiers_wr = {\n",
        "    'Power 5': {'base': 90, 'rec_ypg_min': 100, 'height_min': 71, 'height_max': 75, 'weight_min': 180, 'weight_max': 210,\n",
        "                '40_min': 4.4, '40_max': 4.6, 'vertical_min': 34, 'vertical_max': 36, 'broad_min': 120, 'shuttle_max': 4.3},\n",
        "    'FCS': {'base': 70, 'rec_ypg_min': 80, 'height_min': 70, 'height_max': 74, 'weight_min': 170, 'weight_max': 200,\n",
        "            '40_min': 4.5, '40_max': 4.7, 'vertical_min': 32, 'vertical_max': 35, 'broad_min': 110, 'shuttle_max': 4.4},\n",
        "    'D2': {'base': 50, 'rec_ypg_min': 60, 'height_min': 69, 'height_max': 73, 'weight_min': 165, 'weight_max': 195,\n",
        "           '40_min': 4.6, '40_max': 4.8, 'vertical_min': 30, 'vertical_max': 33, 'broad_min': 100, 'shuttle_max': 4.5},\n",
        "    'D3/NAIA': {'base': 30, 'rec_ypg_min': 0, 'height_min': 68, 'height_max': 999, 'weight_min': 160, 'weight_max': 999,\n",
        "                '40_min': 4.7, '40_max': 5.0, 'vertical_min': 28, 'vertical_max': 31, 'broad_min': 90, 'shuttle_max': 4.6}\n",
        "}\n",
        "\n",
        "tiers = {'qb': tiers_qb, 'rb': tiers_rb, 'wr': tiers_wr}\n",
        "\n",
        "def safe_get(row, key, default):\n",
        "    \"\"\"Safely get value from row, handling None values.\"\"\"\n",
        "    value = row.get(key, default)\n",
        "    return default if value is None else value\n",
        "\n",
        "def assign_tier_base(row, position):\n",
        "    tiers_pos = tiers.get(position, tiers['qb'])\n",
        "    for name, rules in sorted(tiers_pos.items(), key=lambda x: x[1]['base'], reverse=True):\n",
        "        checks = []\n",
        "        if position == 'wr':\n",
        "            checks.append(safe_get(row, 'rec_ypg', 0) >= rules.get('rec_ypg_min', 0))\n",
        "        elif position == 'qb':\n",
        "            checks.append(safe_get(row, 'senior_ypg', 0) >= rules['ypg_min'])\n",
        "        elif position == 'rb':\n",
        "            checks.append(safe_get(row, 'ypg', 0) >= rules['ypg_min'])\n",
        "        checks += [\n",
        "            rules['height_min'] <= safe_get(row, 'height_inches', 0) <= rules['height_max'],\n",
        "            rules['weight_min'] <= safe_get(row, 'weight_lbs', 0) <= rules['weight_max'],\n",
        "            rules['40_min'] <= safe_get(row, 'forty_yard_dash', 5.0) <= rules['40_max'],\n",
        "            (rules['vertical_min'] - 1) <= safe_get(row, 'vertical_jump', 0) <= (rules['vertical_max'] + 1),\n",
        "            safe_get(row, 'shuttle', 5.0) <= rules['shuttle_max'],\n",
        "            safe_get(row, 'broad_jump', 0) >= rules['broad_min']\n",
        "        ]\n",
        "        if sum(checks) >= len(checks) * 0.6:\n",
        "            return rules['base'], name\n",
        "    return tiers_pos['D3/NAIA']['base'], 'D3/NAIA'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 77,
              "statement_ids": [
                77
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.7861784Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.391028Z",
              "execution_finish_time": "2025-08-04T00:24:22.6323973Z",
              "parent_msg_id": "8595f862-2717-4b02-98b4-974e71bf1d2d"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 77, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 357,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Meta-Score: Performance, Versatility, Athleticism, Bonus, Rule Score\n",
        "\n",
        "def safe_percentileofscore(series, value):\n",
        "    \"\"\"Safely compute percentile score, handling missing columns or empty series.\"\"\"\n",
        "    if series is None or len(series.dropna()) == 0:\n",
        "        return 0\n",
        "    return percentileofscore(series.dropna(), value if value is not None else 0)\n",
        "\n",
        "def safe_get(row, key, default):\n",
        "    \"\"\"Safely get value from row, handling None values.\"\"\"\n",
        "    value = row.get(key, default)\n",
        "    return default if value is None else value\n",
        "\n",
        "def compute_performance(df, row, position):\n",
        "    if position == 'qb':\n",
        "        ypg_pct = safe_percentileofscore(df.get('senior_ypg'), safe_get(row, 'senior_ypg', 0))\n",
        "        td_pct = safe_percentileofscore(df.get('senior_tds'), safe_get(row, 'senior_tds', 0))\n",
        "        comp_pct = safe_percentileofscore(df.get('senior_comp_pct'), safe_get(row, 'senior_comp_pct', 0))\n",
        "        traj_pct = safe_percentileofscore(df.get('trajectory'), safe_get(row, 'trajectory', 0))\n",
        "        return (0.4 * ypg_pct + 0.3 * td_pct + 0.2 * comp_pct + 0.1 * traj_pct + 0.1 * safe_get(row, 'trajectory_z', 0)) * 0.35\n",
        "    elif position == 'rb':\n",
        "        ypg_pct = safe_percentileofscore(df.get('ypg'), safe_get(row, 'ypg', 0))\n",
        "        td_pct = safe_percentileofscore(df.get('td_game'), safe_get(row, 'td_game', 0))\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_ypc'), safe_get(row, 'senior_ypc', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        return (0.4 * ypg_pct + 0.3 * td_pct + 0.2 * ypc_pct + 0.1 * rec_pct + 0.1 * safe_get(row, 'eff_ratio', 0)) * 0.35\n",
        "    elif position == 'wr':\n",
        "        ypg_pct = safe_percentileofscore(df.get('rec_ypg'), safe_get(row, 'rec_ypg', 0))\n",
        "        td_pct = safe_percentileofscore(df.get('tds_game'), safe_get(row, 'tds_game', 0))\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_avg'), safe_get(row, 'senior_avg', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        return (0.4 * ypg_pct + 0.3 * td_pct + 0.2 * ypc_pct + 0.1 * rec_pct + 0.1 * safe_get(row, 'eff_ratio', 0)) * 0.35\n",
        "    return 0\n",
        "\n",
        "def compute_versatility(df, row, position):\n",
        "    if position == 'qb':\n",
        "        comp_pct = safe_percentileofscore(df.get('senior_comp_pct'), safe_get(row, 'senior_comp_pct', 0))\n",
        "        speed_pct = 100 - safe_percentileofscore(df.get('forty_yard_dash'), safe_get(row, 'forty_yard_dash', 5.0))\n",
        "        return (0.5 * comp_pct + 0.5 * speed_pct) * 0.35\n",
        "    elif position == 'rb':\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_ypc'), safe_get(row, 'senior_ypc', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        ap_pct = safe_percentileofscore(df.get('all_purpose_game'), safe_get(row, 'all_purpose_game', 0))\n",
        "        return (0.4 * ypc_pct + 0.3 * rec_pct + 0.3 * ap_pct) * 0.4\n",
        "    elif position == 'wr':\n",
        "        ypc_pct = safe_percentileofscore(df.get('senior_avg'), safe_get(row, 'senior_avg', 0))\n",
        "        rec_pct = safe_percentileofscore(df.get('senior_rec'), safe_get(row, 'senior_rec', 0))\n",
        "        rush_pct = safe_percentileofscore(df.get('senior_rush_yds'), safe_get(row, 'senior_rush_yds', 0))\n",
        "        return (0.5 * ypc_pct + 0.3 * rec_pct + 0.2 * rush_pct) * 0.4\n",
        "    return 0\n",
        "\n",
        "def compute_athleticism(df, row, position):\n",
        "    f_pct = 100 - safe_percentileofscore(df.get('forty_yard_dash'), safe_get(row, 'forty_yard_dash', 5.0))\n",
        "    v_pct = safe_percentileofscore(df.get('vertical_jump'), safe_get(row, 'vertical_jump', 0))\n",
        "    s_pct = 100 - safe_percentileofscore(df.get('shuttle'), safe_get(row, 'shuttle', 5.0))\n",
        "    b_pct = safe_percentileofscore(df.get('broad_jump'), safe_get(row, 'broad_jump', 0))\n",
        "    return (0.3 * f_pct + 0.3 * v_pct + 0.2 * s_pct + 0.2 * b_pct) * 0.25\n",
        "\n",
        "def compute_bonus(row, position):\n",
        "    b = 0\n",
        "    th_40 = 4.7 if position == 'qb' else 4.5\n",
        "    th_sh = 4.4 if position == 'qb' else 4.3\n",
        "    if safe_get(row, 'forty_yard_dash', np.nan) < th_40: b += 10\n",
        "    if safe_get(row, 'shuttle', np.nan) < th_sh: b += 5\n",
        "    if safe_get(row, 'trajectory_z', 0) > 1: b += 5\n",
        "    if safe_get(row, 'is_strong_state', 0): b += 3\n",
        "    if safe_get(row, 'hoops_vert', 0) > 35: b += 4\n",
        "    pctile_cols = [c for c in row.index if '_pos_pctile' in c]\n",
        "    if sum(safe_get(row, c, 0) > 0.9 for c in pctile_cols) >= 3: b += 7\n",
        "    return b\n",
        "\n",
        "    \n",
        "def compute_rule_score(df, position):\n",
        "    # Drop all-NaN rows and those missing 'position'\n",
        "    df = df.dropna(how='all')\n",
        "    df = df[df['position'].notnull()]\n",
        "    results = []\n",
        "    tiers_used = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if not isinstance(row, pd.Series):\n",
        "            continue\n",
        "        pos = str(row.get('position', position)).lower()\n",
        "        base, tier_name = assign_tier_base(row, pos)\n",
        "        bonus = compute_bonus(row, pos)\n",
        "        perf = compute_performance(df, row, pos)\n",
        "        vers = compute_versatility(df, row, pos)\n",
        "        ath = compute_athleticism(df, row, pos)\n",
        "        multiplier = safe_get(row, 'multiplier', 1.0)\n",
        "        score = (base * 0.6 + (perf + vers + ath) * 0.4) * (1 + bonus / 100) * multiplier\n",
        "        score = np.clip(score, 0, 100)\n",
        "        results.append(score)\n",
        "        tiers_used.append(tier_name)\n",
        "    df = df.copy()\n",
        "    df['rule_score'] = results\n",
        "    df['rule_score_tier'] = tiers_used\n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 78,
              "statement_ids": [
                78
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.8453549Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.6435214Z",
              "execution_finish_time": "2025-08-04T00:24:22.8749747Z",
              "parent_msg_id": "9041d1e6-1e95-4ff1-b416-f7df2962c31b"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 78, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 358,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Enhanced Pipeline Usage Example with New Functions\n",
        "\n",
        "# 1. Load Data with Enhanced Functions\n",
        "df_qb = load_base_csv_enhanced('qb')\n",
        "df_rb = load_base_csv_enhanced('rb')\n",
        "df_wr = load_base_csv_enhanced('wr')\n",
        "\n",
        "# 2. Enrich data for balanced division representation\n",
        "df_qb = enrich_data_enhanced(df_qb, 'qb')\n",
        "df_rb = enrich_data_enhanced(df_rb, 'rb')\n",
        "df_wr = enrich_data_enhanced(df_wr, 'wr')\n",
        "\n",
        "# 3. Concatenate all positions for multi-position modeling\n",
        "combined_df = pd.concat([df_qb, df_rb, df_wr], ignore_index=True)\n",
        "\n",
        "# 4. Enhanced division normalization\n",
        "combined_df['division_normalized'] = combined_df['division'].str.strip().str.upper()\n",
        "division_map = {'POWER 5': 3, 'FCS': 2, 'D2': 1, 'D3': 0, 'NAIA': 0}\n",
        "combined_df['division_num'] = combined_df['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "# Fix any unmapped divisions\n",
        "unmapped_mask = combined_df['division_num'] == -1\n",
        "if unmapped_mask.any():\n",
        "    print(f\"Fixing {unmapped_mask.sum()} unmapped division values...\")\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('POWER|P5|FBS', na=False), 'division_num'] = 3\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('FCS', na=False), 'division_num'] = 2\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D2|DIV 2|DIVISION 2', na=False), 'division_num'] = 1\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D3|DIV 3|DIVISION 3|NAIA', na=False), 'division_num'] = 0\n",
        "\n",
        "print(\"Enhanced class distribution:\")\n",
        "print(combined_df['division_num'].value_counts().sort_index())\n",
        "\n",
        "# 5. Apply enhanced feature engineering\n",
        "combined_df = enhanced_feature_engineering(combined_df, 'multi')\n",
        "\n",
        "# 6. Compute rule score\n",
        "combined_df = compute_rule_score(combined_df, 'multi')\n",
        "\n",
        "# 7. Output: ready for accuracy evaluation and model training!\n",
        "print(f\"\\nDataset ready with {len(combined_df)} total samples and enhanced features\")\n",
        "combined_df[['name', 'position', 'division_normalized', 'division_num', 'rule_score', 'combine_confidence']].head()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 79,
              "statement_ids": [
                79
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.8884922Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:22.8850282Z",
              "execution_finish_time": "2025-08-04T00:24:37.614558Z",
              "parent_msg_id": "5f7c71da-47a7-4c0b-a5ff-1a80752c2202"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 79, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 220 rows for QB\nUnique divisions for QB: ['Power 5' 'FCS' 'D3' 'D2' 'NAIA']\nLoaded 194 rows for RB\nUnique divisions for RB: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nLoaded 158 rows for WR\nUnique divisions for WR: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for QB\nAdded 10 enhanced synthetic samples for RB\nAdded 10 enhanced synthetic samples for WR\nEnhanced class distribution:\ndivision_num\n0    150\n1    105\n2    107\n3    240\nName: count, dtype: int64\nCombine imputation for MULTI:\n  Imputing 525 missing forty_yard_dash values\n    POWER 5: 201 values from N(4.75, 0.08)\n    FCS: 85 values from N(4.85, 0.07)\n    D3: 61 values from N(5.10, 0.10)\n    D2: 99 values from N(4.95, 0.07)\n    NAIA: 79 values from N(5.00, 0.10)\n  Imputing 525 missing vertical_jump values\n    POWER 5: 201 values from N(32.00, 1.00)\n    FCS: 85 values from N(30.00, 1.00)\n    D3: 61 values from N(26.00, 1.00)\n    D2: 99 values from N(28.00, 1.00)\n    NAIA: 79 values from N(27.00, 1.00)\n  Imputing 538 missing shuttle values\n    POWER 5: 201 values from N(4.45, 0.07)\n    FCS: 98 values from N(4.55, 0.07)\n    D3: 61 values from N(4.75, 0.08)\n    D2: 99 values from N(4.65, 0.07)\n    NAIA: 79 values from N(4.65, 0.07)\n  Imputing 558 missing broad_jump values\n    POWER 5: 221 values from N(113.00, 2.50)\n    FCS: 98 values from N(107.00, 2.50)\n    D3: 61 values from N(95.00, 2.50)\n    D2: 99 values from N(101.00, 2.50)\n    NAIA: 79 values from N(97.00, 2.50)\n    Before duplicate removal: (602, 96)\n    After duplicate removal: (602, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\n\nDataset ready with 572 total samples and enhanced features\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "  name position division_normalized  division_num  rule_score  \\\n0  NaN       qb             POWER 5             3   70.016632   \n1  NaN       qb             POWER 5             3   74.422153   \n2  NaN       qb             POWER 5             3   60.169521   \n3  NaN       qb             POWER 5             3   55.080066   \n4  NaN       qb             POWER 5             3   75.674207   \n\n   combine_confidence  \n0                 0.0  \n1                 0.0  \n2                 0.0  \n3                 0.0  \n4                 0.0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>position</th>\n      <th>division_normalized</th>\n      <th>division_num</th>\n      <th>rule_score</th>\n      <th>combine_confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>70.016632</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>74.422153</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>60.169521</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>55.080066</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>qb</td>\n      <td>POWER 5</td>\n      <td>3</td>\n      <td>75.674207</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 359,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "def train_and_evaluate(train_df, features):\n",
        "    X = train_df[features].fillna(0)\n",
        "    y = train_df['Division_Num']\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        eval_metric='mlogloss',\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X, y)\n",
        "    return model\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 80,
              "statement_ids": [
                80
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.9796202Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:37.6253623Z",
              "execution_finish_time": "2025-08-04T00:24:37.846605Z",
              "parent_msg_id": "7860c6ec-7298-4ae7-9375-1f975ed6f4cc"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 80, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 360,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPREHENSIVE ACCURACY BOOST - FIXED DUPLICATE HANDLING\n",
        "# Target: 80%+ exact accuracy with proper duplicate prevention\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE ACCURACY BOOST WITH DUPLICATE PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Load and combine data using enhanced functions\n",
        "print(\"\\nSTEP 1: ENHANCED DATA LOADING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "positions = ['qb', 'rb', 'wr']\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for pos in positions:\n",
        "    df = load_base_csv_enhanced(pos)\n",
        "    df = enrich_data_enhanced(df, pos)\n",
        "    df['position'] = pos.lower()\n",
        "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "    # Remove duplicates after each concatenation\n",
        "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]\n",
        "\n",
        "# Division normalization\n",
        "combined_df['division_normalized'] = combined_df['division'].str.strip().str.upper()\n",
        "division_map = {'POWER 5': 3, 'FCS': 2, 'D2': 1, 'D3': 0, 'NAIA': 0}\n",
        "combined_df['division_num'] = combined_df['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "print(f\"Total samples: {len(combined_df)}\")\n",
        "print(f\"Class distribution: {dict(combined_df['division_num'].value_counts().sort_index())}\")\n",
        "\n",
        "# STEP 2: Train/Test Split\n",
        "print(\"\\nSTEP 2: TRAIN/TEST SPLIT\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "try:\n",
        "    train_df, test_df = train_test_split(\n",
        "        combined_df, \n",
        "        test_size=0.15, \n",
        "        stratify=combined_df['division_num'], \n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\" Stratified split: Train={len(train_df)}, Test={len(test_df)}\")\n",
        "    use_full_dataset = False\n",
        "except:\n",
        "    print(\" Stratified split failed - using full dataset\")\n",
        "    train_df = test_df = combined_df.copy()\n",
        "    use_full_dataset = True\n",
        "\n",
        "# STEP 3: Enhanced Feature Engineering with AGGRESSIVE duplicate prevention\n",
        "print(\"\\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH DUPLICATE PREVENTION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "def comprehensive_duplicate_removal(df, step_name=\"\"):\n",
        "    \"\"\"Aggressively remove all duplicate columns at every step\"\"\"\n",
        "    print(f\"  {step_name} - Before: {df.shape}\")\n",
        "    \n",
        "    # Method 1: Remove exact duplicate column names\n",
        "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
        "    \n",
        "    # Method 2: Check for any remaining duplicates and handle them\n",
        "    duplicate_cols = df.columns[df.columns.duplicated()].unique()\n",
        "    if len(duplicate_cols) > 0:\n",
        "        print(f\"    Found {len(duplicate_cols)} remaining duplicates: {list(duplicate_cols)}\")\n",
        "        for dup_col in duplicate_cols:\n",
        "            # Keep only the first occurrence\n",
        "            dup_indices = df.columns.get_loc(dup_col)\n",
        "            if hasattr(dup_indices, '__iter__'):\n",
        "                # Multiple occurrences - drop all but first\n",
        "                cols_to_drop = [df.columns[i] for i in dup_indices[1:]]\n",
        "                df = df.drop(columns=cols_to_drop)\n",
        "    \n",
        "    print(f\"  {step_name} - After: {df.shape}\")\n",
        "    \n",
        "    # Final verification\n",
        "    if df.columns.duplicated().any():\n",
        "        print(f\"    ERROR: Still have duplicates!\")\n",
        "        remaining_dups = df.columns[df.columns.duplicated()].unique()\n",
        "        print(f\"    Remaining: {list(remaining_dups)}\")\n",
        "        # Nuclear option: rename duplicates\n",
        "        df.columns = [f\"{col}_{i}\" if df.columns.tolist().count(col) > 1 and df.columns.tolist()[:i+1].count(col) > 1 \n",
        "                     else col for i, col in enumerate(df.columns)]\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply enhanced feature engineering\n",
        "train_df = enhanced_feature_engineering(train_df, 'multi')\n",
        "test_df = enhanced_feature_engineering(test_df, 'multi')\n",
        "\n",
        "# AGGRESSIVE duplicate removal after feature engineering\n",
        "train_df = comprehensive_duplicate_removal(train_df, \"Train after feature engineering\")\n",
        "test_df = comprehensive_duplicate_removal(test_df, \"Test after feature engineering\")\n",
        "\n",
        "# Ensure division_num is preserved\n",
        "for df_name, df_ in [('Train', train_df), ('Test', test_df)]:\n",
        "    if 'division_num' not in df_.columns:\n",
        "        df_['division_num'] = df_['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "# STEP 4: Feature Selection with duplicate checking\n",
        "print(\"\\nSTEP 4: FEATURE SELECTION WITH DUPLICATE CHECKING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Define comprehensive feature set \n",
        "base_features = [\n",
        "    'senior_ypg', 'senior_tds', 'senior_comp_pct', 'senior_ypc', 'senior_yds', \n",
        "    'senior_avg', 'senior_rec', 'senior_td', 'senior_rush_yds', 'rec_ypg', \n",
        "    'ypg', 'tds_game', 'td_game', 'trajectory', 'height_inches', 'weight_lbs', \n",
        "    'forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump', 'bmi', \n",
        "    'eff_ratio', 'ath_power', 'trajectory_z', 'is_strong_state', 'all_purpose_game',\n",
        "    'bmi_ypg', 'height_traj', 'state_eff', 'speed_power_ratio', 'state_talent_score',\n",
        "    'combine_confidence'\n",
        "]\n",
        "\n",
        "# Add engineered features\n",
        "position_features = [col for col in train_df.columns if col.startswith('pos_')]\n",
        "state_features = [col for col in train_df.columns if col.startswith('state_tier_')]\n",
        "interaction_features = [col for col in train_df.columns if any(x in col for x in ['comp_ypg', 'height_comp', 'ypc_speed', 'weight_ypc', 'catch_radius', 'speed_yac'])]\n",
        "\n",
        "# Combine and filter features\n",
        "all_features = base_features + position_features + state_features + interaction_features\n",
        "features = []\n",
        "for col in all_features:\n",
        "    if col in train_df.columns:\n",
        "        if train_df[col].dtype in ['int64', 'float64', 'int32', 'float32', 'bool']:\n",
        "            features.append(col)\n",
        "\n",
        "# Remove any potential duplicates from feature list itself\n",
        "features = list(dict.fromkeys(features))  # Preserves order while removing duplicates\n",
        "\n",
        "print(f\"Selected {len(features)} unique features\")\n",
        "\n",
        "# Compute rule scores\n",
        "if 'rule_score' not in train_df.columns:\n",
        "    print(\"Computing rule scores...\")\n",
        "    train_df = compute_rule_score(train_df, 'multi')\n",
        "    test_df = compute_rule_score(test_df, 'multi')\n",
        "    if 'rule_score' not in features:\n",
        "        features.append('rule_score')\n",
        "\n",
        "# STEP 5: Data preparation with FINAL duplicate check\n",
        "print(\"\\nSTEP 5: FINAL DATA PREPARATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create training matrices\n",
        "X_train = train_df[features].fillna(0)\n",
        "y_train = train_df['division_num'].values\n",
        "\n",
        "X_test = test_df[features].fillna(0)\n",
        "y_test = test_df['division_num'].values\n",
        "\n",
        "# CRITICAL: Apply XGBoost safeguard before any training\n",
        "print(\"\\n APPLYING XGBOOST SAFEGUARDS\")\n",
        "X_train, X_test = xgboost_safeguard(X_train, X_test, \"Final Data Preparation\")\n",
        "\n",
        "print(f\"Final training shapes: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
        "\n",
        "# STEP 6: Class balancing\n",
        "print(\"\\nSTEP 6: CLASS BALANCING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(f\"Original class distribution: {dict(pd.Series(y_train).value_counts().sort_index())}\")\n",
        "\n",
        "try:\n",
        "    adasyn = ADASYN(random_state=42, sampling_strategy='auto')\n",
        "    X_train_aug, y_train_aug = adasyn.fit_resample(X_train, y_train)\n",
        "    print(f\" ADASYN successful: {X_train.shape} -> {X_train_aug.shape}\")\n",
        "    \n",
        "    aug_counts = pd.Series(y_train_aug).value_counts().sort_index()\n",
        "    print(f\"Balanced distribution: {dict(aug_counts)}\")\n",
        "except Exception as e:\n",
        "    print(f\" ADASYN failed: {e}\")\n",
        "    X_train_aug, y_train_aug = X_train, y_train\n",
        "\n",
        "# STEP 7: Enhanced XGBoost Training\n",
        "print(\"\\nSTEP 7: ENHANCED XGBOOST TRAINING\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(\"Training enhanced XGBoost model...\")\n",
        "enhanced_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "enhanced_model.fit(X_train_aug, y_train_aug)\n",
        "print(\" XGBoost training completed successfully!\")\n",
        "\n",
        "# STEP 8: Evaluation\n",
        "print(\"\\nSTEP 8: COMPREHENSIVE EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_pred = enhanced_model.predict(X_test)\n",
        "y_pred_proba = enhanced_model.predict_proba(X_test)\n",
        "\n",
        "exact_acc = accuracy_score(y_test, y_pred)\n",
        "within_one_acc = np.mean(np.abs(y_test - y_pred) <= 1)\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f\"FINAL ENHANCED RESULTS:\")\n",
        "print(f\"Exact Accuracy: {exact_acc*100:.2f}%\")\n",
        "print(f\"Within-One-Division: {within_one_acc*100:.2f}%\")\n",
        "print(f\"F1 Score (Macro): {f1*100:.2f}%\")\n",
        "\n",
        "if use_full_dataset:\n",
        "    print(\" NOTE: Results on same data used for training (potential overfitting)\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "division_names = {0: 'D3/NAIA', 1: 'D2', 2: 'FCS', 3: 'Power 5'}\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Per-class accuracy\n",
        "print(f\"\\nPER-CLASS ACCURACY:\")\n",
        "for class_idx in range(len(np.unique(y_test))):\n",
        "    if class_idx in y_test:\n",
        "        class_mask = y_test == class_idx\n",
        "        if class_mask.any():\n",
        "            class_acc = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
        "            class_within_one = np.mean(np.abs(y_test[class_mask] - y_pred[class_mask]) <= 1)\n",
        "            class_name = division_names.get(class_idx, f'Class {class_idx}')\n",
        "            class_count = class_mask.sum()\n",
        "            print(f\"  {class_name}: {class_acc*100:.1f}% exact, {class_within_one*100:.1f}% within-one (n={class_count})\")\n",
        "\n",
        "# Per-position breakdown\n",
        "print(f\"\\nPER-POSITION BREAKDOWN:\")\n",
        "for pos in test_df['position'].unique():\n",
        "    pos_mask = test_df['position'] == pos\n",
        "    if pos_mask.any():\n",
        "        pos_indices = test_df[pos_mask].index\n",
        "        test_pos_mask = np.array([i for i, idx in enumerate(test_df.index) if idx in pos_indices])\n",
        "        \n",
        "        if len(test_pos_mask) > 0:\n",
        "            pos_y_true = y_test[test_pos_mask]\n",
        "            pos_y_pred = y_pred[test_pos_mask]\n",
        "            \n",
        "            pos_exact = accuracy_score(pos_y_true, pos_y_pred)\n",
        "            pos_within_one = np.mean(np.abs(pos_y_true - pos_y_pred) <= 1)\n",
        "            \n",
        "            print(f\"  {pos.upper()} (n={len(pos_y_true)}): {pos_exact*100:.1f}% exact, {pos_within_one*100:.1f}% within-one\")\n",
        "\n",
        "# Feature importance\n",
        "if hasattr(enhanced_model, 'feature_importances_'):\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': X_train_aug.columns,\n",
        "        'importance': enhanced_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTOP 10 FEATURE IMPORTANCE:\")\n",
        "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
        "        print(f\"  {i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
        "\n",
        "# FINAL SUMMARY\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE ACCURACY BOOST SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\" Enhanced data loading with intelligent combine imputation\")\n",
        "print(f\" Advanced feature engineering with {len(X_train_aug.columns)} features\")\n",
        "print(f\" AGGRESSIVE duplicate column prevention at every step\")\n",
        "print(f\" XGBoost safeguards applied before training\")\n",
        "print(f\" Enhanced class balancing with ADASYN\")\n",
        "print(f\" Optimized XGBoost hyperparameters\")\n",
        "print(f\" Comprehensive evaluation and analysis\")\n",
        "print(f\"\")\n",
        "print(f\"RESULTS:\")\n",
        "print(f\"  Exact Accuracy: {exact_acc*100:.2f}% (Target: 80%+)\")\n",
        "print(f\"  Within-One: {within_one_acc*100:.2f}%\")\n",
        "print(f\"  F1 Score: {f1*100:.2f}%\")\n",
        "\n",
        "improvement = exact_acc * 100 - 53  # Baseline was ~53%\n",
        "print(f\"\\nIMPROVEMENT: +{improvement:.1f}% from baseline\")\n",
        "\n",
        "if exact_acc >= 0.8:\n",
        "    print(f\" SUCCESS: Achieved target accuracy of 80%+!\")\n",
        "else:\n",
        "    print(f\" PROGRESS: Improved accuracy to {exact_acc*100:.1f}%\")\n",
        "    print(f\" NEXT STEPS: Consider ensemble methods or more data\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 81,
              "statement_ids": [
                81
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:28.986137Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:37.8576598Z",
              "execution_finish_time": "2025-08-04T00:24:56.8582397Z",
              "parent_msg_id": "1f4f9a3d-32ce-4453-8d76-8cb5f30d3fed"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 81, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\nCOMPREHENSIVE ACCURACY BOOST WITH DUPLICATE PREVENTION\n================================================================================\n\nSTEP 1: ENHANCED DATA LOADING\n------------------------------------------------------------\nLoaded 220 rows for QB\nUnique divisions for QB: ['Power 5' 'FCS' 'D3' 'D2' 'NAIA']\nAdded 10 enhanced synthetic samples for QB\nLoaded 194 rows for RB\nUnique divisions for RB: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for RB\nLoaded 158 rows for WR\nUnique divisions for WR: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for WR\nTotal samples: 602\nClass distribution: {0: 150, 1: 105, 2: 107, 3: 240}\n\nSTEP 2: TRAIN/TEST SPLIT\n------------------------------------------------------------\n Stratified split: Train=511, Test=91\n\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH DUPLICATE PREVENTION\n------------------------------------------------------------\nCombine imputation for MULTI:\n  Imputing 447 missing forty_yard_dash values\n    NAIA: 67 values from N(5.00, 0.10)\n    D2: 85 values from N(4.95, 0.07)\n    POWER 5: 170 values from N(4.75, 0.08)\n    FCS: 71 values from N(4.85, 0.07)\n    D3: 54 values from N(5.10, 0.10)\n  Imputing 447 missing vertical_jump values\n    NAIA: 67 values from N(27.00, 1.00)\n    D2: 85 values from N(28.00, 1.00)\n    POWER 5: 170 values from N(32.00, 1.00)\n    FCS: 71 values from N(30.00, 1.00)\n    D3: 54 values from N(26.00, 1.00)\n  Imputing 459 missing shuttle values\n    NAIA: 67 values from N(4.65, 0.07)\n    D2: 85 values from N(4.65, 0.07)\n    POWER 5: 170 values from N(4.45, 0.07)\n    FCS: 83 values from N(4.55, 0.07)\n    D3: 54 values from N(4.75, 0.08)\n  Imputing 478 missing broad_jump values\n    NAIA: 67 values from N(97.00, 2.50)\n    D2: 85 values from N(101.00, 2.50)\n    POWER 5: 189 values from N(113.00, 2.50)\n    FCS: 83 values from N(107.00, 2.50)\n    D3: 54 values from N(95.00, 2.50)\n    Before duplicate removal: (511, 96)\n    After duplicate removal: (511, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\nCombine imputation for MULTI:\n  Imputing 78 missing forty_yard_dash values\n    FCS: 14 values from N(4.85, 0.07)\n    D3: 7 values from N(5.10, 0.10)\n    POWER 5: 31 values from N(4.75, 0.08)\n    NAIA: 12 values from N(5.00, 0.10)\n    D2: 14 values from N(4.95, 0.07)\n  Imputing 78 missing vertical_jump values\n    FCS: 14 values from N(30.00, 1.00)\n    D3: 7 values from N(26.00, 1.00)\n    POWER 5: 31 values from N(32.00, 1.00)\n    NAIA: 12 values from N(27.00, 1.00)\n    D2: 14 values from N(28.00, 1.00)\n  Imputing 79 missing shuttle values\n    FCS: 15 values from N(4.55, 0.07)\n    D3: 7 values from N(4.75, 0.08)\n    POWER 5: 31 values from N(4.45, 0.07)\n    NAIA: 12 values from N(4.65, 0.07)\n    D2: 14 values from N(4.65, 0.07)\n  Imputing 80 missing broad_jump values\n    FCS: 15 values from N(107.00, 2.50)\n    D3: 7 values from N(95.00, 2.50)\n    POWER 5: 32 values from N(113.00, 2.50)\n    NAIA: 12 values from N(97.00, 2.50)\n    D2: 14 values from N(101.00, 2.50)\n    Before duplicate removal: (91, 96)\n    After duplicate removal: (91, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\n  Train after feature engineering - Before: (511, 96)\n  Train after feature engineering - After: (511, 96)\n  Test after feature engineering - Before: (91, 96)\n  Test after feature engineering - After: (91, 96)\n\nSTEP 4: FEATURE SELECTION WITH DUPLICATE CHECKING\n------------------------------------------------------------\nSelected 38 unique features\nComputing rule scores...\n\nSTEP 5: FINAL DATA PREPARATION\n------------------------------------------------------------\n\n APPLYING XGBOOST SAFEGUARDS\n\n  XGBoost Safeguard - Final Data Preparation\n--------------------------------------------------\n XGBoost Safeguard Complete\n   Training data: (511, 39)\n   Test data: (91, 39)\n   Data types: {dtype('float64'): 30, dtype('int64'): 9}\nFinal training shapes: X_train=(511, 39), X_test=(91, 39)\n\nSTEP 6: CLASS BALANCING\n------------------------------------------------------------\nOriginal class distribution: {0: 127, 1: 89, 2: 91, 3: 204}\n ADASYN successful: (511, 39) -> (786, 39)\nBalanced distribution: {0: 187, 1: 188, 2: 207, 3: 204}\n\nSTEP 7: ENHANCED XGBOOST TRAINING\n------------------------------------------------------------\nTraining enhanced XGBoost model...\n XGBoost training completed successfully!\n\nSTEP 8: COMPREHENSIVE EVALUATION\n================================================================================\nFINAL ENHANCED RESULTS:\nExact Accuracy: 87.91%\nWithin-One-Division: 98.90%\nF1 Score (Macro): 85.71%\n\nConfusion Matrix:\n[[19  3  0  1]\n [ 3 12  1  0]\n [ 0  1 14  1]\n [ 0  0  1 35]]\n\nPER-CLASS ACCURACY:\n  D3/NAIA: 82.6% exact, 95.7% within-one (n=23)\n  D2: 75.0% exact, 100.0% within-one (n=16)\n  FCS: 87.5% exact, 100.0% within-one (n=16)\n  Power 5: 97.2% exact, 100.0% within-one (n=36)\n\nPER-POSITION BREAKDOWN:\n  QB (n=36): 91.7% exact, 100.0% within-one\n  WR (n=22): 77.3% exact, 95.5% within-one\n  RB (n=33): 90.9% exact, 100.0% within-one\n\nTOP 10 FEATURE IMPORTANCE:\n   1. speed_power_ratio        : 0.2096\n   2. ath_power                : 0.1145\n   3. combine_confidence       : 0.0577\n   4. senior_ypc               : 0.0348\n   5. vertical_jump            : 0.0285\n   6. shuttle                  : 0.0257\n   7. pos_wr                   : 0.0239\n   8. senior_tds               : 0.0239\n   9. eff_ratio                : 0.0233\n  10. senior_rush_yds          : 0.0232\n\n================================================================================\nCOMPREHENSIVE ACCURACY BOOST SUMMARY\n================================================================================\n Enhanced data loading with intelligent combine imputation\n Advanced feature engineering with 39 features\n AGGRESSIVE duplicate column prevention at every step\n XGBoost safeguards applied before training\n Enhanced class balancing with ADASYN\n Optimized XGBoost hyperparameters\n Comprehensive evaluation and analysis\n\nRESULTS:\n  Exact Accuracy: 87.91% (Target: 80%+)\n  Within-One: 98.90%\n  F1 Score: 85.71%\n\nIMPROVEMENT: +34.9% from baseline\n SUCCESS: Achieved target accuracy of 80%+!\n================================================================================\n"
          ]
        }
      ],
      "execution_count": 361,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCURACY BOOST IMPLEMENTATION - FULL UPGRADE PLAN WITH COMPREHENSIVE DUPLICATE PREVENTION\n",
        "# Target: 80%+ exact accuracy for every class\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install additional dependencies\n",
        "try:\n",
        "    from autogluon.tabular import TabularPredictor\n",
        "    print(\" AutoGluon available\")\n",
        "except ImportError:\n",
        "    print(\" AutoGluon not available - will use XGBoost fallback\")\n",
        "    TabularPredictor = None\n",
        "\n",
        "try:\n",
        "    from ctgan import CTGAN\n",
        "    print(\" CTGAN available\")\n",
        "except ImportError:\n",
        "    print(\" CTGAN not available - will use ADASYN fallback\")\n",
        "    CTGAN = None\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    print(\" SHAP available\")\n",
        "except ImportError:\n",
        "    print(\" SHAP not available - will skip feature importance analysis\")\n",
        "    shap = None\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"IMPLEMENTING COMPREHENSIVE ACCURACY BOOST PLAN WITH DUPLICATE PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Enhanced Data Loading & Normalization\n",
        "print(\"\\nSTEP 1: ENHANCED DATA LOADING & NORMALIZATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "positions = ['qb', 'rb', 'wr']\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for pos in positions:\n",
        "    df = load_base_csv_enhanced(pos)  # Use enhanced function\n",
        "    df = enrich_data_enhanced(df, pos)  # Use enhanced function\n",
        "    df['position'] = pos.lower()\n",
        "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "    # CRITICAL: Remove duplicates after each concatenation\n",
        "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]\n",
        "    print(f\"  After {pos.upper()} concatenation: {combined_df.shape}\")\n",
        "\n",
        "# Apply robust division normalization\n",
        "combined_df['division_normalized'] = combined_df['division'].str.strip().str.upper()\n",
        "division_map = {'POWER 5': 3, 'FCS': 2, 'D2': 1, 'D3': 0, 'NAIA': 0}\n",
        "combined_df['division_num'] = combined_df['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "\n",
        "# Handle any unmapped values\n",
        "unmapped_mask = combined_df['division_num'] == -1\n",
        "if unmapped_mask.any():\n",
        "    print(f\"Fixing {unmapped_mask.sum()} unmapped division values...\")\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('POWER|P5|FBS', na=False), 'division_num'] = 3\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('FCS', na=False), 'division_num'] = 2\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D2|DIV 2|DIVISION 2', na=False), 'division_num'] = 1\n",
        "    combined_df.loc[combined_df['division_normalized'].str.contains('D3|DIV 3|DIVISION 3|NAIA', na=False), 'division_num'] = 0\n",
        "\n",
        "print(\"Class distribution after normalization:\")\n",
        "print(combined_df['division_num'].value_counts().sort_index())\n",
        "\n",
        "# STEP 2: Stratified Split with Enhanced Logic\n",
        "print(\"\\nSTEP 2: STRATIFIED SPLIT WITH ENHANCED LOGIC\")  \n",
        "print(\"-\" * 60)\n",
        "\n",
        "test_size = 0.15\n",
        "try:\n",
        "    train_df, test_df = train_test_split(\n",
        "        combined_df, \n",
        "        test_size=test_size, \n",
        "        stratify=combined_df['division_num'], \n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\" Successful stratified split: Train={len(train_df)}, Test={len(test_df)}\")\n",
        "    use_full_dataset = False\n",
        "except:\n",
        "    print(\" Stratified split failed - using full dataset\")\n",
        "    train_df = test_df = combined_df.copy()\n",
        "    use_full_dataset = True\n",
        "\n",
        "# STEP 3: Enhanced Feature Engineering with Duplicate Prevention\n",
        "print(\"\\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH COMPREHENSIVE DUPLICATE PREVENTION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Apply enhanced feature engineering to both datasets\n",
        "train_df = enhanced_feature_engineering(train_df, 'multi')\n",
        "test_df = enhanced_feature_engineering(test_df, 'multi')\n",
        "\n",
        "# CRITICAL: Comprehensive duplicate removal function\n",
        "def comprehensive_duplicate_removal(df, step_name=\"\"):\n",
        "    \"\"\"Aggressively remove ALL duplicate columns with multiple methods\"\"\"\n",
        "    print(f\"   {step_name} - Before duplicate removal: {df.shape}\")\n",
        "    \n",
        "    original_cols = list(df.columns)\n",
        "    \n",
        "    # Method 1: Basic duplicate removal\n",
        "    df_clean = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
        "    \n",
        "    # Method 2: Check for any remaining duplicates\n",
        "    remaining_dups = df_clean.columns[df_clean.columns.duplicated()].unique()\n",
        "    if len(remaining_dups) > 0:\n",
        "        print(f\"     Found {len(remaining_dups)} remaining duplicates: {list(remaining_dups)}\")\n",
        "        # More aggressive removal\n",
        "        seen_cols = set()\n",
        "        keep_cols = []\n",
        "        for col in df_clean.columns:\n",
        "            if col not in seen_cols:\n",
        "                keep_cols.append(col)\n",
        "                seen_cols.add(col)\n",
        "        df_clean = df_clean[keep_cols]\n",
        "    \n",
        "    # Method 3: Final verification and nuclear option if needed\n",
        "    if df_clean.columns.duplicated().any():\n",
        "        print(f\"     NUCLEAR OPTION: Renaming all duplicate columns\")\n",
        "        new_columns = []\n",
        "        seen_names = {}\n",
        "        for col in df_clean.columns:\n",
        "            if col not in seen_names:\n",
        "                new_columns.append(col)\n",
        "                seen_names[col] = 1\n",
        "            else:\n",
        "                seen_names[col] += 1\n",
        "                new_columns.append(f\"{col}_dup_{seen_names[col]}\")\n",
        "        df_clean.columns = new_columns\n",
        "    \n",
        "    removed_count = len(original_cols) - len(df_clean.columns)\n",
        "    print(f\"   {step_name} - After duplicate removal: {df_clean.shape} (removed {removed_count} duplicates)\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply comprehensive duplicate removal\n",
        "train_df = comprehensive_duplicate_removal(train_df, \"Training data\")\n",
        "test_df = comprehensive_duplicate_removal(test_df, \"Test data\")\n",
        "\n",
        "# Ensure division_num is preserved after preprocessing\n",
        "for df_name, df_ in [('Train', train_df), ('Test', test_df)]:\n",
        "    if 'division_num' not in df_.columns:\n",
        "        df_['division_num'] = df_['division_normalized'].map(division_map).fillna(-1).astype(int)\n",
        "        print(f\"   Restored division_num to {df_name} dataset\")\n",
        "\n",
        "# STEP 4: Feature Selection and Preparation\n",
        "print(\"\\nSTEP 4: FEATURE SELECTION AND PREPARATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Define comprehensive feature set with enhanced features\n",
        "base_features = [\n",
        "    'senior_ypg', 'senior_tds', 'senior_comp_pct', 'senior_ypc', 'senior_yds', \n",
        "    'senior_avg', 'senior_rec', 'senior_td', 'senior_rush_yds', 'rec_ypg', \n",
        "    'ypg', 'tds_game', 'td_game', 'trajectory', 'height_inches', 'weight_lbs', \n",
        "    'forty_yard_dash', 'vertical_jump', 'shuttle', 'broad_jump', 'bmi', \n",
        "    'eff_ratio', 'ath_power', 'trajectory_z', 'is_strong_state', 'all_purpose_game',\n",
        "    # Enhanced interaction features\n",
        "    'bmi_ypg', 'height_traj', 'state_eff', 'speed_power_ratio', 'state_talent_score',\n",
        "    'combine_confidence'\n",
        "]\n",
        "\n",
        "# Add position features and other engineered features\n",
        "position_features = [col for col in train_df.columns if col.startswith('pos_')]\n",
        "state_tier_features = [col for col in train_df.columns if col.startswith('state_tier_')]\n",
        "interaction_features = []\n",
        "\n",
        "# Add position-specific interaction features\n",
        "for col in train_df.columns:\n",
        "    if any(x in col for x in ['comp_ypg', 'height_comp', 'ypc_speed', 'weight_ypc', 'catch_radius', 'speed_yac']):\n",
        "        interaction_features.append(col)\n",
        "\n",
        "# Combine all feature types\n",
        "all_features = base_features + position_features + state_tier_features + interaction_features\n",
        "\n",
        "# Only use features that exist and are numeric - WITH DUPLICATE REMOVAL\n",
        "features = []\n",
        "seen_features = set()\n",
        "for col in all_features:\n",
        "    if col in train_df.columns and col not in seen_features:\n",
        "        # Only include numeric columns to avoid XGBoost issues\n",
        "        if train_df[col].dtype in ['int64', 'float64', 'int32', 'float32', 'bool']:\n",
        "            features.append(col)\n",
        "            seen_features.add(col)\n",
        "        else:\n",
        "            print(f\"  Skipping non-numeric feature: {col} (dtype: {train_df[col].dtype})\")\n",
        "\n",
        "print(f\"Selected {len(features)} unique features for training\")\n",
        "\n",
        "# Compute rule scores if not already done\n",
        "if 'rule_score' not in train_df.columns:\n",
        "    print(\"Computing rule scores...\")\n",
        "    train_df = compute_rule_score(train_df, 'multi')\n",
        "    test_df = compute_rule_score(test_df, 'multi')\n",
        "    if 'rule_score' not in features and 'rule_score' not in seen_features:\n",
        "        features.append('rule_score')\n",
        "        seen_features.add('rule_score')\n",
        "\n",
        "# Apply advanced winsorization and scaling\n",
        "print(\"Applying advanced winsorization and scaling...\")\n",
        "numeric_features = [f for f in features if f in train_df.columns and train_df[f].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
        "train_df, test_df = advanced_winsorize_and_scale(train_df, test_df, numeric_features)\n",
        "\n",
        "# STEP 5: Final Data Preparation with Comprehensive Validation\n",
        "print(\"\\nSTEP 5: FINAL DATA PREPARATION WITH COMPREHENSIVE VALIDATION\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Prepare training data with comprehensive validation\n",
        "X_train = train_df[features].fillna(0)\n",
        "y_train = train_df['division_num'].values\n",
        "\n",
        "X_test = test_df[features].fillna(0)\n",
        "y_test = test_df['division_num'].values\n",
        "\n",
        "print(f\"Initial training data shape: {X_train.shape}\")\n",
        "print(f\"Initial test data shape: {X_test.shape}\")\n",
        "\n",
        "# CRITICAL: Apply XGBoost safeguard BEFORE any model operations\n",
        "print(\"\\n APPLYING COMPREHENSIVE XGBOOST SAFEGUARDS\")\n",
        "X_train, X_test = xgboost_safeguard(X_train, X_test, \"Pre-Processing Safety Check\")\n",
        "\n",
        "# Final validation: Check for any remaining issues\n",
        "print(\"Final data validation:\")\n",
        "print(f\"  X_train dtypes: {X_train.dtypes.value_counts().to_dict()}\")\n",
        "print(f\"  X_test dtypes: {X_test.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "# Ensure all features are numeric for XGBoost\n",
        "non_numeric_cols = []\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype not in ['int64', 'float64', 'int32', 'float32', 'bool']:\n",
        "        non_numeric_cols.append(col)\n",
        "\n",
        "if non_numeric_cols:\n",
        "    print(f\"  Converting non-numeric columns to numeric: {non_numeric_cols}\")\n",
        "    for col in non_numeric_cols:\n",
        "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)\n",
        "        X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n",
        "\n",
        "print(f\"\\nValidated feature list ({len(X_train.columns)} features):\")\n",
        "for i, feature in enumerate(X_train.columns[:10]):  # Show first 10\n",
        "    print(f\"  {i+1}. {feature}\")\n",
        "if len(X_train.columns) > 10:\n",
        "    print(f\"  ... and {len(X_train.columns) - 10} more features\")\n",
        "\n",
        "# STEP 6: Enhanced Class Balancing with Additional Safeguards\n",
        "print(\"\\nSTEP 6: ENHANCED CLASS BALANCING WITH SAFEGUARDS\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "print(f\"Original class distribution: {dict(class_counts)}\")\n",
        "\n",
        "# Apply ADASYN for class balancing with safeguards\n",
        "print(\"Applying ADASYN class balancing...\")\n",
        "try:\n",
        "    # CRITICAL: Additional safeguard right before ADASYN\n",
        "    print(\" Pre-ADASYN safety check...\")\n",
        "    if X_train.columns.duplicated().any():\n",
        "        print(\" Found duplicates before ADASYN - applying emergency fix\")\n",
        "        X_train = X_train.loc[:, ~X_train.columns.duplicated(keep='first')]\n",
        "        X_test = X_test.loc[:, ~X_test.columns.duplicated(keep='first')]\n",
        "    \n",
        "    adasyn = ADASYN(random_state=42, sampling_strategy='auto')\n",
        "    X_train_aug, y_train_aug = adasyn.fit_resample(X_train, y_train)\n",
        "    print(f\" ADASYN successful: {X_train.shape} -> {X_train_aug.shape}\")\n",
        "    \n",
        "    # CRITICAL: Post-ADASYN safety check\n",
        "    print(\" Post-ADASYN safety check...\")\n",
        "    if hasattr(X_train_aug, 'columns') and X_train_aug.columns.duplicated().any():\n",
        "        print(\" ADASYN introduced duplicates - fixing...\")\n",
        "        X_train_aug = pd.DataFrame(X_train_aug).loc[:, ~pd.DataFrame(X_train_aug).columns.duplicated(keep='first')]\n",
        "    \n",
        "    aug_class_counts = pd.Series(y_train_aug).value_counts().sort_index()\n",
        "    print(f\"Balanced class distribution: {dict(aug_class_counts)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" ADASYN failed: {e}\")\n",
        "    print(\"Using original data without balancing\")\n",
        "    X_train_aug, y_train_aug = X_train, y_train\n",
        "\n",
        "# STEP 7: Enhanced XGBoost Training with Final Safeguards\n",
        "print(\"\\nSTEP 7: ENHANCED XGBOOST TRAINING WITH FINAL SAFEGUARDS\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# FINAL NUCLEAR SAFETY CHECK before XGBoost\n",
        "print(\" FINAL NUCLEAR SAFETY CHECK BEFORE XGBOOST\")\n",
        "if hasattr(X_train_aug, 'columns'):\n",
        "    if X_train_aug.columns.duplicated().any():\n",
        "        print(\" APPLYING NUCLEAR FIX: Renaming all columns to ensure absolute uniqueness\")\n",
        "        n_cols = len(X_train_aug.columns)\n",
        "        unique_names = [f\"feature_{i:03d}\" for i in range(n_cols)]\n",
        "        X_train_aug.columns = unique_names\n",
        "        X_test.columns = unique_names[:len(X_test.columns)]\n",
        "    else:\n",
        "        print(\" No duplicate columns detected in final nuclear check\")\n",
        "else:\n",
        "    # Convert numpy array to DataFrame with guaranteed unique names\n",
        "    print(\" Converting numpy array to DataFrame with guaranteed unique column names\")\n",
        "    n_cols = X_train_aug.shape[1] if hasattr(X_train_aug, 'shape') else len(X_train_aug[0])\n",
        "    unique_names = [f\"feature_{i:03d}\" for i in range(n_cols)]\n",
        "    X_train_aug = pd.DataFrame(X_train_aug, columns=unique_names)\n",
        "    X_test = pd.DataFrame(X_test, columns=unique_names)\n",
        "\n",
        "print(\"Training enhanced XGBoost model...\")\n",
        "enhanced_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "enhanced_model.fit(X_train_aug, y_train_aug)\n",
        "\n",
        "# Comprehensive evaluation\n",
        "y_pred = enhanced_model.predict(X_test)\n",
        "y_pred_proba = enhanced_model.predict_proba(X_test)\n",
        "\n",
        "exact_acc = accuracy_score(y_test, y_pred)\n",
        "within_one_acc = np.mean(np.abs(y_test - y_pred) <= 1)\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\" Enhanced XGBoost training completed successfully without duplicate errors!\")\n",
        "\n",
        "# STEP 8: Meta-blending with Rule Score\n",
        "print(\"\\nSTEP 8: META-BLENDING WITH RULE SCORE\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create meta-features for blending\n",
        "def create_meta_features(rule_scores, model_preds, model_probas):\n",
        "    \"\"\"Create meta-features for blending\"\"\"\n",
        "    meta_features = pd.DataFrame({\n",
        "        'rule_score': rule_scores,\n",
        "        'model_pred': model_preds,\n",
        "        'model_confidence': np.max(model_probas, axis=1),\n",
        "        'model_uncertainty': 1 - np.max(model_probas, axis=1)\n",
        "    })\n",
        "    \n",
        "    # Add probability features for each class\n",
        "    for i in range(model_probas.shape[1]):\n",
        "        meta_features[f'prob_class_{i}'] = model_probas[:, i]\n",
        "    \n",
        "    return meta_features\n",
        "\n",
        "# Create meta-features for train and test\n",
        "train_rule_scores = train_df['rule_score'].fillna(50).values\n",
        "test_rule_scores = test_df['rule_score'].fillna(50).values\n",
        "\n",
        "# Get training predictions for meta-model\n",
        "y_train_pred = enhanced_model.predict(X_train)\n",
        "y_train_pred_proba = enhanced_model.predict_proba(X_train)\n",
        "\n",
        "meta_features_train = create_meta_features(train_rule_scores, y_train_pred, y_train_pred_proba)\n",
        "meta_features_test = create_meta_features(test_rule_scores, y_pred, y_pred_proba)\n",
        "\n",
        "# Train meta-model with safeguards\n",
        "print(\"Training meta-blending model...\")\n",
        "meta_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "meta_model.fit(meta_features_train, y_train)\n",
        "y_pred_final = meta_model.predict(meta_features_test)\n",
        "\n",
        "print(\" Meta-blending model trained successfully\")\n",
        "\n",
        "# STEP 9: Comprehensive Results\n",
        "print(\"\\nSTEP 9: COMPREHENSIVE RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final metrics\n",
        "final_exact_acc = accuracy_score(y_test, y_pred_final)\n",
        "final_within_one_acc = np.mean(np.abs(y_test - y_pred_final) <= 1)\n",
        "final_f1 = f1_score(y_test, y_pred_final, average='macro')\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "print(f\"FINAL ENHANCED RESULTS:\")\n",
        "print(f\"Exact Accuracy: {final_exact_acc*100:.2f}%\")\n",
        "print(f\"Within-One-Division: {final_within_one_acc*100:.2f}%\")\n",
        "print(f\"F1 Score (Macro): {final_f1*100:.2f}%\")\n",
        "\n",
        "if use_full_dataset:\n",
        "    print(\" NOTE: Results are on the same data used for training (potential overfitting)\")\n",
        "\n",
        "# Detailed confusion matrix\n",
        "division_names = {0: 'D3/NAIA', 1: 'D2', 2: 'FCS', 3: 'Power 5'}\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Per-class accuracy\n",
        "print(f\"\\nPER-CLASS ACCURACY:\")\n",
        "for class_idx in range(len(np.unique(y_test))):\n",
        "    if class_idx in y_test:\n",
        "        class_mask = y_test == class_idx\n",
        "        if class_mask.any():\n",
        "            class_acc = accuracy_score(y_test[class_mask], y_pred_final[class_mask])\n",
        "            class_within_one = np.mean(np.abs(y_test[class_mask] - y_pred_final[class_mask]) <= 1)\n",
        "            class_name = division_names.get(class_idx, f'Class {class_idx}')\n",
        "            class_count = class_mask.sum()\n",
        "            print(f\"  {class_name}: {class_acc*100:.1f}% exact, {class_within_one*100:.1f}% within-one (n={class_count})\")\n",
        "\n",
        "# Per-position breakdown\n",
        "print(f\"\\nPER-POSITION BREAKDOWN:\")\n",
        "for pos in test_df['position'].unique():\n",
        "    pos_mask = test_df['position'] == pos\n",
        "    if pos_mask.any():\n",
        "        pos_indices = test_df[pos_mask].index\n",
        "        # Map to test set indices\n",
        "        test_pos_mask = np.array([i for i, idx in enumerate(test_df.index) if idx in pos_indices])\n",
        "        \n",
        "        if len(test_pos_mask) > 0:\n",
        "            pos_y_true = y_test[test_pos_mask]\n",
        "            pos_y_pred = y_pred_final[test_pos_mask]\n",
        "            \n",
        "            pos_exact = accuracy_score(pos_y_true, pos_y_pred)\n",
        "            pos_within_one = np.mean(np.abs(pos_y_true - pos_y_pred) <= 1)\n",
        "            \n",
        "            print(f\"  {pos.upper()} (n={len(pos_y_true)}): {pos_exact*100:.1f}% exact, {pos_within_one*100:.1f}% within-one\")\n",
        "\n",
        "# FCS-specific analysis\n",
        "print(f\"\\nFCS-SPECIFIC ANALYSIS:\")\n",
        "fcs_mask = y_test == 2\n",
        "if fcs_mask.any():\n",
        "    fcs_count = fcs_mask.sum()\n",
        "    fcs_correct = (y_test[fcs_mask] == y_pred_final[fcs_mask]).sum()\n",
        "    fcs_accuracy = fcs_correct / fcs_count\n",
        "    \n",
        "    print(f\" FCS Class Analysis:\")\n",
        "    print(f\"  Total FCS samples in test: {fcs_count}\")\n",
        "    print(f\"  Correctly predicted: {fcs_correct}\")\n",
        "    print(f\"  FCS Accuracy: {fcs_accuracy*100:.1f}%\")\n",
        "    \n",
        "    # Show FCS predictions breakdown\n",
        "    fcs_predictions = y_pred_final[fcs_mask]\n",
        "    print(f\"  FCS prediction breakdown:\")\n",
        "    for pred_class in np.unique(fcs_predictions):\n",
        "        pred_count = (fcs_predictions == pred_class).sum()\n",
        "        pred_name = division_names.get(pred_class, f'Class {pred_class}')\n",
        "        print(f\"    Predicted as {pred_name}: {pred_count} samples\")\n",
        "else:\n",
        "    print(\" No FCS samples in test set\")\n",
        "\n",
        "# Feature importance\n",
        "if hasattr(enhanced_model, 'feature_importances_'):\n",
        "    feature_names = X_train_aug.columns if hasattr(X_train_aug, 'columns') else [f\"feature_{i}\" for i in range(len(enhanced_model.feature_importances_))]\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': enhanced_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTOP 10 FEATURE IMPORTANCE:\")\n",
        "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
        "        print(f\"  {i+1:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
        "\n",
        "# FINAL SUMMARY\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ACCURACY BOOST IMPLEMENTATION SUMMARY WITH DUPLICATE PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\" Enhanced data loading with intelligent combine imputation\")\n",
        "print(f\" Advanced feature engineering with comprehensive duplicate prevention\")\n",
        "print(f\" Multiple layers of duplicate column safeguards:\")\n",
        "print(f\"  - After each data concatenation\")\n",
        "print(f\"  - After feature engineering\")\n",
        "print(f\"  - Before and after ADASYN\")\n",
        "print(f\"  - Nuclear safety check before XGBoost\")\n",
        "print(f\" State embeddings and interaction features\")\n",
        "print(f\" Enhanced class balancing with ADASYN\")\n",
        "print(f\" Meta-blending with rule scores\")\n",
        "print(f\" Comprehensive evaluation and analysis\")\n",
        "print(f\"\")\n",
        "print(f\"FINAL RESULTS:\")\n",
        "print(f\"  Exact Accuracy: {final_exact_acc*100:.2f}% (Target: 80%+)\")\n",
        "print(f\"  Within-One: {final_within_one_acc*100:.2f}%\")\n",
        "print(f\"  F1 Score: {final_f1*100:.2f}%\")\n",
        "\n",
        "improvement = final_exact_acc * 100 - 53  # Baseline was ~53%\n",
        "print(f\"\\nPERFORMANCE IMPROVEMENT: +{improvement:.1f}% from baseline\")\n",
        "\n",
        "if final_exact_acc >= 0.8:\n",
        "    print(f\" SUCCESS: Achieved target accuracy of 80%+!\")\n",
        "else:\n",
        "    print(f\" PROGRESS: Improved accuracy to {final_exact_acc*100:.1f}%\")\n",
        "    print(f\" NEXT STEPS: Consider adding more data for rare classes or ensemble methods\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" DUPLICATE COLUMN ERROR PREVENTION: COMPREHENSIVE SAFEGUARDS APPLIED\")\n",
        "print(\"=\"*80)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "pocsparkpool",
              "statement_id": 82,
              "statement_ids": [
                82
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "51",
              "normalized_state": "finished",
              "queued_time": "2025-08-04T00:23:29.2012317Z",
              "session_start_time": null,
              "execution_start_time": "2025-08-04T00:24:56.8710604Z",
              "execution_finish_time": "2025-08-04T00:25:38.7630883Z",
              "parent_msg_id": "5f85f15a-12ee-4f5e-ad3b-5f0b1177d6d8"
            },
            "text/plain": "StatementMeta(pocsparkpool, 51, 82, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AutoGluon available\n CTGAN available\n SHAP available\n================================================================================\nIMPLEMENTING COMPREHENSIVE ACCURACY BOOST PLAN WITH DUPLICATE PREVENTION\n================================================================================\n\nSTEP 1: ENHANCED DATA LOADING & NORMALIZATION\n------------------------------------------------------------\nLoaded 220 rows for QB\nUnique divisions for QB: ['Power 5' 'FCS' 'D3' 'D2' 'NAIA']\nAdded 10 enhanced synthetic samples for QB\n  After QB concatenation: (230, 29)\nLoaded 194 rows for RB\nUnique divisions for RB: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for RB\n  After RB concatenation: (434, 52)\nLoaded 158 rows for WR\nUnique divisions for WR: ['Power 5' 'NAIA' 'FCS' 'D3' 'D2']\nAdded 10 enhanced synthetic samples for WR\n  After WR concatenation: (602, 64)\nClass distribution after normalization:\ndivision_num\n0    150\n1    105\n2    107\n3    240\nName: count, dtype: int64\n\nSTEP 2: STRATIFIED SPLIT WITH ENHANCED LOGIC\n------------------------------------------------------------\n Successful stratified split: Train=511, Test=91\n\nSTEP 3: ENHANCED FEATURE ENGINEERING WITH COMPREHENSIVE DUPLICATE PREVENTION\n------------------------------------------------------------\nCombine imputation for MULTI:\n  Imputing 447 missing forty_yard_dash values\n    NAIA: 67 values from N(5.00, 0.10)\n    D2: 85 values from N(4.95, 0.07)\n    POWER 5: 170 values from N(4.75, 0.08)\n    FCS: 71 values from N(4.85, 0.07)\n    D3: 54 values from N(5.10, 0.10)\n  Imputing 447 missing vertical_jump values\n    NAIA: 67 values from N(27.00, 1.00)\n    D2: 85 values from N(28.00, 1.00)\n    POWER 5: 170 values from N(32.00, 1.00)\n    FCS: 71 values from N(30.00, 1.00)\n    D3: 54 values from N(26.00, 1.00)\n  Imputing 459 missing shuttle values\n    NAIA: 67 values from N(4.65, 0.07)\n    D2: 85 values from N(4.65, 0.07)\n    POWER 5: 170 values from N(4.45, 0.07)\n    FCS: 83 values from N(4.55, 0.07)\n    D3: 54 values from N(4.75, 0.08)\n  Imputing 478 missing broad_jump values\n    NAIA: 67 values from N(97.00, 2.50)\n    D2: 85 values from N(101.00, 2.50)\n    POWER 5: 189 values from N(113.00, 2.50)\n    FCS: 83 values from N(107.00, 2.50)\n    D3: 54 values from N(95.00, 2.50)\n    Before duplicate removal: (511, 96)\n    After duplicate removal: (511, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\nCombine imputation for MULTI:\n  Imputing 78 missing forty_yard_dash values\n    FCS: 14 values from N(4.85, 0.07)\n    D3: 7 values from N(5.10, 0.10)\n    POWER 5: 31 values from N(4.75, 0.08)\n    NAIA: 12 values from N(5.00, 0.10)\n    D2: 14 values from N(4.95, 0.07)\n  Imputing 78 missing vertical_jump values\n    FCS: 14 values from N(30.00, 1.00)\n    D3: 7 values from N(26.00, 1.00)\n    POWER 5: 31 values from N(32.00, 1.00)\n    NAIA: 12 values from N(27.00, 1.00)\n    D2: 14 values from N(28.00, 1.00)\n  Imputing 79 missing shuttle values\n    FCS: 15 values from N(4.55, 0.07)\n    D3: 7 values from N(4.75, 0.08)\n    POWER 5: 31 values from N(4.45, 0.07)\n    NAIA: 12 values from N(4.65, 0.07)\n    D2: 14 values from N(4.65, 0.07)\n  Imputing 80 missing broad_jump values\n    FCS: 15 values from N(107.00, 2.50)\n    D3: 7 values from N(95.00, 2.50)\n    POWER 5: 32 values from N(113.00, 2.50)\n    NAIA: 12 values from N(97.00, 2.50)\n    D2: 14 values from N(101.00, 2.50)\n    Before duplicate removal: (91, 96)\n    After duplicate removal: (91, 96)\nEnhanced feature engineering completed for MULTI\n  - Applied intelligent combine imputation\n  - Created state embeddings and talent scores\n  - Generated interaction features: bmi_ypg, height_traj, state_eff, speed_power_ratio\n  - Added position-specific features\n  - Calculated combine confidence scores\n  - Applied duplicate column removal\n  - Ensured all columns are XGBoost-compatible (numeric types only)\n   Training data - Before duplicate removal: (511, 96)\n   Training data - After duplicate removal: (511, 96) (removed 0 duplicates)\n   Test data - Before duplicate removal: (91, 96)\n   Test data - After duplicate removal: (91, 96) (removed 0 duplicates)\n\nSTEP 4: FEATURE SELECTION AND PREPARATION\n------------------------------------------------------------\n  Skipping non-numeric feature: broad_jump (dtype: object)\nSelected 38 unique features for training\nComputing rule scores...\nApplying advanced winsorization and scaling...\nAdvanced winsorization applied to 38 features\n  senior_ypg: [4.68, 344.88]\n  senior_tds: [9.14, 23.72]\n  senior_comp_pct: [48.28, 63.86]\n  senior_ypc: [3.82, 5.76]\n  senior_yds: [11.00, 4445.50]\n  ... and 33 more features\nRemoving duplicates after winsorization...\n\nSTEP 5: FINAL DATA PREPARATION WITH COMPREHENSIVE VALIDATION\n------------------------------------------------------------\nInitial training data shape: (511, 39)\nInitial test data shape: (91, 39)\n\n APPLYING COMPREHENSIVE XGBOOST SAFEGUARDS\n\n  XGBoost Safeguard - Pre-Processing Safety Check\n--------------------------------------------------\n XGBoost Safeguard Complete\n   Training data: (511, 39)\n   Test data: (91, 39)\n   Data types: {dtype('float64'): 30, dtype('int64'): 9}\nFinal data validation:\n  X_train dtypes: {dtype('float64'): 30, dtype('int64'): 9}\n  X_test dtypes: {dtype('float64'): 30, dtype('int64'): 9}\n\nValidated feature list (39 features):\n  1. senior_ypg\n  2. senior_tds\n  3. senior_comp_pct\n  4. senior_ypc\n  5. senior_yds\n  6. senior_avg\n  7. senior_rec\n  8. senior_td\n  9. senior_rush_yds\n  10. rec_ypg\n  ... and 29 more features\n\nSTEP 6: ENHANCED CLASS BALANCING WITH SAFEGUARDS\n------------------------------------------------------------\nOriginal class distribution: {0: 127, 1: 89, 2: 91, 3: 204}\nApplying ADASYN class balancing...\n Pre-ADASYN safety check...\n ADASYN successful: (511, 39) -> (810, 39)\n Post-ADASYN safety check...\nBalanced class distribution: {0: 197, 1: 201, 2: 208, 3: 204}\n\nSTEP 7: ENHANCED XGBOOST TRAINING WITH FINAL SAFEGUARDS\n------------------------------------------------------------\n FINAL NUCLEAR SAFETY CHECK BEFORE XGBOOST\n No duplicate columns detected in final nuclear check\nTraining enhanced XGBoost model...\n Enhanced XGBoost training completed successfully without duplicate errors!\n\nSTEP 8: META-BLENDING WITH RULE SCORE\n------------------------------------------------------------\nTraining meta-blending model...\n Meta-blending model trained successfully\n\nSTEP 9: COMPREHENSIVE RESULTS\n================================================================================\nFINAL ENHANCED RESULTS:\nExact Accuracy: 86.81%\nWithin-One-Division: 98.90%\nF1 Score (Macro): 84.28%\n\nConfusion Matrix:\n[[19  3  0  1]\n [ 3 13  0  0]\n [ 0  2 11  3]\n [ 0  0  0 36]]\n\nPER-CLASS ACCURACY:\n  D3/NAIA: 82.6% exact, 95.7% within-one (n=23)\n  D2: 81.2% exact, 100.0% within-one (n=16)\n  FCS: 68.8% exact, 100.0% within-one (n=16)\n  Power 5: 100.0% exact, 100.0% within-one (n=36)\n\nPER-POSITION BREAKDOWN:\n  QB (n=36): 86.1% exact, 100.0% within-one\n  WR (n=22): 77.3% exact, 95.5% within-one\n  RB (n=33): 93.9% exact, 100.0% within-one\n\nFCS-SPECIFIC ANALYSIS:\n FCS Class Analysis:\n  Total FCS samples in test: 16\n  Correctly predicted: 11\n  FCS Accuracy: 68.8%\n  FCS prediction breakdown:\n    Predicted as D2: 2 samples\n    Predicted as FCS: 11 samples\n    Predicted as Power 5: 3 samples\n\nTOP 10 FEATURE IMPORTANCE:\n   1. speed_power_ratio        : 0.2022\n   2. ath_power                : 0.1488\n   3. combine_confidence       : 0.0548\n   4. senior_ypc               : 0.0442\n   5. senior_tds               : 0.0331\n   6. shuttle                  : 0.0300\n   7. senior_td                : 0.0291\n   8. vertical_jump            : 0.0289\n   9. senior_rec               : 0.0254\n  10. tds_game                 : 0.0223\n\n================================================================================\nACCURACY BOOST IMPLEMENTATION SUMMARY WITH DUPLICATE PREVENTION\n================================================================================\n Enhanced data loading with intelligent combine imputation\n Advanced feature engineering with comprehensive duplicate prevention\n Multiple layers of duplicate column safeguards:\n  - After each data concatenation\n  - After feature engineering\n  - Before and after ADASYN\n  - Nuclear safety check before XGBoost\n State embeddings and interaction features\n Enhanced class balancing with ADASYN\n Meta-blending with rule scores\n Comprehensive evaluation and analysis\n\nFINAL RESULTS:\n  Exact Accuracy: 86.81% (Target: 80%+)\n  Within-One: 98.90%\n  F1 Score: 84.28%\n\nPERFORMANCE IMPROVEMENT: +33.8% from baseline\n SUCCESS: Achieved target accuracy of 80%+!\n================================================================================\n DUPLICATE COLUMN ERROR PREVENTION: COMPREHENSIVE SAFEGUARDS APPLIED\n================================================================================\n"
          ]
        }
      ],
      "execution_count": 362,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}